W0427 18:26:25.009822 23456247911936 torch/distributed/run.py:757] 
W0427 18:26:25.009822 23456247911936 torch/distributed/run.py:757] *****************************************
W0427 18:26:25.009822 23456247911936 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0427 18:26:25.009822 23456247911936 torch/distributed/run.py:757] *****************************************
W0427 18:26:25.027316 23456247911936 torch/distributed/run.py:757] 
W0427 18:26:25.027316 23456247911936 torch/distributed/run.py:757] *****************************************
W0427 18:26:25.027316 23456247911936 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0427 18:26:25.027316 23456247911936 torch/distributed/run.py:757] *****************************************
W0427 18:26:25.030000 23456247911936 torch/distributed/run.py:757] 
W0427 18:26:25.030000 23456247911936 torch/distributed/run.py:757] *****************************************
W0427 18:26:25.030000 23456247911936 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0427 18:26:25.030000 23456247911936 torch/distributed/run.py:757] *****************************************
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 4): env://, gpu 0
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 5): env://, gpu 1
| distributed init (rank 6): env://, gpu 2
| distributed init (rank 7): env://, gpu 3
| distributed init (rank 8): env://, gpu 0
| distributed init (rank 10): env://, gpu 2
| distributed init (rank 11): env://, gpu 3
| distributed init (rank 9): env://, gpu 1
[18:26:34.631501] [18:26:34.631501] job dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_mae

[18:26:34.631517] job dir: /mnt/home/mpaez/cvii-final/med_mae
[18:26:34.631507] [18:26:34.631509] job dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_mae

[18:26:34.631598] [18:26:34.631598] Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=2,
gpu=2,
distributed=True,
[18:26:34.631537] job dir: /mnt/home/mpaez/cvii-final/med_mae
[18:26:34.631567] [18:26:34.631568] [18:26:34.631564] job dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_mae


dist_backend='nccl')Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=3,
gpu=3,
distributed=True,
dist_backend='nccl')
[18:26:34.631534] [18:26:34.631536] job dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_mae

[18:26:34.631609] Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=1,
gpu=1,
distributed=True,
dist_backend='nccl')

[18:26:34.631631] Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=7,
gpu=3,
distributed=True,
dist_backend='nccl')
[18:26:34.631608] [18:26:34.631608] Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=10,
gpu=2,
distributed=True,
[18:26:34.631661] job dir: /mnt/home/mpaez/cvii-final/med_mae
[18:26:34.631664] [18:26:34.631665] [18:26:34.631666] Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=4,
gpu=0,
distributed=True,
dist_backend='nccl')Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=11,
gpu=3,
distributed=True,
dist_backend='nccl')

[18:26:34.631807] Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=0,
gpu=0,
distributed=True,
dist_backend='nccl')
dist_backend='nccl')Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=6,
gpu=2,
distributed=True,
[18:26:34.631628] Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=9,
gpu=1,
distributed=True,
[18:26:34.632528] Using Directly-Resize Mode. (no RandomResizedCrop)
dist_backend='nccl')Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=5,
gpu=1,
distributed=True,
dist_backend='nccl')[18:26:34.631631] Namespace(batch_size=32,
epochs=100,
accum_iter=1,
model='vit_small_patch16',
input_size=224,
drop_path=0.2,
vit_dropout_rate=0.0,
clip_grad=None,
weight_decay=0.05,
lr=None,
blr=0.00025,
layer_decay=0.55,
fixed_lr=False,
min_lr=1e-06,
optimizer='adamw',
loss_func=None,
warmup_epochs=5,
smoothing=0.1,
repeated_aug=False,
mixup=0.0,
cutmix=0.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
global_pool=True,
dataset='chestxray14',
nb_classes=14,
output_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
log_dir='/mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/',
device='cuda',
seed=0,
resume='',
checkpoint_type=None,
start_epoch=0,
eval=False,
eval_interval=10,
dist_eval=False,
num_workers=4,
pin_mem=True,
world_size=12,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=8,
gpu=0,
distributed=True,
[18:26:34.632535] Using Directly-Resize Mode. (no RandomResizedCrop)
dist_backend='nccl')


dist_backend='nccl')

[18:26:34.632549] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:26:34.632564] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:26:34.632559] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:26:34.632948] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:26:34.632611] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:26:34.632570] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:26:34.632588] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:26:34.632609] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:26:34.632643] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:26:34.632653] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:26:35.241629] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491bfc730>
[18:26:35.241949] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491b207c0>
[18:26:35.242444] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491ada400>
[18:26:35.242532] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491ada400>
[18:26:35.242861] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491b207c0>
[18:26:35.242904] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491b21730>
[18:26:35.242957] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491b20730>
[18:26:35.243037] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491b21730>
[18:26:35.243350] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491ada400>
[18:26:35.243893] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491bfc730>
[18:26:35.244313] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491d60be0>
[18:26:35.245630] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x155491b1f760>
[18:26:35.853723] Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth
[18:26:35.853774] Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth
[18:26:35.853819] Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth
[18:26:35.854022] Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth
[18:26:35.854671] Loaded Index: cls_token from Saved Weights
[18:26:35.854689] Loaded Index: pos_embed from Saved Weights
[18:26:35.854696] mask_token not found in Init Model
[18:26:35.854702] decoder_pos_embed not found in Init Model
[18:26:35.854711] Loaded Index: patch_embed.proj.weight from Saved Weights
[18:26:35.854719] Loaded Index: patch_embed.proj.bias from Saved Weights
[18:26:35.854727] Loaded Index: blocks.0.norm1.weight from Saved Weights
[18:26:35.854735] Loaded Index: blocks.0.norm1.bias from Saved Weights
[18:26:35.854742] Loaded Index: blocks.0.attn.qkv.weight from Saved Weights
[18:26:35.854737] Loaded Index: cls_token from Saved Weights
[18:26:35.854750] Loaded Index: blocks.0.attn.qkv.bias from Saved Weights
[18:26:35.854756] [18:26:35.854758] Loaded Index: pos_embed from Saved WeightsLoaded Index: blocks.0.attn.proj.weight from Saved Weights

[18:26:35.854764] [18:26:35.854765] mask_token not found in Init Model
Loaded Index: blocks.0.attn.proj.bias from Saved Weights
[18:26:35.854771] decoder_pos_embed not found in Init Model
[18:26:35.854773] Loaded Index: blocks.0.norm2.weight from Saved Weights
[18:26:35.854780] [18:26:35.854780] Loaded Index: patch_embed.proj.weight from Saved WeightsLoaded Index: blocks.0.norm2.bias from Saved Weights

[18:26:35.854788] [18:26:35.854788] Loaded Index: blocks.0.mlp.fc1.weight from Saved Weights
Loaded Index: patch_embed.proj.bias from Saved Weights
[18:26:35.854796] [18:26:35.854797] Loaded Index: blocks.0.mlp.fc1.bias from Saved Weights
Loaded Index: blocks.0.norm1.weight from Saved Weights
[18:26:35.854804] [18:26:35.854805] Loaded Index: blocks.0.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.0.norm1.bias from Saved Weights

[18:26:35.854812] [18:26:35.854813] Loaded Index: blocks.0.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.0.attn.qkv.weight from Saved Weights

[18:26:35.854820] [18:26:35.854820] Loaded Index: blocks.1.norm1.weight from Saved WeightsLoaded Index: blocks.0.attn.qkv.bias from Saved Weights

[18:26:35.854828] [18:26:35.854828] Loaded Index: blocks.1.norm1.bias from Saved Weights
Loaded Index: blocks.0.attn.proj.weight from Saved Weights
[18:26:35.854835] [18:26:35.854836] Loaded Index: blocks.1.attn.qkv.weight from Saved WeightsLoaded Index: blocks.0.attn.proj.bias from Saved Weights

[18:26:35.854844] [18:26:35.854844] Loaded Index: blocks.1.attn.qkv.bias from Saved WeightsLoaded Index: blocks.0.norm2.weight from Saved Weights

[18:26:35.854851] [18:26:35.854851] Loaded Index: blocks.0.norm2.bias from Saved WeightsLoaded Index: blocks.1.attn.proj.weight from Saved Weights

[18:26:35.854852] [18:26:35.854860] [18:26:35.854860] Loaded Index: blocks.0.mlp.fc1.weight from Saved WeightsLoaded Index: cls_token from Saved WeightsLoaded Index: blocks.1.attn.proj.bias from Saved Weights


[18:26:35.854868] [18:26:35.854868] Loaded Index: blocks.0.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.1.norm2.weight from Saved Weights

[18:26:35.854870] Loaded Index: pos_embed from Saved Weights
[18:26:35.854876] [18:26:35.854876] Loaded Index: blocks.0.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.1.norm2.bias from Saved Weights

[18:26:35.854879] mask_token not found in Init Model
[18:26:35.854884] [18:26:35.854884] Loaded Index: blocks.1.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.0.mlp.fc2.bias from Saved Weights[18:26:35.854885] 

decoder_pos_embed not found in Init Model
[18:26:35.854892] [18:26:35.854892] Loaded Index: blocks.1.mlp.fc1.bias from Saved Weights
Loaded Index: blocks.1.norm1.weight from Saved Weights
[18:26:35.854894] Loaded Index: patch_embed.proj.weight from Saved Weights
[18:26:35.854899] [18:26:35.854900] Loaded Index: blocks.1.mlp.fc2.weight from Saved Weights
Loaded Index: blocks.1.norm1.bias from Saved Weights[18:26:35.854902] 
Loaded Index: patch_embed.proj.bias from Saved Weights
[18:26:35.854908] [18:26:35.854908] Loaded Index: blocks.1.mlp.fc2.bias from Saved Weights
Loaded Index: blocks.1.attn.qkv.weight from Saved Weights[18:26:35.854910] 
Loaded Index: blocks.0.norm1.weight from Saved Weights
[18:26:35.854915] [18:26:35.854916] Loaded Index: blocks.2.norm1.weight from Saved Weights
Loaded Index: blocks.1.attn.qkv.bias from Saved Weights[18:26:35.854917] 
Loaded Index: blocks.0.norm1.bias from Saved Weights
[18:26:35.854922] Loaded Index: blocks.2.norm1.bias from Saved Weights
[18:26:35.854924] Loaded Index: blocks.1.attn.proj.weight from Saved Weights
[18:26:35.854925] Loaded Index: blocks.0.attn.qkv.weight from Saved Weights
[18:26:35.854929] Loaded Index: blocks.2.attn.qkv.weight from Saved Weights
[18:26:35.854931] Loaded Index: blocks.1.attn.proj.bias from Saved Weights
[18:26:35.854933] Loaded Index: blocks.0.attn.qkv.bias from Saved Weights
[18:26:35.854936] Loaded Index: blocks.2.attn.qkv.bias from Saved Weights[18:26:35.854938] 
Loaded Index: blocks.1.norm2.weight from Saved Weights
[18:26:35.854940] Loaded Index: blocks.0.attn.proj.weight from Saved Weights
[18:26:35.854944] Loaded Index: blocks.2.attn.proj.weight from Saved Weights[18:26:35.854945] 
Loaded Index: blocks.1.norm2.bias from Saved Weights
[18:26:35.854948] Loaded Index: blocks.0.attn.proj.bias from Saved Weights
[18:26:35.854951] Loaded Index: blocks.2.attn.proj.bias from Saved Weights
[18:26:35.854953] Loaded Index: blocks.1.mlp.fc1.weight from Saved Weights
[18:26:35.854955] Loaded Index: blocks.0.norm2.weight from Saved Weights
[18:26:35.854958] Loaded Index: blocks.2.norm2.weight from Saved Weights
[18:26:35.854960] Loaded Index: blocks.1.mlp.fc1.bias from Saved Weights
[18:26:35.854963] Loaded Index: blocks.0.norm2.bias from Saved Weights
[18:26:35.854966] Loaded Index: blocks.2.norm2.bias from Saved Weights
[18:26:35.854968] Loaded Index: blocks.1.mlp.fc2.weight from Saved Weights
[18:26:35.854971] Loaded Index: blocks.0.mlp.fc1.weight from Saved Weights
[18:26:35.854973] Loaded Index: blocks.2.mlp.fc1.weight from Saved Weights
[18:26:35.854975] Loaded Index: blocks.1.mlp.fc2.bias from Saved Weights
[18:26:35.854978] Loaded Index: blocks.0.mlp.fc1.bias from Saved Weights
[18:26:35.854980] Loaded Index: blocks.2.mlp.fc1.bias from Saved Weights
[18:26:35.854983] Loaded Index: blocks.2.norm1.weight from Saved Weights
[18:26:35.854986] Loaded Index: blocks.0.mlp.fc2.weight from Saved Weights
[18:26:35.854988] Loaded Index: blocks.2.mlp.fc2.weight from Saved Weights
[18:26:35.854990] Loaded Index: blocks.2.norm1.bias from Saved Weights
[18:26:35.854993] Loaded Index: blocks.0.mlp.fc2.bias from Saved Weights
[18:26:35.854995] Loaded Index: blocks.2.mlp.fc2.bias from Saved Weights
[18:26:35.854998] Loaded Index: blocks.2.attn.qkv.weight from Saved Weights
[18:26:35.855000] Loaded Index: blocks.1.norm1.weight from Saved Weights
[18:26:35.855002] Loaded Index: blocks.3.norm1.weight from Saved Weights
[18:26:35.855005] Loaded Index: blocks.2.attn.qkv.bias from Saved Weights
[18:26:35.855007] Loaded Index: blocks.1.norm1.bias from Saved Weights
[18:26:35.855010] Loaded Index: blocks.3.norm1.bias from Saved Weights
[18:26:35.855012] Loaded Index: blocks.2.attn.proj.weight from Saved Weights
[18:26:35.855014] Loaded Index: blocks.1.attn.qkv.weight from Saved Weights
[18:26:35.855017] Loaded Index: blocks.3.attn.qkv.weight from Saved Weights
[18:26:35.855020] Loaded Index: blocks.2.attn.proj.bias from Saved Weights
[18:26:35.855022] Loaded Index: blocks.1.attn.qkv.bias from Saved Weights
[18:26:35.855024] Loaded Index: blocks.3.attn.qkv.bias from Saved Weights
[18:26:35.855027] Loaded Index: blocks.2.norm2.weight from Saved Weights
[18:26:35.855030] Loaded Index: blocks.1.attn.proj.weight from Saved Weights[18:26:35.855031] 
Loaded Index: blocks.3.attn.proj.weight from Saved Weights
[18:26:35.855034] Loaded Index: blocks.2.norm2.bias from Saved Weights
[18:26:35.855037] Loaded Index: blocks.1.attn.proj.bias from Saved Weights[18:26:35.855039] 
Loaded Index: blocks.3.attn.proj.bias from Saved Weights
[18:26:35.855041] Loaded Index: blocks.2.mlp.fc1.weight from Saved Weights
[18:26:35.855045] [18:26:35.855046] Loaded Index: blocks.1.norm2.weight from Saved Weights
Loaded Index: blocks.3.norm2.weight from Saved Weights[18:26:35.855048] 
Loaded Index: blocks.2.mlp.fc1.bias from Saved Weights
[18:26:35.855053] Loaded Index: blocks.1.norm2.bias from Saved Weights[18:26:35.855054] [18:26:35.855055] 
Loaded Index: blocks.3.norm2.bias from Saved WeightsLoaded Index: blocks.2.mlp.fc2.weight from Saved Weights

[18:26:35.855061] Loaded Index: blocks.1.mlp.fc1.weight from Saved Weights[18:26:35.855062] [18:26:35.855063] 
Loaded Index: blocks.3.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.2.mlp.fc2.bias from Saved Weights

[18:26:35.855069] Loaded Index: blocks.1.mlp.fc1.bias from Saved Weights[18:26:35.855071] [18:26:35.855071] 
Loaded Index: blocks.3.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.3.norm1.weight from Saved Weights

[18:26:35.855077] Loaded Index: blocks.1.mlp.fc2.weight from Saved Weights
[18:26:35.855079] [18:26:35.855079] Loaded Index: blocks.3.mlp.fc2.weight from Saved Weights[18:26:35.855073] Loaded Index: blocks.3.norm1.bias from Saved Weights

Loaded Index: cls_token from Saved Weights
[18:26:35.855084] Loaded Index: blocks.1.mlp.fc2.bias from Saved Weights
[18:26:35.855087] [18:26:35.855087] Loaded Index: blocks.3.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.3.attn.qkv.weight from Saved Weights

[18:26:35.855092] [18:26:35.855091] Loaded Index: blocks.2.norm1.weight from Saved WeightsLoaded Index: pos_embed from Saved Weights

[18:26:35.855094] [18:26:35.855094] Loaded Index: blocks.4.norm1.weight from Saved WeightsLoaded Index: blocks.3.attn.qkv.bias from Saved Weights

[18:26:35.855099] [18:26:35.855099] mask_token not found in Init ModelLoaded Index: blocks.2.norm1.bias from Saved Weights

[18:26:35.855102] [18:26:35.855102] Loaded Index: blocks.4.norm1.bias from Saved WeightsLoaded Index: blocks.3.attn.proj.weight from Saved Weights

[18:26:35.855106] decoder_pos_embed not found in Init Model[18:26:35.855107] 
Loaded Index: blocks.2.attn.qkv.weight from Saved Weights[18:26:35.855109] 
[18:26:35.855109] Loaded Index: blocks.4.attn.qkv.weight from Saved WeightsLoaded Index: blocks.3.attn.proj.bias from Saved Weights

[18:26:35.855115] [18:26:35.855115] Loaded Index: blocks.2.attn.qkv.bias from Saved WeightsLoaded Index: patch_embed.proj.weight from Saved Weights[18:26:35.855117] 
[18:26:35.855117] 
Loaded Index: blocks.4.attn.qkv.bias from Saved WeightsLoaded Index: blocks.3.norm2.weight from Saved Weights

[18:26:35.855123] [18:26:35.855123] Loaded Index: blocks.2.attn.proj.weight from Saved Weights
[18:26:35.855124] Loaded Index: patch_embed.proj.bias from Saved Weights[18:26:35.855125] 
Loaded Index: blocks.4.attn.proj.weight from Saved WeightsLoaded Index: blocks.3.norm2.bias from Saved Weights

[18:26:35.855130] Loaded Index: blocks.2.attn.proj.bias from Saved Weights[18:26:35.855131] 
[18:26:35.855132] [18:26:35.855133] Loaded Index: blocks.0.norm1.weight from Saved WeightsLoaded Index: blocks.4.attn.proj.bias from Saved Weights
Loaded Index: blocks.3.mlp.fc1.weight from Saved Weights

[18:26:35.855138] Loaded Index: blocks.2.norm2.weight from Saved Weights
[18:26:35.855139] [18:26:35.855139] [18:26:35.855140] Loaded Index: blocks.0.norm1.bias from Saved WeightsLoaded Index: blocks.4.norm2.weight from Saved WeightsLoaded Index: blocks.3.mlp.fc1.bias from Saved Weights


[18:26:35.855145] Loaded Index: blocks.2.norm2.bias from Saved Weights
[18:26:35.855147] Loaded Index: blocks.4.norm2.bias from Saved Weights[18:26:35.855148] 
[18:26:35.855149] Loaded Index: blocks.3.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.0.attn.qkv.weight from Saved Weights

[18:26:35.855153] Loaded Index: blocks.2.mlp.fc1.weight from Saved Weights
[18:26:35.855155] Loaded Index: blocks.4.mlp.fc1.weight from Saved Weights[18:26:35.855156] [18:26:35.855156] 
Loaded Index: blocks.3.mlp.fc2.bias from Saved Weights
Loaded Index: blocks.0.attn.qkv.bias from Saved Weights
[18:26:35.855160] Loaded Index: blocks.2.mlp.fc1.bias from Saved Weights
[18:26:35.855163] Loaded Index: blocks.4.mlp.fc1.bias from Saved Weights[18:26:35.855164] 
[18:26:35.855165] Loaded Index: blocks.4.norm1.weight from Saved Weights
Loaded Index: blocks.0.attn.proj.weight from Saved Weights[18:26:35.855167] 
Loaded Index: blocks.2.mlp.fc2.weight from Saved Weights
[18:26:35.855170] Loaded Index: blocks.4.mlp.fc2.weight from Saved Weights[18:26:35.855172] 
[18:26:35.855173] Loaded Index: blocks.4.norm1.bias from Saved Weights
Loaded Index: blocks.0.attn.proj.bias from Saved Weights
[18:26:35.855175] Loaded Index: blocks.2.mlp.fc2.bias from Saved Weights
[18:26:35.855178] Loaded Index: blocks.4.mlp.fc2.bias from Saved Weights
[18:26:35.855179] [18:26:35.855180] Loaded Index: blocks.4.attn.qkv.weight from Saved Weights
Loaded Index: blocks.0.norm2.weight from Saved Weights
[18:26:35.855182] Loaded Index: blocks.3.norm1.weight from Saved Weights
[18:26:35.855185] Loaded Index: blocks.5.norm1.weight from Saved Weights
[18:26:35.855187] [18:26:35.855188] Loaded Index: blocks.4.attn.qkv.bias from Saved Weights
Loaded Index: blocks.0.norm2.bias from Saved Weights
[18:26:35.855190] Loaded Index: blocks.3.norm1.bias from Saved Weights
[18:26:35.855192] Loaded Index: blocks.5.norm1.bias from Saved Weights
[18:26:35.855194] Loaded Index: blocks.4.attn.proj.weight from Saved Weights[18:26:35.855195] 
Loaded Index: blocks.0.mlp.fc1.weight from Saved Weights[18:26:35.855197] 
Loaded Index: blocks.3.attn.qkv.weight from Saved Weights
[18:26:35.855199] Loaded Index: blocks.5.attn.qkv.weight from Saved Weights
[18:26:35.855201] Loaded Index: blocks.4.attn.proj.bias from Saved Weights
[18:26:35.855203] [18:26:35.855204] Loaded Index: blocks.0.mlp.fc1.bias from Saved Weights
Loaded Index: blocks.3.attn.qkv.bias from Saved Weights
[18:26:35.855206] Loaded Index: blocks.5.attn.qkv.bias from Saved Weights
[18:26:35.855208] Loaded Index: blocks.4.norm2.weight from Saved Weights
[18:26:35.855211] [18:26:35.855212] Loaded Index: blocks.0.mlp.fc2.weight from Saved Weights
Loaded Index: blocks.3.attn.proj.weight from Saved Weights[18:26:35.855214] 
Loaded Index: blocks.5.attn.proj.weight from Saved Weights
[18:26:35.855215] Loaded Index: blocks.4.norm2.bias from Saved Weights
[18:26:35.855219] [18:26:35.855220] Loaded Index: blocks.0.mlp.fc2.bias from Saved Weights[18:26:35.855221] Loaded Index: blocks.3.attn.proj.bias from Saved Weights

Loaded Index: blocks.5.attn.proj.bias from Saved Weights[18:26:35.855222] 
Loaded Index: blocks.4.mlp.fc1.weight from Saved Weights
[18:26:35.855227] [18:26:35.855228] [18:26:35.855228] Loaded Index: blocks.1.norm1.weight from Saved WeightsLoaded Index: blocks.3.norm2.weight from Saved Weights

Loaded Index: blocks.5.norm2.weight from Saved Weights
[18:26:35.855230] Loaded Index: blocks.4.mlp.fc1.bias from Saved Weights
[18:26:35.855235] [18:26:35.855235] Loaded Index: blocks.1.norm1.bias from Saved Weights[18:26:35.855236] Loaded Index: blocks.3.norm2.bias from Saved Weights

Loaded Index: blocks.5.norm2.bias from Saved Weights[18:26:35.855238] 
Loaded Index: blocks.4.mlp.fc2.weight from Saved Weights
[18:26:35.855242] [18:26:35.855242] Loaded Index: blocks.1.attn.qkv.weight from Saved Weights[18:26:35.855243] 
Loaded Index: blocks.3.mlp.fc1.weight from Saved Weights
Loaded Index: blocks.5.mlp.fc1.weight from Saved Weights[18:26:35.855245] 
Loaded Index: blocks.4.mlp.fc2.bias from Saved Weights
[18:26:35.855249] Loaded Index: blocks.1.attn.qkv.bias from Saved Weights
[18:26:35.855250] [18:26:35.855251] [18:26:35.855252] Loaded Index: blocks.3.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.5.mlp.fc1.bias from Saved Weights
Loaded Index: blocks.5.norm1.weight from Saved Weights

[18:26:35.855257] Loaded Index: blocks.1.attn.proj.weight from Saved Weights
[18:26:35.855259] [18:26:35.855260] [18:26:35.855260] Loaded Index: blocks.3.mlp.fc2.weight from Saved Weights
Loaded Index: blocks.5.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.5.norm1.bias from Saved Weights

[18:26:35.855265] Loaded Index: blocks.1.attn.proj.bias from Saved Weights
[18:26:35.855267] [18:26:35.855268] Loaded Index: blocks.3.mlp.fc2.bias from Saved Weights[18:26:35.855269] 
Loaded Index: blocks.5.mlp.fc2.bias from Saved Weights
Loaded Index: blocks.5.attn.qkv.weight from Saved Weights
[18:26:35.855272] Loaded Index: blocks.1.norm2.weight from Saved Weights
[18:26:35.855275] Loaded Index: blocks.4.norm1.weight from Saved Weights[18:26:35.855277] [18:26:35.855277] 
Loaded Index: blocks.5.attn.qkv.bias from Saved WeightsLoaded Index: blocks.6.norm1.weight from Saved Weights

[18:26:35.855279] Loaded Index: blocks.1.norm2.bias from Saved Weights
[18:26:35.855283] Loaded Index: blocks.4.norm1.bias from Saved Weights[18:26:35.855285] 
[18:26:35.855285] Loaded Index: blocks.5.attn.proj.weight from Saved Weights
[18:26:35.855287] Loaded Index: blocks.6.norm1.bias from Saved Weights
Loaded Index: blocks.1.mlp.fc1.weight from Saved Weights
[18:26:35.855291] Loaded Index: blocks.4.attn.qkv.weight from Saved Weights[18:26:35.855293] 
[18:26:35.855294] Loaded Index: blocks.5.attn.proj.bias from Saved Weights
[18:26:35.855295] Loaded Index: blocks.6.attn.qkv.weight from Saved Weights
Loaded Index: blocks.1.mlp.fc1.bias from Saved Weights
[18:26:35.855299] Loaded Index: blocks.4.attn.qkv.bias from Saved Weights[18:26:35.855300] 
[18:26:35.855301] Loaded Index: blocks.5.norm2.weight from Saved Weights
Loaded Index: blocks.6.attn.qkv.bias from Saved Weights[18:26:35.855302] 
Loaded Index: blocks.1.mlp.fc2.weight from Saved Weights
[18:26:35.855307] Loaded Index: blocks.4.attn.proj.weight from Saved Weights[18:26:35.855308] 
[18:26:35.855309] Loaded Index: blocks.5.norm2.bias from Saved WeightsLoaded Index: blocks.6.attn.proj.weight from Saved Weights

[18:26:35.855310] Loaded Index: blocks.1.mlp.fc2.bias from Saved Weights
[18:26:35.855314] Loaded Index: blocks.4.attn.proj.bias from Saved Weights
[18:26:35.855316] [18:26:35.855316] Loaded Index: blocks.5.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.6.attn.proj.bias from Saved Weights[18:26:35.855318] 

Loaded Index: blocks.2.norm1.weight from Saved Weights
[18:26:35.855321] Loaded Index: blocks.4.norm2.weight from Saved Weights
[18:26:35.855324] [18:26:35.855325] Loaded Index: blocks.6.norm2.weight from Saved Weights[18:26:35.855325] Loaded Index: blocks.5.mlp.fc1.bias from Saved Weights

Loaded Index: blocks.2.norm1.bias from Saved Weights
[18:26:35.855328] Loaded Index: blocks.4.norm2.bias from Saved Weights
[18:26:35.855332] [18:26:35.855332] Loaded Index: blocks.6.norm2.bias from Saved Weights[18:26:35.855333] 
Loaded Index: blocks.5.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.2.attn.qkv.weight from Saved Weights

[18:26:35.855335] Loaded Index: blocks.4.mlp.fc1.weight from Saved Weights
[18:26:35.855339] [18:26:35.855340] [18:26:35.855340] Loaded Index: blocks.6.mlp.fc1.weight from Saved Weights
Loaded Index: blocks.5.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.2.attn.qkv.bias from Saved Weights

[18:26:35.855343] Loaded Index: blocks.4.mlp.fc1.bias from Saved Weights
[18:26:35.855347] Loaded Index: blocks.6.mlp.fc1.bias from Saved Weights[18:26:35.855348] [18:26:35.855348] 
Loaded Index: blocks.6.norm1.weight from Saved WeightsLoaded Index: blocks.2.attn.proj.weight from Saved Weights

[18:26:35.855350] Loaded Index: blocks.4.mlp.fc2.weight from Saved Weights
[18:26:35.855355] Loaded Index: blocks.6.mlp.fc2.weight from Saved Weights[18:26:35.855356] [18:26:35.855356] 
Loaded Index: blocks.2.attn.proj.bias from Saved WeightsLoaded Index: blocks.6.norm1.bias from Saved Weights[18:26:35.855358] 

Loaded Index: blocks.4.mlp.fc2.bias from Saved Weights
[18:26:35.855363] Loaded Index: blocks.6.mlp.fc2.bias from Saved Weights[18:26:35.855364] 
[18:26:35.855364] Loaded Index: blocks.2.norm2.weight from Saved Weights[18:26:35.855365] Loaded Index: blocks.6.attn.qkv.weight from Saved Weights

Loaded Index: blocks.5.norm1.weight from Saved Weights
[18:26:35.855370] Loaded Index: blocks.7.norm1.weight from Saved Weights[18:26:35.855371] 
[18:26:35.855372] Loaded Index: blocks.2.norm2.bias from Saved Weights[18:26:35.855373] Loaded Index: blocks.6.attn.qkv.bias from Saved Weights
Loaded Index: blocks.5.norm1.bias from Saved Weights

[18:26:35.855378] Loaded Index: blocks.7.norm1.bias from Saved Weights
[18:26:35.855380] [18:26:35.855380] [18:26:35.855381] Loaded Index: blocks.2.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.6.attn.proj.weight from Saved Weights
Loaded Index: blocks.5.attn.qkv.weight from Saved Weights

[18:26:35.855385] Loaded Index: blocks.7.attn.qkv.weight from Saved Weights
[18:26:35.855389] [18:26:35.855389] [18:26:35.855390] Loaded Index: blocks.2.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.6.attn.proj.bias from Saved Weights
Loaded Index: blocks.5.attn.qkv.bias from Saved Weights

[18:26:35.855392] Loaded Index: blocks.7.attn.qkv.bias from Saved Weights
[18:26:35.855396] [18:26:35.855397] [18:26:35.855397] Loaded Index: blocks.2.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.6.norm2.weight from Saved Weights
Loaded Index: blocks.5.attn.proj.weight from Saved Weights

[18:26:35.855400] Loaded Index: blocks.7.attn.proj.weight from Saved Weights
[18:26:35.855404] [18:26:35.855404] [18:26:35.855405] Loaded Index: blocks.2.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.6.norm2.bias from Saved WeightsLoaded Index: blocks.5.attn.proj.bias from Saved Weights


[18:26:35.855407] Loaded Index: blocks.7.attn.proj.bias from Saved Weights
[18:26:35.855412] [18:26:35.855412] [18:26:35.855412] Loaded Index: blocks.6.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.3.norm1.weight from Saved Weights
Loaded Index: blocks.5.norm2.weight from Saved Weights
[18:26:35.855414] 
Loaded Index: blocks.7.norm2.weight from Saved Weights
[18:26:35.855420] [18:26:35.855420] Loaded Index: blocks.6.mlp.fc1.bias from Saved Weights
Loaded Index: blocks.3.norm1.bias from Saved Weights[18:26:35.855421] 
[18:26:35.855423] Loaded Index: blocks.5.norm2.bias from Saved Weights
Loaded Index: blocks.7.norm2.bias from Saved Weights
[18:26:35.855427] [18:26:35.855428] Loaded Index: blocks.6.mlp.fc2.weight from Saved Weights
Loaded Index: blocks.3.attn.qkv.weight from Saved Weights
[18:26:35.855430] [18:26:35.855430] Loaded Index: blocks.5.mlp.fc1.weight from Saved Weights
Loaded Index: blocks.7.mlp.fc1.weight from Saved Weights
[18:26:35.855434] [18:26:35.855435] Loaded Index: blocks.6.mlp.fc2.bias from Saved Weights
Loaded Index: blocks.3.attn.qkv.bias from Saved Weights
[18:26:35.855437] [18:26:35.855438] Loaded Index: blocks.5.mlp.fc1.bias from Saved Weights
Loaded Index: blocks.7.mlp.fc1.bias from Saved Weights
[18:26:35.855441] [18:26:35.855442] Loaded Index: blocks.7.norm1.weight from Saved Weights
Loaded Index: blocks.3.attn.proj.weight from Saved Weights
[18:26:35.855444] [18:26:35.855445] Loaded Index: blocks.5.mlp.fc2.weight from Saved Weights
Loaded Index: blocks.7.mlp.fc2.weight from Saved Weights
[18:26:35.855449] [18:26:35.855450] Loaded Index: blocks.7.norm1.bias from Saved Weights
Loaded Index: blocks.3.attn.proj.bias from Saved Weights
[18:26:35.855452] [18:26:35.855453] Loaded Index: blocks.5.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.7.mlp.fc2.bias from Saved Weights

[18:26:35.855456] [18:26:35.855457] Loaded Index: blocks.7.attn.qkv.weight from Saved WeightsLoaded Index: blocks.3.norm2.weight from Saved Weights

[18:26:35.855460] [18:26:35.855460] Loaded Index: blocks.6.norm1.weight from Saved WeightsLoaded Index: blocks.8.norm1.weight from Saved Weights

[18:26:35.855464] [18:26:35.855464] Loaded Index: blocks.7.attn.qkv.bias from Saved Weights
Loaded Index: blocks.3.norm2.bias from Saved Weights
[18:26:35.855467] [18:26:35.855468] Loaded Index: blocks.8.norm1.bias from Saved WeightsLoaded Index: blocks.6.norm1.bias from Saved Weights

[18:26:35.855471] [18:26:35.855471] Loaded Index: blocks.7.attn.proj.weight from Saved WeightsLoaded Index: blocks.3.mlp.fc1.weight from Saved Weights

[18:26:35.855475] [18:26:35.855475] Loaded Index: blocks.8.attn.qkv.weight from Saved WeightsLoaded Index: blocks.6.attn.qkv.weight from Saved Weights

[18:26:35.855478] [18:26:35.855479] Loaded Index: blocks.7.attn.proj.bias from Saved Weights
Loaded Index: blocks.3.mlp.fc1.bias from Saved Weights
[18:26:35.855483] [18:26:35.855483] Loaded Index: blocks.8.attn.qkv.bias from Saved WeightsLoaded Index: blocks.6.attn.qkv.bias from Saved Weights

[18:26:35.855485] [18:26:35.855486] Loaded Index: blocks.7.norm2.weight from Saved WeightsLoaded Index: blocks.3.mlp.fc2.weight from Saved Weights

[18:26:35.855490] [18:26:35.855490] Loaded Index: blocks.8.attn.proj.weight from Saved WeightsLoaded Index: blocks.6.attn.proj.weight from Saved Weights

[18:26:35.855493] [18:26:35.855493] Loaded Index: blocks.3.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.7.norm2.bias from Saved Weights

[18:26:35.855497] [18:26:35.855498] Loaded Index: blocks.8.attn.proj.bias from Saved WeightsLoaded Index: blocks.6.attn.proj.bias from Saved Weights

[18:26:35.855501] [18:26:35.855502] Loaded Index: blocks.4.norm1.weight from Saved Weights
Loaded Index: blocks.7.mlp.fc1.weight from Saved Weights
[18:26:35.855505] [18:26:35.855505] Loaded Index: blocks.8.norm2.weight from Saved WeightsLoaded Index: blocks.6.norm2.weight from Saved Weights

[18:26:35.855509] [18:26:35.855510] Loaded Index: blocks.4.norm1.bias from Saved WeightsLoaded Index: blocks.7.mlp.fc1.bias from Saved Weights

[18:26:35.855513] [18:26:35.855513] Loaded Index: blocks.8.norm2.bias from Saved WeightsLoaded Index: blocks.6.norm2.bias from Saved Weights

[18:26:35.855517] [18:26:35.855518] Loaded Index: blocks.4.attn.qkv.weight from Saved WeightsLoaded Index: blocks.7.mlp.fc2.weight from Saved Weights

[18:26:35.855520] [18:26:35.855521] Loaded Index: blocks.8.mlp.fc1.weight from Saved Weights
Loaded Index: blocks.6.mlp.fc1.weight from Saved Weights
[18:26:35.855525] [18:26:35.855525] Loaded Index: blocks.4.attn.qkv.bias from Saved WeightsLoaded Index: blocks.7.mlp.fc2.bias from Saved Weights

[18:26:35.855528] Loaded Index: blocks.8.mlp.fc1.bias from Saved Weights[18:26:35.855528] 
Loaded Index: blocks.6.mlp.fc1.bias from Saved Weights
[18:26:35.855533] [18:26:35.855533] Loaded Index: blocks.8.norm1.weight from Saved WeightsLoaded Index: blocks.4.attn.proj.weight from Saved Weights

[18:26:35.855535] [18:26:35.855536] Loaded Index: blocks.8.mlp.fc2.weight from Saved Weights
Loaded Index: blocks.6.mlp.fc2.weight from Saved Weights
[18:26:35.855541] [18:26:35.855541] Loaded Index: blocks.8.norm1.bias from Saved WeightsLoaded Index: blocks.4.attn.proj.bias from Saved Weights
[18:26:35.855543] 
[18:26:35.855544] Loaded Index: blocks.8.mlp.fc2.bias from Saved Weights
Loaded Index: blocks.6.mlp.fc2.bias from Saved Weights
[18:26:35.855548] [18:26:35.855548] Loaded Index: blocks.8.attn.qkv.weight from Saved Weights[18:26:35.855550] Loaded Index: blocks.4.norm2.weight from Saved Weights

Loaded Index: blocks.9.norm1.weight from Saved Weights[18:26:35.855551] 
Loaded Index: blocks.7.norm1.weight from Saved Weights
[18:26:35.855557] [18:26:35.855557] [18:26:35.855558] Loaded Index: blocks.4.norm2.bias from Saved WeightsLoaded Index: blocks.8.attn.qkv.bias from Saved Weights
Loaded Index: blocks.9.norm1.bias from Saved Weights
[18:26:35.855559] 
Loaded Index: blocks.7.norm1.bias from Saved Weights
[18:26:35.855564] [18:26:35.855565] Loaded Index: blocks.4.mlp.fc1.weight from Saved Weights[18:26:35.855565] 
Loaded Index: blocks.8.attn.proj.weight from Saved WeightsLoaded Index: blocks.9.attn.qkv.weight from Saved Weights
[18:26:35.855567] 
Loaded Index: blocks.7.attn.qkv.weight from Saved Weights
[18:26:35.855572] [18:26:35.855573] Loaded Index: blocks.4.mlp.fc1.bias from Saved Weights
[18:26:35.855573] Loaded Index: blocks.8.attn.proj.bias from Saved Weights
Loaded Index: blocks.9.attn.qkv.bias from Saved Weights[18:26:35.855575] 
Loaded Index: blocks.7.attn.qkv.bias from Saved Weights
[18:26:35.855579] [18:26:35.855580] Loaded Index: blocks.4.mlp.fc2.weight from Saved Weights
Loaded Index: blocks.8.norm2.weight from Saved Weights[18:26:35.855581] 
[18:26:35.855582] Loaded Index: blocks.9.attn.proj.weight from Saved WeightsLoaded Index: blocks.7.attn.proj.weight from Saved Weights

[18:26:35.855586] [18:26:35.855587] Loaded Index: blocks.4.mlp.fc2.bias from Saved Weights
Loaded Index: blocks.8.norm2.bias from Saved Weights
[18:26:35.855590] [18:26:35.855590] Loaded Index: blocks.9.attn.proj.bias from Saved Weights
Loaded Index: blocks.7.attn.proj.bias from Saved Weights
[18:26:35.855593] Loaded Index: blocks.5.norm1.weight from Saved Weights[18:26:35.855594] 
Loaded Index: blocks.8.mlp.fc1.weight from Saved Weights
[18:26:35.855597] [18:26:35.855597] Loaded Index: blocks.9.norm2.weight from Saved Weights
Loaded Index: blocks.7.norm2.weight from Saved Weights
[18:26:35.855601] Loaded Index: blocks.5.norm1.bias from Saved Weights[18:26:35.855602] 
Loaded Index: blocks.8.mlp.fc1.bias from Saved Weights
[18:26:35.855604] Loaded Index: blocks.9.norm2.bias from Saved Weights[18:26:35.855605] 
Loaded Index: blocks.7.norm2.bias from Saved Weights
[18:26:35.855608] Loaded Index: blocks.5.attn.qkv.weight from Saved Weights[18:26:35.855609] 
Loaded Index: blocks.8.mlp.fc2.weight from Saved Weights
[18:26:35.855612] Loaded Index: blocks.9.mlp.fc1.weight from Saved Weights[18:26:35.855613] 
Loaded Index: blocks.7.mlp.fc1.weight from Saved Weights
[18:26:35.855616] [18:26:35.855617] Loaded Index: blocks.5.attn.qkv.bias from Saved Weights
Loaded Index: blocks.8.mlp.fc2.bias from Saved Weights
[18:26:35.855619] Loaded Index: blocks.9.mlp.fc1.bias from Saved Weights
[18:26:35.855620] Loaded Index: blocks.7.mlp.fc1.bias from Saved Weights
[18:26:35.855623] [18:26:35.855624] Loaded Index: blocks.5.attn.proj.weight from Saved Weights
Loaded Index: blocks.9.norm1.weight from Saved Weights
[18:26:35.855626] Loaded Index: blocks.9.mlp.fc2.weight from Saved Weights[18:26:35.855628] 
Loaded Index: blocks.7.mlp.fc2.weight from Saved Weights
[18:26:35.855630] Loaded Index: blocks.5.attn.proj.bias from Saved Weights[18:26:35.855632] 
Loaded Index: blocks.9.norm1.bias from Saved Weights
[18:26:35.855634] Loaded Index: blocks.9.mlp.fc2.bias from Saved Weights[18:26:35.855635] 
Loaded Index: blocks.7.mlp.fc2.bias from Saved Weights
[18:26:35.855638] Loaded Index: blocks.5.norm2.weight from Saved Weights[18:26:35.855639] 
Loaded Index: blocks.9.attn.qkv.weight from Saved Weights
[18:26:35.855641] Loaded Index: blocks.10.norm1.weight from Saved Weights[18:26:35.855642] 
Loaded Index: blocks.8.norm1.weight from Saved Weights
[18:26:35.855645] [18:26:35.855646] Loaded Index: blocks.5.norm2.bias from Saved Weights
Loaded Index: blocks.9.attn.qkv.bias from Saved Weights
[18:26:35.855648] Loaded Index: blocks.10.norm1.bias from Saved Weights[18:26:35.855649] 
Loaded Index: blocks.8.norm1.bias from Saved Weights
[18:26:35.855652] [18:26:35.855653] Loaded Index: blocks.5.mlp.fc1.weight from Saved Weights
Loaded Index: blocks.9.attn.proj.weight from Saved Weights
[18:26:35.855656] Loaded Index: blocks.10.attn.qkv.weight from Saved Weights[18:26:35.855657] 
Loaded Index: blocks.8.attn.qkv.weight from Saved Weights
[18:26:35.855660] [18:26:35.855661] Loaded Index: blocks.5.mlp.fc1.bias from Saved Weights
Loaded Index: blocks.9.attn.proj.bias from Saved Weights
[18:26:35.855663] Loaded Index: blocks.10.attn.qkv.bias from Saved Weights[18:26:35.855664] 
Loaded Index: blocks.8.attn.qkv.bias from Saved Weights
[18:26:35.855667] [18:26:35.855668] Loaded Index: blocks.5.mlp.fc2.weight from Saved Weights
Loaded Index: blocks.9.norm2.weight from Saved Weights
[18:26:35.855671] Loaded Index: blocks.10.attn.proj.weight from Saved Weights[18:26:35.855672] 
Loaded Index: blocks.8.attn.proj.weight from Saved Weights
[18:26:35.855674] Loaded Index: blocks.5.mlp.fc2.bias from Saved Weights[18:26:35.855676] 
Loaded Index: blocks.9.norm2.bias from Saved Weights[18:26:35.855678] 
[18:26:35.855679] Loaded Index: blocks.10.attn.proj.bias from Saved Weights
Loaded Index: blocks.8.attn.proj.bias from Saved Weights
[18:26:35.855683] Loaded Index: blocks.6.norm1.weight from Saved Weights
[18:26:35.855684] Loaded Index: blocks.9.mlp.fc1.weight from Saved Weights[18:26:35.855686] 
[18:26:35.855686] Loaded Index: blocks.10.norm2.weight from Saved Weights
Loaded Index: blocks.8.norm2.weight from Saved Weights
[18:26:35.855690] Loaded Index: blocks.6.norm1.bias from Saved Weights
[18:26:35.855692] Loaded Index: blocks.9.mlp.fc1.bias from Saved Weights[18:26:35.855693] 
[18:26:35.855694] Loaded Index: blocks.10.norm2.bias from Saved Weights
Loaded Index: blocks.8.norm2.bias from Saved Weights
[18:26:35.855698] Loaded Index: blocks.6.attn.qkv.weight from Saved Weights
[18:26:35.855700] [18:26:35.855701] Loaded Index: blocks.9.mlp.fc2.weight from Saved Weights[18:26:35.855701] 
Loaded Index: blocks.10.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.8.mlp.fc1.weight from Saved Weights

[18:26:35.855704] Loaded Index: blocks.6.attn.qkv.bias from Saved Weights
[18:26:35.855708] [18:26:35.855709] Loaded Index: blocks.9.mlp.fc2.bias from Saved Weights[18:26:35.855710] 
Loaded Index: blocks.10.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.8.mlp.fc1.bias from Saved Weights
[18:26:35.855712] 
Loaded Index: blocks.6.attn.proj.weight from Saved Weights
[18:26:35.855716] [18:26:35.855717] Loaded Index: blocks.10.norm1.weight from Saved Weights
[18:26:35.855718] Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights[18:26:35.855719] 
Loaded Index: blocks.8.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.6.attn.proj.bias from Saved Weights

[18:26:35.855724] Loaded Index: blocks.10.norm1.bias from Saved Weights[18:26:35.855725] 
[18:26:35.855727] [18:26:35.855727] Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights
Loaded Index: blocks.6.norm2.weight from Saved WeightsLoaded Index: blocks.8.mlp.fc2.bias from Saved Weights

[18:26:35.855732] Loaded Index: blocks.10.attn.qkv.weight from Saved Weights[18:26:35.855734] 
Loaded Index: blocks.11.norm1.weight from Saved Weights[18:26:35.855734] [18:26:35.855735] 
Loaded Index: blocks.6.norm2.bias from Saved WeightsLoaded Index: blocks.9.norm1.weight from Saved Weights

[18:26:35.855741] [18:26:35.855742] Loaded Index: blocks.10.attn.qkv.bias from Saved Weights
Loaded Index: blocks.11.norm1.bias from Saved Weights[18:26:35.855743] [18:26:35.855743] 
Loaded Index: blocks.6.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.9.norm1.bias from Saved Weights

[18:26:35.855748] Loaded Index: blocks.10.attn.proj.weight from Saved Weights[18:26:35.855749] 
[18:26:35.855750] Loaded Index: blocks.11.attn.qkv.weight from Saved Weights[18:26:35.855751] 
Loaded Index: blocks.6.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.9.attn.qkv.weight from Saved Weights

[18:26:35.855755] Loaded Index: blocks.10.attn.proj.bias from Saved Weights
[18:26:35.855757] Loaded Index: blocks.11.attn.qkv.bias from Saved Weights
[18:26:35.855759] [18:26:35.855759] Loaded Index: blocks.6.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.9.attn.qkv.bias from Saved Weights

[18:26:35.855763] Loaded Index: blocks.10.norm2.weight from Saved Weights
[18:26:35.855764] Loaded Index: blocks.11.attn.proj.weight from Saved Weights
[18:26:35.855766] [18:26:35.855767] Loaded Index: blocks.6.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.9.attn.proj.weight from Saved Weights

[18:26:35.855770] Loaded Index: blocks.10.norm2.bias from Saved Weights
[18:26:35.855772] Loaded Index: blocks.11.attn.proj.bias from Saved Weights
[18:26:35.855774] [18:26:35.855774] Loaded Index: blocks.9.attn.proj.bias from Saved WeightsLoaded Index: blocks.7.norm1.weight from Saved Weights

[18:26:35.855777] Loaded Index: blocks.10.mlp.fc1.weight from Saved Weights
[18:26:35.855779] Loaded Index: blocks.11.norm2.weight from Saved Weights
[18:26:35.855782] [18:26:35.855782] Loaded Index: blocks.7.norm1.bias from Saved WeightsLoaded Index: blocks.9.norm2.weight from Saved Weights[18:26:35.855784] 

Loaded Index: blocks.10.mlp.fc1.bias from Saved Weights
[18:26:35.855787] Loaded Index: blocks.11.norm2.bias from Saved Weights
[18:26:35.855790] [18:26:35.855790] Loaded Index: blocks.7.attn.qkv.weight from Saved WeightsLoaded Index: blocks.9.norm2.bias from Saved Weights[18:26:35.855791] 

Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights
[18:26:35.855794] Loaded Index: blocks.11.mlp.fc1.weight from Saved Weights
[18:26:35.855798] [18:26:35.855798] Loaded Index: blocks.7.attn.qkv.bias from Saved WeightsLoaded Index: blocks.9.mlp.fc1.weight from Saved Weights
[18:26:35.855799] 
Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights[18:26:35.855801] 
Loaded Index: blocks.11.mlp.fc1.bias from Saved Weights
[18:26:35.855806] [18:26:35.855806] Loaded Index: blocks.7.attn.proj.weight from Saved WeightsLoaded Index: blocks.9.mlp.fc1.bias from Saved Weights
[18:26:35.855808] 
[18:26:35.855809] Loaded Index: blocks.11.norm1.weight from Saved Weights
Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights
[18:26:35.855813] [18:26:35.855814] Loaded Index: blocks.7.attn.proj.bias from Saved Weights
Loaded Index: blocks.9.mlp.fc2.weight from Saved Weights[18:26:35.855815] 
[18:26:35.855816] Loaded Index: blocks.11.norm1.bias from Saved Weights
Loaded Index: blocks.11.mlp.fc2.bias from Saved Weights
[18:26:35.855820] Loaded Index: blocks.7.norm2.weight from Saved Weights
[18:26:35.855822] [18:26:35.855823] Loaded Index: blocks.9.mlp.fc2.bias from Saved Weights[18:26:35.855823] 
norm.weight not found in Init Model
Loaded Index: blocks.11.attn.qkv.weight from Saved Weights
[18:26:35.855827] Loaded Index: blocks.7.norm2.bias from Saved Weights
[18:26:35.855829] [18:26:35.855830] norm.bias not found in Init ModelLoaded Index: blocks.10.norm1.weight from Saved Weights
[18:26:35.855831] 
Loaded Index: blocks.11.attn.qkv.bias from Saved Weights
[18:26:35.855834] Loaded Index: blocks.7.mlp.fc1.weight from Saved Weights
[18:26:35.855836] decoder_embed.weight not found in Init Model[18:26:35.855838] 
Loaded Index: blocks.10.norm1.bias from Saved Weights[18:26:35.855839] 
Loaded Index: blocks.11.attn.proj.weight from Saved Weights
[18:26:35.855841] Loaded Index: blocks.7.mlp.fc1.bias from Saved Weights
[18:26:35.855843] decoder_embed.bias not found in Init Model
[18:26:35.855846] [18:26:35.855846] Loaded Index: blocks.10.attn.qkv.weight from Saved Weights
Loaded Index: blocks.11.attn.proj.bias from Saved Weights
[18:26:35.855848] [18:26:35.855849] Loaded Index: blocks.7.mlp.fc2.weight from Saved Weightsdecoder_blocks.0.norm1.weight not found in Init Model

[18:26:35.855853] [18:26:35.855854] Loaded Index: blocks.10.attn.qkv.bias from Saved Weights
Loaded Index: blocks.11.norm2.weight from Saved Weights[18:26:35.855855] 
[18:26:35.855856] decoder_blocks.0.norm1.bias not found in Init Model
Loaded Index: blocks.7.mlp.fc2.bias from Saved Weights
[18:26:35.855860] Loaded Index: blocks.10.attn.proj.weight from Saved Weights[18:26:35.855862] [18:26:35.855862] 
Loaded Index: blocks.11.norm2.bias from Saved Weightsdecoder_blocks.0.attn.qkv.weight not found in Init Model[18:26:35.855863] 

Loaded Index: blocks.8.norm1.weight from Saved Weights
[18:26:35.855868] Loaded Index: blocks.10.attn.proj.bias from Saved Weights[18:26:35.855869] 
decoder_blocks.0.attn.qkv.bias not found in Init Model[18:26:35.855870] 
[18:26:35.855871] Loaded Index: blocks.11.mlp.fc1.weight from Saved Weights
Loaded Index: blocks.8.norm1.bias from Saved Weights
[18:26:35.855875] [18:26:35.855876] Loaded Index: blocks.10.norm2.weight from Saved Weightsdecoder_blocks.0.attn.proj.weight not found in Init Model

[18:26:35.855879] [18:26:35.855880] Loaded Index: blocks.11.mlp.fc1.bias from Saved Weights
Loaded Index: blocks.8.attn.qkv.weight from Saved Weights
[18:26:35.855882] [18:26:35.855883] decoder_blocks.0.attn.proj.bias not found in Init Model
Loaded Index: blocks.10.norm2.bias from Saved Weights
[18:26:35.855886] [18:26:35.855887] Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights
Loaded Index: blocks.8.attn.qkv.bias from Saved Weights[18:26:35.855888] 
decoder_blocks.0.norm2.weight not found in Init Model
[18:26:35.855891] Loaded Index: blocks.10.mlp.fc1.weight from Saved Weights
[18:26:35.855894] [18:26:35.855894] Loaded Index: blocks.11.mlp.fc2.bias from Saved Weights[18:26:35.855894] Loaded Index: blocks.8.attn.proj.weight from Saved Weights
decoder_blocks.0.norm2.bias not found in Init Model

[18:26:35.855898] Loaded Index: blocks.10.mlp.fc1.bias from Saved Weights
[18:26:35.855901] [18:26:35.855902] norm.weight not found in Init Model[18:26:35.855902] 
decoder_blocks.0.mlp.fc1.weight not found in Init ModelLoaded Index: blocks.8.attn.proj.bias from Saved Weights

[18:26:35.855905] Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights
[18:26:35.855908] [18:26:35.855909] norm.bias not found in Init Model
decoder_blocks.0.mlp.fc1.bias not found in Init Model[18:26:35.855910] 
Loaded Index: blocks.8.norm2.weight from Saved Weights
[18:26:35.855912] Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights
[18:26:35.855914] decoder_embed.weight not found in Init Model[18:26:35.855915] 
decoder_blocks.0.mlp.fc2.weight not found in Init Model
[18:26:35.855918] Loaded Index: blocks.8.norm2.bias from Saved Weights
[18:26:35.855920] [18:26:35.855920] [18:26:35.855921] Loaded Index: blocks.11.norm1.weight from Saved Weightsdecoder_embed.bias not found in Init Modeldecoder_blocks.0.mlp.fc2.bias not found in Init Model


[18:26:35.855926] Loaded Index: blocks.8.mlp.fc1.weight from Saved Weights
[18:26:35.855928] [18:26:35.855928] decoder_blocks.0.norm1.weight not found in Init Model
decoder_blocks.1.norm1.weight not found in Init Model[18:26:35.855929] 
Loaded Index: blocks.11.norm1.bias from Saved Weights
[18:26:35.855933] Loaded Index: blocks.8.mlp.fc1.bias from Saved Weights[18:26:35.855934] 
[18:26:35.855935] decoder_blocks.0.norm1.bias not found in Init Modeldecoder_blocks.1.norm1.bias not found in Init Model

[18:26:35.855937] Loaded Index: blocks.11.attn.qkv.weight from Saved Weights
[18:26:35.855940] [18:26:35.855941] Loaded Index: blocks.8.mlp.fc2.weight from Saved Weights[18:26:35.855941] 
decoder_blocks.0.attn.qkv.weight not found in Init Modeldecoder_blocks.1.attn.qkv.weight not found in Init Model

[18:26:35.855944] Loaded Index: blocks.11.attn.qkv.bias from Saved Weights
[18:26:35.855948] [18:26:35.855948] [18:26:35.855948] decoder_blocks.0.attn.qkv.bias not found in Init ModelLoaded Index: blocks.8.mlp.fc2.bias from Saved Weights

decoder_blocks.1.attn.qkv.bias not found in Init Model
[18:26:35.855951] Loaded Index: blocks.11.attn.proj.weight from Saved Weights
[18:26:35.855955] decoder_blocks.0.attn.proj.weight not found in Init Model[18:26:35.855956] [18:26:35.855956] 
decoder_blocks.1.attn.proj.weight not found in Init ModelLoaded Index: blocks.9.norm1.weight from Saved Weights

[18:26:35.855959] Loaded Index: blocks.11.attn.proj.bias from Saved Weights
[18:26:35.855961] decoder_blocks.0.attn.proj.bias not found in Init Model
[18:26:35.855963] decoder_blocks.1.attn.proj.bias not found in Init Model
[18:26:35.855965] Loaded Index: blocks.9.norm1.bias from Saved Weights[18:26:35.855966] 
[18:26:35.855967] Loaded Index: blocks.11.norm2.weight from Saved Weights
decoder_blocks.0.norm2.weight not found in Init Model[18:26:35.855969] 
decoder_blocks.1.norm2.weight not found in Init Model
[18:26:35.855973] [18:26:35.855974] Loaded Index: blocks.9.attn.qkv.weight from Saved Weights[18:26:35.855974] 
decoder_blocks.0.norm2.bias not found in Init ModelLoaded Index: blocks.11.norm2.bias from Saved Weights[18:26:35.855975] 

decoder_blocks.1.norm2.bias not found in Init Model
[18:26:35.855980] [18:26:35.855981] Loaded Index: blocks.9.attn.qkv.bias from Saved Weightsdecoder_blocks.0.mlp.fc1.weight not found in Init Model
[18:26:35.855982] [18:26:35.855982] 
decoder_blocks.1.mlp.fc1.weight not found in Init ModelLoaded Index: blocks.11.mlp.fc1.weight from Saved Weights

[18:26:35.855988] [18:26:35.855988] decoder_blocks.0.mlp.fc1.bias not found in Init ModelLoaded Index: blocks.9.attn.proj.weight from Saved Weights
[18:26:35.855989] 
decoder_blocks.1.mlp.fc1.bias not found in Init Model[18:26:35.855991] 
Loaded Index: blocks.11.mlp.fc1.bias from Saved Weights
[18:26:35.855994] decoder_blocks.0.mlp.fc2.weight not found in Init Model
[18:26:35.855996] [18:26:35.855996] decoder_blocks.1.mlp.fc2.weight not found in Init ModelLoaded Index: blocks.9.attn.proj.bias from Saved Weights

[18:26:35.855998] Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights[18:26:35.856000] 
decoder_blocks.0.mlp.fc2.bias not found in Init Model
[18:26:35.856002] decoder_blocks.1.mlp.fc2.bias not found in Init Model[18:26:35.856003] 
Loaded Index: blocks.9.norm2.weight from Saved Weights
[18:26:35.856006] [18:26:35.856007] decoder_blocks.1.norm1.weight not found in Init ModelLoaded Index: blocks.11.mlp.fc2.bias from Saved Weights

[18:26:35.856008] decoder_norm.weight not found in Init Model
[18:26:35.856011] Loaded Index: blocks.9.norm2.bias from Saved Weights
[18:26:35.856013] [18:26:35.856014] decoder_blocks.1.norm1.bias not found in Init Model
norm.weight not found in Init Model[18:26:35.856015] 
decoder_norm.bias not found in Init Model
[18:26:35.856018] [18:26:35.856019] Loaded Index: blocks.9.mlp.fc1.weight from Saved Weights
decoder_blocks.1.attn.qkv.weight not found in Init Model
[18:26:35.856020] [18:26:35.856021] norm.bias not found in Init Modeldecoder_pred.weight not found in Init Model

[18:26:35.856026] [18:26:35.856026] decoder_blocks.1.attn.qkv.bias not found in Init ModelLoaded Index: blocks.9.mlp.fc1.bias from Saved Weights
[18:26:35.856027] [18:26:35.856028] 
decoder_embed.weight not found in Init Modeldecoder_pred.bias not found in Init Model

[18:26:35.856032] decoder_blocks.1.attn.proj.weight not found in Init Model
[18:26:35.856034] [18:26:35.856034] Loaded Index: blocks.9.mlp.fc2.weight from Saved Weightsdecoder_embed.bias not found in Init Model

[18:26:35.856038] decoder_blocks.1.attn.proj.bias not found in Init Model
[18:26:35.856041] [18:26:35.856042] decoder_blocks.0.norm1.weight not found in Init ModelLoaded Index: blocks.9.mlp.fc2.bias from Saved Weights
[18:26:35.856044] 
decoder_blocks.1.norm2.weight not found in Init Model
[18:26:35.856048] decoder_blocks.0.norm1.bias not found in Init Model
[18:26:35.856050] [18:26:35.856050] decoder_blocks.1.norm2.bias not found in Init ModelLoaded Index: blocks.10.norm1.weight from Saved Weights

[18:26:35.856054] decoder_blocks.0.attn.qkv.weight not found in Init Model
[18:26:35.856057] decoder_blocks.1.mlp.fc1.weight not found in Init Model
[18:26:35.856058] Loaded Index: blocks.10.norm1.bias from Saved Weights
[18:26:35.856060] decoder_blocks.0.attn.qkv.bias not found in Init Model
[18:26:35.856062] decoder_blocks.1.mlp.fc1.bias not found in Init Model
[18:26:35.856066] [18:26:35.856066] Loaded Index: blocks.10.attn.qkv.weight from Saved Weightsdecoder_blocks.0.attn.proj.weight not found in Init Model

[18:26:35.856068] decoder_blocks.1.mlp.fc2.weight not found in Init Model
[18:26:35.856072] [18:26:35.856073] decoder_blocks.0.attn.proj.bias not found in Init Model
[18:26:35.856074] Loaded Index: blocks.10.attn.qkv.bias from Saved Weights
decoder_blocks.1.mlp.fc2.bias not found in Init Model
[18:26:35.856079] decoder_blocks.0.norm2.weight not found in Init Model
[18:26:35.856080] [18:26:35.856081] decoder_norm.weight not found in Init ModelLoaded Index: blocks.10.attn.proj.weight from Saved Weights

[18:26:35.856085] decoder_blocks.0.norm2.bias not found in Init Model
[18:26:35.856088] decoder_norm.bias not found in Init Model[18:26:35.856089] 
Loaded Index: blocks.10.attn.proj.bias from Saved Weights
[18:26:35.856091] decoder_blocks.0.mlp.fc1.weight not found in Init Model
[18:26:35.856094] decoder_pred.weight not found in Init Model
[18:26:35.856096] [18:26:35.856097] Loaded Index: blocks.10.norm2.weight from Saved Weights
decoder_blocks.0.mlp.fc1.bias not found in Init Model
[18:26:35.856100] decoder_pred.bias not found in Init Model
[18:26:35.856103] [18:26:35.856104] decoder_blocks.0.mlp.fc2.weight not found in Init ModelLoaded Index: blocks.10.norm2.bias from Saved Weights

[18:26:35.856110] decoder_blocks.0.mlp.fc2.bias not found in Init Model[18:26:35.856111] 
Loaded Index: blocks.10.mlp.fc1.weight from Saved Weights
[18:26:35.856117] decoder_blocks.1.norm1.weight not found in Init Model
[18:26:35.856119] Loaded Index: blocks.10.mlp.fc1.bias from Saved Weights
[18:26:35.856123] decoder_blocks.1.norm1.bias not found in Init Model
[18:26:35.856127] Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights
[18:26:35.856129] decoder_blocks.1.attn.qkv.weight not found in Init Model
[18:26:35.856134] [18:26:35.856135] Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights
decoder_blocks.1.attn.qkv.bias not found in Init Model
[18:26:35.856141] [18:26:35.856142] decoder_blocks.1.attn.proj.weight not found in Init ModelLoaded Index: blocks.11.norm1.weight from Saved Weights

[18:26:35.856148] decoder_blocks.1.attn.proj.bias not found in Init Model[18:26:35.856149] 
Loaded Index: blocks.11.norm1.bias from Saved Weights
[18:26:35.856155] decoder_blocks.1.norm2.weight not found in Init Model
[18:26:35.856158] Loaded Index: blocks.11.attn.qkv.weight from Saved Weights
[18:26:35.856161] decoder_blocks.1.norm2.bias not found in Init Model
[18:26:35.856165] Loaded Index: blocks.11.attn.qkv.bias from Saved Weights
[18:26:35.856168] decoder_blocks.1.mlp.fc1.weight not found in Init Model
[18:26:35.856173] Loaded Index: blocks.11.attn.proj.weight from Saved Weights[18:26:35.856174] 
decoder_blocks.1.mlp.fc1.bias not found in Init Model
[18:26:35.856180] [18:26:35.856181] decoder_blocks.1.mlp.fc2.weight not found in Init ModelLoaded Index: blocks.11.attn.proj.bias from Saved Weights

[18:26:35.856188] decoder_blocks.1.mlp.fc2.bias not found in Init Model
[18:26:35.856189] Loaded Index: blocks.11.norm2.weight from Saved Weights
[18:26:35.856194] decoder_norm.weight not found in Init Model
[18:26:35.856197] Loaded Index: blocks.11.norm2.bias from Saved Weights
[18:26:35.856200] decoder_norm.bias not found in Init Model
[18:26:35.856205] Loaded Index: blocks.11.mlp.fc1.weight from Saved Weights[18:26:35.856206] 
decoder_pred.weight not found in Init Model
[18:26:35.856212] [18:26:35.856212] decoder_pred.bias not found in Init ModelLoaded Index: blocks.11.mlp.fc1.bias from Saved Weights

[18:26:35.856221] Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights
[18:26:35.856228] Loaded Index: blocks.11.mlp.fc2.bias from Saved Weights
[18:26:35.856235] norm.weight not found in Init Model
[18:26:35.856242] norm.bias not found in Init Model
[18:26:35.856248] decoder_embed.weight not found in Init Model
[18:26:35.856254] decoder_embed.bias not found in Init Model
[18:26:35.856261] decoder_blocks.0.norm1.weight not found in Init Model
[18:26:35.856267] decoder_blocks.0.norm1.bias not found in Init Model
[18:26:35.856274] decoder_blocks.0.attn.qkv.weight not found in Init Model
[18:26:35.856280] decoder_blocks.0.attn.qkv.bias not found in Init Model
[18:26:35.856287] decoder_blocks.0.attn.proj.weight not found in Init Model
[18:26:35.856293] decoder_blocks.0.attn.proj.bias not found in Init Model
[18:26:35.856300] decoder_blocks.0.norm2.weight not found in Init Model
[18:26:35.856306] decoder_blocks.0.norm2.bias not found in Init Model
[18:26:35.856312] decoder_blocks.0.mlp.fc1.weight not found in Init Model
[18:26:35.856318] decoder_blocks.0.mlp.fc1.bias not found in Init Model
[18:26:35.856324] decoder_blocks.0.mlp.fc2.weight not found in Init Model
[18:26:35.856330] decoder_blocks.0.mlp.fc2.bias not found in Init Model
[18:26:35.856335] decoder_blocks.1.norm1.weight not found in Init Model
[18:26:35.856341] decoder_blocks.1.norm1.bias not found in Init Model
[18:26:35.856347] decoder_blocks.1.attn.qkv.weight not found in Init Model
[18:26:35.856353] decoder_blocks.1.attn.qkv.bias not found in Init Model
[18:26:35.856359] decoder_blocks.1.attn.proj.weight not found in Init Model
[18:26:35.856365] decoder_blocks.1.attn.proj.bias not found in Init Model
[18:26:35.856370] decoder_blocks.1.norm2.weight not found in Init Model
[18:26:35.856376] decoder_blocks.1.norm2.bias not found in Init Model
[18:26:35.856383] decoder_blocks.1.mlp.fc1.weight not found in Init Model
[18:26:35.856388] decoder_blocks.1.mlp.fc1.bias not found in Init Model
[18:26:35.856394] decoder_blocks.1.mlp.fc2.weight not found in Init Model
[18:26:35.856401] decoder_blocks.1.mlp.fc2.bias not found in Init Model
[18:26:35.856407] decoder_norm.weight not found in Init Model
[18:26:35.856414] decoder_norm.bias not found in Init Model
[18:26:35.856420] decoder_pred.weight not found in Init Model
[18:26:35.856426] decoder_pred.bias not found in Init Model
[18:26:35.868018] Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth
[18:26:35.868171] Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth
[18:26:35.868234] Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth
[18:26:35.868990] Loaded Index: cls_token from Saved Weights
[18:26:35.869009] Loaded Index: pos_embed from Saved Weights
[18:26:35.869018] mask_token not found in Init Model
[18:26:35.869024] decoder_pos_embed not found in Init Model
[18:26:35.869032] Loaded Index: patch_embed.proj.weight from Saved Weights
[18:26:35.869039] Loaded Index: patch_embed.proj.bias from Saved Weights
[18:26:35.869047] Loaded Index: blocks.0.norm1.weight from Saved Weights
[18:26:35.869054] Loaded Index: blocks.0.norm1.bias from Saved Weights
[18:26:35.869061] Loaded Index: blocks.0.attn.qkv.weight from Saved Weights
[18:26:35.869068] Loaded Index: blocks.0.attn.qkv.bias from Saved Weights
[18:26:35.869075] Loaded Index: blocks.0.attn.proj.weight from Saved Weights
[18:26:35.869083] Loaded Index: blocks.0.attn.proj.bias from Saved Weights
[18:26:35.869090] Loaded Index: blocks.0.norm2.weight from Saved Weights
[18:26:35.869097] Loaded Index: blocks.0.norm2.bias from Saved Weights
[18:26:35.869104] Loaded Index: blocks.0.mlp.fc1.weight from Saved Weights
[18:26:35.869111] Loaded Index: blocks.0.mlp.fc1.bias from Saved Weights
[18:26:35.869117] Loaded Index: blocks.0.mlp.fc2.weight from Saved Weights
[18:26:35.869125] Loaded Index: blocks.0.mlp.fc2.bias from Saved Weights
[18:26:35.869131] Loaded Index: blocks.1.norm1.weight from Saved Weights
[18:26:35.869139] Loaded Index: blocks.1.norm1.bias from Saved Weights
[18:26:35.869146] Loaded Index: blocks.1.attn.qkv.weight from Saved Weights
[18:26:35.869155] Loaded Index: blocks.1.attn.qkv.bias from Saved Weights
[18:26:35.869162] Loaded Index: blocks.1.attn.proj.weight from Saved Weights
[18:26:35.869169] Loaded Index: blocks.1.attn.proj.bias from Saved Weights
[18:26:35.869166] Loaded Index: cls_token from Saved Weights
[18:26:35.869176] Loaded Index: blocks.1.norm2.weight from Saved Weights
[18:26:35.869183] [18:26:35.869184] Loaded Index: blocks.1.norm2.bias from Saved WeightsLoaded Index: pos_embed from Saved Weights

[18:26:35.869193] [18:26:35.869193] Loaded Index: blocks.1.mlp.fc1.weight from Saved Weightsmask_token not found in Init Model

[18:26:35.869200] [18:26:35.869200] decoder_pos_embed not found in Init ModelLoaded Index: blocks.1.mlp.fc1.bias from Saved Weights

[18:26:35.869208] [18:26:35.869209] Loaded Index: blocks.1.mlp.fc2.weight from Saved Weights
Loaded Index: patch_embed.proj.weight from Saved Weights
[18:26:35.869215] Loaded Index: blocks.1.mlp.fc2.bias from Saved Weights
[18:26:35.869217] Loaded Index: patch_embed.proj.bias from Saved Weights
[18:26:35.869222] Loaded Index: blocks.2.norm1.weight from Saved Weights
[18:26:35.869225] Loaded Index: blocks.0.norm1.weight from Saved Weights
[18:26:35.869230] Loaded Index: blocks.2.norm1.bias from Saved Weights
[18:26:35.869232] Loaded Index: blocks.0.norm1.bias from Saved Weights
[18:26:35.869237] Loaded Index: blocks.2.attn.qkv.weight from Saved Weights
[18:26:35.869240] Loaded Index: blocks.0.attn.qkv.weight from Saved Weights
[18:26:35.869244] Loaded Index: blocks.2.attn.qkv.bias from Saved Weights
[18:26:35.869247] Loaded Index: blocks.0.attn.qkv.bias from Saved Weights
[18:26:35.869251] Loaded Index: blocks.2.attn.proj.weight from Saved Weights
[18:26:35.869255] [18:26:35.869248] Loaded Index: blocks.0.attn.proj.weight from Saved Weights
Loaded Index: cls_token from Saved Weights[18:26:35.869258] 
Loaded Index: blocks.2.attn.proj.bias from Saved Weights
[18:26:35.869262] Loaded Index: blocks.0.attn.proj.bias from Saved Weights
[18:26:35.869266] [18:26:35.869266] Loaded Index: blocks.2.norm2.weight from Saved Weights
Loaded Index: pos_embed from Saved Weights
[18:26:35.869270] Loaded Index: blocks.0.norm2.weight from Saved Weights
[18:26:35.869273] [18:26:35.869274] Loaded Index: blocks.2.norm2.bias from Saved Weightsmask_token not found in Init Model

[18:26:35.869277] Loaded Index: blocks.0.norm2.bias from Saved Weights
[18:26:35.869281] [18:26:35.869281] decoder_pos_embed not found in Init ModelLoaded Index: blocks.2.mlp.fc1.weight from Saved Weights

[18:26:35.869285] Loaded Index: blocks.0.mlp.fc1.weight from Saved Weights
[18:26:35.869288] Loaded Index: blocks.2.mlp.fc1.bias from Saved Weights[18:26:35.869289] 
Loaded Index: patch_embed.proj.weight from Saved Weights
[18:26:35.869292] Loaded Index: blocks.0.mlp.fc1.bias from Saved Weights
[18:26:35.869296] Loaded Index: blocks.2.mlp.fc2.weight from Saved Weights
[18:26:35.869297] Loaded Index: patch_embed.proj.bias from Saved Weights
[18:26:35.869299] Loaded Index: blocks.0.mlp.fc2.weight from Saved Weights
[18:26:35.869303] Loaded Index: blocks.2.mlp.fc2.bias from Saved Weights
[18:26:35.869305] Loaded Index: blocks.0.norm1.weight from Saved Weights
[18:26:35.869307] Loaded Index: blocks.0.mlp.fc2.bias from Saved Weights
[18:26:35.869310] Loaded Index: blocks.3.norm1.weight from Saved Weights
[18:26:35.872244] Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth
[18:26:35.873258] Loaded Index: cls_token from Saved Weights
[18:26:35.873277] Loaded Index: pos_embed from Saved Weights
[18:26:35.873284] mask_token not found in Init Model
[18:26:35.873291] decoder_pos_embed not found in Init Model
[18:26:35.873299] Loaded Index: patch_embed.proj.weight from Saved Weights
[18:26:35.873307] Loaded Index: patch_embed.proj.bias from Saved Weights
[18:26:35.873315] Loaded Index: blocks.0.norm1.weight from Saved Weights
[18:26:35.873324] Loaded Index: blocks.0.norm1.bias from Saved Weights
[18:26:35.873331] Loaded Index: blocks.0.attn.qkv.weight from Saved Weights
[18:26:35.873339] Loaded Index: blocks.0.attn.qkv.bias from Saved Weights
[18:26:35.873347] Loaded Index: blocks.0.attn.proj.weight from Saved Weights
[18:26:35.873354] Loaded Index: blocks.0.attn.proj.bias from Saved Weights
[18:26:35.873362] Loaded Index: blocks.0.norm2.weight from Saved Weights
[18:26:35.873370] Loaded Index: blocks.0.norm2.bias from Saved Weights
[18:26:35.873378] Loaded Index: blocks.0.mlp.fc1.weight from Saved Weights
[18:26:35.873386] Loaded Index: blocks.0.mlp.fc1.bias from Saved Weights
[18:26:35.873394] Loaded Index: blocks.0.mlp.fc2.weight from Saved Weights
[18:26:35.873402] Loaded Index: blocks.0.mlp.fc2.bias from Saved Weights
[18:26:35.873409] Loaded Index: blocks.1.norm1.weight from Saved Weights
[18:26:35.873417] Loaded Index: blocks.1.norm1.bias from Saved Weights
[18:26:35.873424] Loaded Index: blocks.1.attn.qkv.weight from Saved Weights
[18:26:35.873432] Loaded Index: blocks.1.attn.qkv.bias from Saved Weights
[18:26:35.873440] Loaded Index: blocks.1.attn.proj.weight from Saved Weights
[18:26:35.873447] Loaded Index: blocks.1.attn.proj.bias from Saved Weights
[18:26:35.873455] Loaded Index: blocks.1.norm2.weight from Saved Weights
[18:26:35.873462] Loaded Index: blocks.1.norm2.bias from Saved Weights
[18:26:35.873470] Loaded Index: blocks.1.mlp.fc1.weight from Saved Weights
[18:26:35.873477] Loaded Index: blocks.1.mlp.fc1.bias from Saved Weights
[18:26:35.873485] Loaded Index: blocks.1.mlp.fc2.weight from Saved Weights
[18:26:35.873492] Loaded Index: blocks.1.mlp.fc2.bias from Saved Weights
[18:26:35.873499] Loaded Index: blocks.2.norm1.weight from Saved Weights
[18:26:35.873507] Loaded Index: blocks.2.norm1.bias from Saved Weights
[18:26:35.873514] Loaded Index: blocks.2.attn.qkv.weight from Saved Weights
[18:26:35.873522] Loaded Index: blocks.2.attn.qkv.bias from Saved Weights
[18:26:35.873529] Loaded Index: blocks.2.attn.proj.weight from Saved Weights
[18:26:35.873537] Loaded Index: blocks.2.attn.proj.bias from Saved Weights
[18:26:35.873544] Loaded Index: blocks.2.norm2.weight from Saved Weights
[18:26:35.873551] Loaded Index: blocks.2.norm2.bias from Saved Weights
[18:26:35.873558] Loaded Index: blocks.2.mlp.fc1.weight from Saved Weights
[18:26:35.873566] Loaded Index: blocks.2.mlp.fc1.bias from Saved Weights
[18:26:35.873574] Loaded Index: blocks.2.mlp.fc2.weight from Saved Weights
[18:26:35.873582] Loaded Index: blocks.2.mlp.fc2.bias from Saved Weights
[18:26:35.873590] Loaded Index: blocks.3.norm1.weight from Saved Weights
[18:26:35.873597] Loaded Index: blocks.3.norm1.bias from Saved Weights
[18:26:35.873605] Loaded Index: blocks.3.attn.qkv.weight from Saved Weights
[18:26:35.873612] Loaded Index: blocks.3.attn.qkv.bias from Saved Weights
[18:26:35.873620] Loaded Index: blocks.3.attn.proj.weight from Saved Weights
[18:26:35.873628] Loaded Index: blocks.3.attn.proj.bias from Saved Weights
[18:26:35.873635] Loaded Index: blocks.3.norm2.weight from Saved Weights
[18:26:35.873643] Loaded Index: blocks.3.norm2.bias from Saved Weights
[18:26:35.873651] Loaded Index: blocks.3.mlp.fc1.weight from Saved Weights
[18:26:35.873658] Loaded Index: blocks.3.mlp.fc1.bias from Saved Weights
[18:26:35.873666] Loaded Index: blocks.3.mlp.fc2.weight from Saved Weights
[18:26:35.873673] Loaded Index: blocks.3.mlp.fc2.bias from Saved Weights
[18:26:35.873680] Loaded Index: blocks.4.norm1.weight from Saved Weights
[18:26:35.873688] Loaded Index: blocks.4.norm1.bias from Saved Weights
[18:26:35.873696] Loaded Index: blocks.4.attn.qkv.weight from Saved Weights
[18:26:35.873704] Loaded Index: blocks.4.attn.qkv.bias from Saved Weights
[18:26:35.873711] Loaded Index: blocks.4.attn.proj.weight from Saved Weights
[18:26:35.873719] Loaded Index: blocks.4.attn.proj.bias from Saved Weights
[18:26:35.873727] Loaded Index: blocks.4.norm2.weight from Saved Weights
[18:26:35.873734] Loaded Index: blocks.4.norm2.bias from Saved Weights
[18:26:35.873741] Loaded Index: blocks.4.mlp.fc1.weight from Saved Weights
[18:26:35.873749] Loaded Index: blocks.4.mlp.fc1.bias from Saved Weights
[18:26:35.873756] Loaded Index: blocks.4.mlp.fc2.weight from Saved Weights
[18:26:35.873763] Loaded Index: blocks.4.mlp.fc2.bias from Saved Weights
[18:26:35.873771] Loaded Index: blocks.5.norm1.weight from Saved Weights
[18:26:35.873779] Loaded Index: blocks.5.norm1.bias from Saved Weights
[18:26:35.873793] Loaded Index: blocks.5.attn.qkv.weight from Saved Weights
[18:26:35.873801] Loaded Index: blocks.5.attn.qkv.bias from Saved Weights
[18:26:35.873809] Loaded Index: blocks.5.attn.proj.weight from Saved Weights
[18:26:35.873816] Loaded Index: blocks.5.attn.proj.bias from Saved Weights
[18:26:35.873824] Loaded Index: blocks.5.norm2.weight from Saved Weights
[18:26:35.873831] Loaded Index: blocks.5.norm2.bias from Saved Weights
[18:26:35.873839] Loaded Index: blocks.5.mlp.fc1.weight from Saved Weights
[18:26:35.873847] Loaded Index: blocks.5.mlp.fc1.bias from Saved Weights
[18:26:35.873854] Loaded Index: blocks.5.mlp.fc2.weight from Saved Weights
[18:26:35.873862] Loaded Index: blocks.5.mlp.fc2.bias from Saved Weights
[18:26:35.873869] Loaded Index: blocks.6.norm1.weight from Saved Weights
[18:26:35.873876] Loaded Index: blocks.6.norm1.bias from Saved Weights
[18:26:35.873884] Loaded Index: blocks.6.attn.qkv.weight from Saved Weights
[18:26:35.873892] Loaded Index: blocks.6.attn.qkv.bias from Saved Weights
[18:26:35.873899] Loaded Index: blocks.6.attn.proj.weight from Saved Weights
[18:26:35.873907] Loaded Index: blocks.6.attn.proj.bias from Saved Weights
[18:26:35.873915] Loaded Index: blocks.6.norm2.weight from Saved Weights
[18:26:35.873922] [18:26:35.873905] Loaded Index: blocks.6.norm2.bias from Saved Weights
Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth[18:26:35.873912] 
Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth[18:26:35.873930] 
Loaded Index: blocks.6.mlp.fc1.weight from Saved Weights
[18:26:35.873938] Loaded Index: blocks.6.mlp.fc1.bias from Saved Weights
[18:26:35.873945] Loaded Index: blocks.6.mlp.fc2.weight from Saved Weights
[18:26:35.873953] Loaded Index: blocks.6.mlp.fc2.bias from Saved Weights
[18:26:35.873961] Loaded Index: blocks.7.norm1.weight from Saved Weights
[18:26:35.873968] Loaded Index: blocks.7.norm1.bias from Saved Weights
[18:26:35.873975] Loaded Index: blocks.7.attn.qkv.weight from Saved Weights
[18:26:35.873983] Loaded Index: blocks.7.attn.qkv.bias from Saved Weights
[18:26:35.873990] Loaded Index: blocks.7.attn.proj.weight from Saved Weights
[18:26:35.873998] Loaded Index: blocks.7.attn.proj.bias from Saved Weights
[18:26:35.874005] Loaded Index: blocks.7.norm2.weight from Saved Weights
[18:26:35.874013] Loaded Index: blocks.7.norm2.bias from Saved Weights
[18:26:35.874021] Loaded Index: blocks.7.mlp.fc1.weight from Saved Weights
[18:26:35.874028] Loaded Index: blocks.7.mlp.fc1.bias from Saved Weights
[18:26:35.874036] Loaded Index: blocks.7.mlp.fc2.weight from Saved Weights
[18:26:35.874043] Loaded Index: blocks.7.mlp.fc2.bias from Saved Weights
[18:26:35.874050] Loaded Index: blocks.8.norm1.weight from Saved Weights
[18:26:35.874058] Loaded Index: blocks.8.norm1.bias from Saved Weights
[18:26:35.874066] Loaded Index: blocks.8.attn.qkv.weight from Saved Weights
[18:26:35.874073] Loaded Index: blocks.8.attn.qkv.bias from Saved Weights
[18:26:35.874080] Loaded Index: blocks.8.attn.proj.weight from Saved Weights
[18:26:35.874088] Loaded Index: blocks.8.attn.proj.bias from Saved Weights
[18:26:35.874095] Loaded Index: blocks.8.norm2.weight from Saved Weights
[18:26:35.874103] Loaded Index: blocks.8.norm2.bias from Saved Weights
[18:26:35.874110] Loaded Index: blocks.8.mlp.fc1.weight from Saved Weights
[18:26:35.874117] Loaded Index: blocks.8.mlp.fc1.bias from Saved Weights
[18:26:35.874125] Loaded Index: blocks.8.mlp.fc2.weight from Saved Weights
[18:26:35.874133] Loaded Index: blocks.8.mlp.fc2.bias from Saved Weights
[18:26:35.874140] Loaded Index: blocks.9.norm1.weight from Saved Weights
[18:26:35.874147] Loaded Index: blocks.9.norm1.bias from Saved Weights
[18:26:35.874155] Loaded Index: blocks.9.attn.qkv.weight from Saved Weights
[18:26:35.874162] Loaded Index: blocks.9.attn.qkv.bias from Saved Weights
[18:26:35.874170] Loaded Index: blocks.9.attn.proj.weight from Saved Weights
[18:26:35.874178] Loaded Index: blocks.9.attn.proj.bias from Saved Weights
[18:26:35.874185] Loaded Index: blocks.9.norm2.weight from Saved Weights
[18:26:35.874193] Loaded Index: blocks.9.norm2.bias from Saved Weights
[18:26:35.874200] Loaded Index: blocks.9.mlp.fc1.weight from Saved Weights
[18:26:35.874208] Loaded Index: blocks.9.mlp.fc1.bias from Saved Weights
[18:26:35.874215] Loaded Index: blocks.9.mlp.fc2.weight from Saved Weights
[18:26:35.874223] Loaded Index: blocks.9.mlp.fc2.bias from Saved Weights
[18:26:35.874230] Loaded Index: blocks.10.norm1.weight from Saved Weights
[18:26:35.874238] Loaded Index: blocks.10.norm1.bias from Saved Weights
[18:26:35.874246] Loaded Index: blocks.10.attn.qkv.weight from Saved Weights
[18:26:35.874254] Loaded Index: blocks.10.attn.qkv.bias from Saved Weights
[18:26:35.874261] Loaded Index: blocks.10.attn.proj.weight from Saved Weights
[18:26:35.874269] Loaded Index: blocks.10.attn.proj.bias from Saved Weights
[18:26:35.874277] Loaded Index: blocks.10.norm2.weight from Saved Weights
[18:26:35.874285] Loaded Index: blocks.10.norm2.bias from Saved Weights
[18:26:35.874292] Loaded Index: blocks.10.mlp.fc1.weight from Saved Weights
[18:26:35.874300] Loaded Index: blocks.10.mlp.fc1.bias from Saved Weights
[18:26:35.874308] Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights
[18:26:35.874316] Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights
[18:26:35.874323] Loaded Index: blocks.11.norm1.weight from Saved Weights
[18:26:35.874330] Loaded Index: blocks.11.norm1.bias from Saved Weights
[18:26:35.874338] Loaded Index: blocks.11.attn.qkv.weight from Saved Weights
[18:26:35.874346] Loaded Index: blocks.11.attn.qkv.bias from Saved Weights
[18:26:35.874353] Loaded Index: blocks.11.attn.proj.weight from Saved Weights
[18:26:35.874361] Loaded Index: blocks.11.attn.proj.bias from Saved Weights
[18:26:35.874369] Loaded Index: blocks.11.norm2.weight from Saved Weights
[18:26:35.874376] Loaded Index: blocks.11.norm2.bias from Saved Weights
[18:26:35.874384] Loaded Index: blocks.11.mlp.fc1.weight from Saved Weights
[18:26:35.874392] Loaded Index: blocks.11.mlp.fc1.bias from Saved Weights
[18:26:35.874399] Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights
[18:26:35.874407] Loaded Index: blocks.11.mlp.fc2.bias from Saved Weights
[18:26:35.874414] norm.weight not found in Init Model
[18:26:35.874420] norm.bias not found in Init Model
[18:26:35.874426] decoder_embed.weight not found in Init Model
[18:26:35.874432] decoder_embed.bias not found in Init Model
[18:26:35.874439] decoder_blocks.0.norm1.weight not found in Init Model
[18:26:35.874446] decoder_blocks.0.norm1.bias not found in Init Model
[18:26:35.874452] decoder_blocks.0.attn.qkv.weight not found in Init Model
[18:26:35.874458] decoder_blocks.0.attn.qkv.bias not found in Init Model
[18:26:35.874465] decoder_blocks.0.attn.proj.weight not found in Init Model
[18:26:35.874471] decoder_blocks.0.attn.proj.bias not found in Init Model
[18:26:35.874477] decoder_blocks.0.norm2.weight not found in Init Model
[18:26:35.874483] decoder_blocks.0.norm2.bias not found in Init Model
[18:26:35.874490] decoder_blocks.0.mlp.fc1.weight not found in Init Model
[18:26:35.874496] decoder_blocks.0.mlp.fc1.bias not found in Init Model
[18:26:35.874503] decoder_blocks.0.mlp.fc2.weight not found in Init Model
[18:26:35.874509] decoder_blocks.0.mlp.fc2.bias not found in Init Model
[18:26:35.874515] decoder_blocks.1.norm1.weight not found in Init Model
[18:26:35.874521] decoder_blocks.1.norm1.bias not found in Init Model
[18:26:35.874527] decoder_blocks.1.attn.qkv.weight not found in Init Model
[18:26:35.874534] decoder_blocks.1.attn.qkv.bias not found in Init Model
[18:26:35.874540] decoder_blocks.1.attn.proj.weight not found in Init Model
[18:26:35.874546] decoder_blocks.1.attn.proj.bias not found in Init Model
[18:26:35.874553] decoder_blocks.1.norm2.weight not found in Init Model
[18:26:35.874559] decoder_blocks.1.norm2.bias not found in Init Model
[18:26:35.874565] decoder_blocks.1.mlp.fc1.weight not found in Init Model
[18:26:35.874571] decoder_blocks.1.mlp.fc1.bias not found in Init Model
[18:26:35.874577] decoder_blocks.1.mlp.fc2.weight not found in Init Model
[18:26:35.874584] decoder_blocks.1.mlp.fc2.bias not found in Init Model
[18:26:35.874590] decoder_norm.weight not found in Init Model
[18:26:35.874596] decoder_norm.bias not found in Init Model
[18:26:35.874602] decoder_pred.weight not found in Init Model
[18:26:35.874609] decoder_pred.bias not found in Init Model
[18:26:35.874908] Loaded Index: cls_token from Saved Weights
[18:26:35.874927] Loaded Index: pos_embed from Saved Weights
[18:26:35.874935] mask_token not found in Init Model
[18:26:35.874931] Loaded Index: cls_token from Saved Weights
[18:26:35.874942] decoder_pos_embed not found in Init Model
[18:26:35.874948] Loaded Index: pos_embed from Saved Weights[18:26:35.874950] 
Loaded Index: patch_embed.proj.weight from Saved Weights
[18:26:35.874957] mask_token not found in Init Model
[18:26:35.874960] Loaded Index: patch_embed.proj.bias from Saved Weights
[18:26:35.874964] decoder_pos_embed not found in Init Model
[18:26:35.874967] Loaded Index: blocks.0.norm1.weight from Saved Weights
[18:26:35.874972] Loaded Index: patch_embed.proj.weight from Saved Weights[18:26:35.874974] 
Loaded Index: blocks.0.norm1.bias from Saved Weights
[18:26:35.874981] Loaded Index: patch_embed.proj.bias from Saved Weights
[18:26:35.874983] Loaded Index: blocks.0.attn.qkv.weight from Saved Weights
[18:26:35.874989] Loaded Index: blocks.0.norm1.weight from Saved Weights
[18:26:35.874991] Loaded Index: blocks.0.attn.qkv.bias from Saved Weights
[18:26:35.874996] Loaded Index: blocks.0.norm1.bias from Saved Weights
[18:26:35.874998] Loaded Index: blocks.0.attn.proj.weight from Saved Weights
[18:26:35.875004] Loaded Index: blocks.0.attn.qkv.weight from Saved Weights
[18:26:35.875006] Loaded Index: blocks.0.attn.proj.bias from Saved Weights
[18:26:35.875012] Loaded Index: blocks.0.attn.qkv.bias from Saved Weights
[18:26:35.875013] Loaded Index: blocks.0.norm2.weight from Saved Weights
[18:26:35.875019] Loaded Index: blocks.0.attn.proj.weight from Saved Weights[18:26:35.875021] 
Loaded Index: blocks.0.norm2.bias from Saved Weights
[18:26:35.875027] Loaded Index: blocks.0.attn.proj.bias from Saved Weights[18:26:35.875028] 
Loaded Index: blocks.0.mlp.fc1.weight from Saved Weights
[18:26:35.875035] Loaded Index: blocks.0.norm2.weight from Saved Weights
[18:26:35.875037] Loaded Index: blocks.0.mlp.fc1.bias from Saved Weights
[18:26:35.875042] Loaded Index: blocks.0.norm2.bias from Saved Weights
[18:26:35.875044] Loaded Index: blocks.0.mlp.fc2.weight from Saved Weights
[18:26:35.875049] Loaded Index: blocks.0.mlp.fc1.weight from Saved Weights
[18:26:35.875051] Loaded Index: blocks.0.mlp.fc2.bias from Saved Weights
[18:26:35.875056] Loaded Index: blocks.0.mlp.fc1.bias from Saved Weights
[18:26:35.875059] Loaded Index: blocks.1.norm1.weight from Saved Weights
[18:26:35.875064] Loaded Index: blocks.0.mlp.fc2.weight from Saved Weights
[18:26:35.875066] Loaded Index: blocks.1.norm1.bias from Saved Weights
[18:26:35.875071] Loaded Index: blocks.0.mlp.fc2.bias from Saved Weights
[18:26:35.875073] Loaded Index: blocks.1.attn.qkv.weight from Saved Weights
[18:26:35.875078] Loaded Index: blocks.1.norm1.weight from Saved Weights
[18:26:35.875080] Loaded Index: blocks.1.attn.qkv.bias from Saved Weights
[18:26:35.875085] Loaded Index: blocks.1.norm1.bias from Saved Weights
[18:26:35.875088] Loaded Index: blocks.1.attn.proj.weight from Saved Weights
[18:26:35.875093] Loaded Index: blocks.1.attn.qkv.weight from Saved Weights
[18:26:35.875095] Loaded Index: blocks.1.attn.proj.bias from Saved Weights
[18:26:35.875100] Loaded Index: blocks.1.attn.qkv.bias from Saved Weights
[18:26:35.875103] Loaded Index: blocks.1.norm2.weight from Saved Weights
[18:26:35.875107] Loaded Index: blocks.1.attn.proj.weight from Saved Weights
[18:26:35.875110] Loaded Index: blocks.1.norm2.bias from Saved Weights
[18:26:35.875114] Loaded Index: blocks.1.attn.proj.bias from Saved Weights
[18:26:35.875117] Loaded Index: blocks.1.mlp.fc1.weight from Saved Weights
[18:26:35.875121] Loaded Index: blocks.1.norm2.weight from Saved Weights
[18:26:35.875124] Loaded Index: blocks.1.mlp.fc1.bias from Saved Weights
[18:26:35.875128] Loaded Index: blocks.1.norm2.bias from Saved Weights
[18:26:35.875131] Loaded Index: blocks.1.mlp.fc2.weight from Saved Weights
[18:26:35.875136] Loaded Index: blocks.1.mlp.fc1.weight from Saved Weights
[18:26:35.875138] Loaded Index: blocks.1.mlp.fc2.bias from Saved Weights
[18:26:35.875142] Loaded Index: blocks.1.mlp.fc1.bias from Saved Weights
[18:26:35.875145] Loaded Index: blocks.2.norm1.weight from Saved Weights
[18:26:35.875149] Loaded Index: blocks.1.mlp.fc2.weight from Saved Weights
[18:26:35.875152] Loaded Index: blocks.2.norm1.bias from Saved Weights
[18:26:35.875157] Loaded Index: blocks.1.mlp.fc2.bias from Saved Weights
[18:26:35.875159] Loaded Index: blocks.2.attn.qkv.weight from Saved Weights
[18:26:35.875164] Loaded Index: blocks.2.norm1.weight from Saved Weights
[18:26:35.875166] Loaded Index: blocks.2.attn.qkv.bias from Saved Weights
[18:26:35.875171] Loaded Index: blocks.2.norm1.bias from Saved Weights
[18:26:35.875173] Loaded Index: blocks.2.attn.proj.weight from Saved Weights
[18:26:35.875179] Loaded Index: blocks.2.attn.qkv.weight from Saved Weights
[18:26:35.875180] Loaded Index: blocks.2.attn.proj.bias from Saved Weights
[18:26:35.875186] Loaded Index: blocks.2.attn.qkv.bias from Saved Weights
[18:26:35.875188] Loaded Index: blocks.2.norm2.weight from Saved Weights
[18:26:35.875193] Loaded Index: blocks.2.attn.proj.weight from Saved Weights
[18:26:35.875196] Loaded Index: blocks.2.norm2.bias from Saved Weights
[18:26:35.875200] Loaded Index: blocks.2.attn.proj.bias from Saved Weights
[18:26:35.875203] Loaded Index: blocks.2.mlp.fc1.weight from Saved Weights
[18:26:35.875207] Loaded Index: blocks.2.norm2.weight from Saved Weights
[18:26:35.875210] Loaded Index: blocks.2.mlp.fc1.bias from Saved Weights
[18:26:35.875214] Loaded Index: blocks.2.norm2.bias from Saved Weights
[18:26:35.875217] Loaded Index: blocks.2.mlp.fc2.weight from Saved Weights
[18:26:35.875221] Loaded Index: blocks.2.mlp.fc1.weight from Saved Weights
[18:26:35.875224] Loaded Index: blocks.2.mlp.fc2.bias from Saved Weights
[18:26:35.875228] Loaded Index: blocks.2.mlp.fc1.bias from Saved Weights
[18:26:35.875231] Loaded Index: blocks.3.norm1.weight from Saved Weights
[18:26:35.875235] Loaded Index: blocks.2.mlp.fc2.weight from Saved Weights
[18:26:35.875238] Loaded Index: blocks.3.norm1.bias from Saved Weights
[18:26:35.875242] Loaded Index: blocks.2.mlp.fc2.bias from Saved Weights
[18:26:35.875245] Loaded Index: blocks.3.attn.qkv.weight from Saved Weights
[18:26:35.875249] Loaded Index: blocks.3.norm1.weight from Saved Weights
[18:26:35.875252] Loaded Index: blocks.3.attn.qkv.bias from Saved Weights
[18:26:35.875256] Loaded Index: blocks.3.norm1.bias from Saved Weights
[18:26:35.875260] Loaded Index: blocks.3.attn.proj.weight from Saved Weights
[18:26:35.875263] Loaded Index: blocks.3.attn.qkv.weight from Saved Weights
[18:26:35.875267] Loaded Index: blocks.3.attn.proj.bias from Saved Weights
[18:26:35.875270] Loaded Index: blocks.3.attn.qkv.bias from Saved Weights
[18:26:35.875275] Loaded Index: blocks.3.norm2.weight from Saved Weights
[18:26:35.875278] Loaded Index: blocks.3.attn.proj.weight from Saved Weights
[18:26:35.875282] Loaded Index: blocks.3.norm2.bias from Saved Weights
[18:26:35.875285] Loaded Index: blocks.3.attn.proj.bias from Saved Weights
[18:26:35.875289] Loaded Index: blocks.3.mlp.fc1.weight from Saved Weights
[18:26:35.875292] Loaded Index: blocks.3.norm2.weight from Saved Weights
[18:26:35.875296] Loaded Index: blocks.3.mlp.fc1.bias from Saved Weights
[18:26:35.875299] Loaded Index: blocks.3.norm2.bias from Saved Weights
[18:26:35.875303] Loaded Index: blocks.3.mlp.fc2.weight from Saved Weights
[18:26:35.875306] Loaded Index: blocks.3.mlp.fc1.weight from Saved Weights
[18:26:35.875311] Loaded Index: blocks.3.mlp.fc2.bias from Saved Weights
[18:26:35.875313] Loaded Index: blocks.3.mlp.fc1.bias from Saved Weights
[18:26:35.875318] Loaded Index: blocks.4.norm1.weight from Saved Weights
[18:26:35.875321] Loaded Index: blocks.3.mlp.fc2.weight from Saved Weights
[18:26:35.875326] Loaded Index: blocks.4.norm1.bias from Saved Weights
[18:26:35.875328] Loaded Index: blocks.3.mlp.fc2.bias from Saved Weights
[18:26:35.875333] Loaded Index: blocks.4.attn.qkv.weight from Saved Weights
[18:26:35.875335] Loaded Index: blocks.4.norm1.weight from Saved Weights
[18:26:35.875340] Loaded Index: blocks.4.attn.qkv.bias from Saved Weights
[18:26:35.875342] Loaded Index: blocks.4.norm1.bias from Saved Weights
[18:26:35.875347] Loaded Index: blocks.4.attn.proj.weight from Saved Weights
[18:26:35.875350] Loaded Index: blocks.4.attn.qkv.weight from Saved Weights
[18:26:35.875354] Loaded Index: blocks.4.attn.proj.bias from Saved Weights
[18:26:35.875357] Loaded Index: blocks.4.attn.qkv.bias from Saved Weights
[18:26:35.875361] Loaded Index: blocks.4.norm2.weight from Saved Weights
[18:26:35.875364] Loaded Index: blocks.4.attn.proj.weight from Saved Weights
[18:26:35.875368] Loaded Index: blocks.4.norm2.bias from Saved Weights
[18:26:35.875371] Loaded Index: blocks.4.attn.proj.bias from Saved Weights
[18:26:35.875376] Loaded Index: blocks.4.mlp.fc1.weight from Saved Weights
[18:26:35.875378] Loaded Index: blocks.4.norm2.weight from Saved Weights
[18:26:35.875383] Loaded Index: blocks.4.mlp.fc1.bias from Saved Weights
[18:26:35.875385] Loaded Index: blocks.4.norm2.bias from Saved Weights
[18:26:35.875391] Loaded Index: blocks.4.mlp.fc2.weight from Saved Weights
[18:26:35.875394] Loaded Index: blocks.4.mlp.fc1.weight from Saved Weights
[18:26:35.875399] Loaded Index: blocks.4.mlp.fc2.bias from Saved Weights[18:26:35.875401] 
Loaded Index: blocks.4.mlp.fc1.bias from Saved Weights
[18:26:35.875407] Loaded Index: blocks.5.norm1.weight from Saved Weights[18:26:35.875408] 
Loaded Index: blocks.4.mlp.fc2.weight from Saved Weights
[18:26:35.875414] Loaded Index: blocks.5.norm1.bias from Saved Weights
[18:26:35.875416] Loaded Index: blocks.4.mlp.fc2.bias from Saved Weights
[18:26:35.875421] Loaded Index: blocks.5.attn.qkv.weight from Saved Weights
[18:26:35.875432] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decod[18:26:35.875423] Loaded Index: blocks.5.norm1.weight from Saved Weights
[18:26:35.875428] Loaded Index: blocks.5.attn.qkv.bias from Saved Weights
[18:26:35.875431] Loaded Index: blocks.5.norm1.bias from Saved Weights
[18:26:35.875435] Loaded Index: blocks.5.attn.proj.weight from Saved Weights
[18:26:35.875438] Loaded Index: blocks.5.attn.qkv.weight from Saved Weights
[18:26:35.875443] Loaded Index: blocks.5.attn.proj.bias from Saved Weights[18:26:35.875445] 
Loaded Index: blocks.5.attn.qkv.bias from Saved Weights
[18:26:35.875451] [18:26:35.875452] Loaded Index: blocks.5.norm2.weight from Saved WeightsLoaded Index: blocks.5.attn.proj.weight from Saved Weights

[18:26:35.875459] [18:26:35.875460] Loaded Index: blocks.5.norm2.bias from Saved Weights
Loaded Index: blocks.5.attn.proj.bias from Saved Weights
[18:26:35.875466] [18:26:35.875467] Loaded Index: blocks.5.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.5.norm2.weight from Saved Weights

[18:26:35.875474] [18:26:35.875474] Loaded Index: blocks.5.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.5.norm2.bias from Saved Weights

[18:26:35.875482] [18:26:35.875482] Loaded Index: blocks.5.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.5.mlp.fc1.weight from Saved Weights

[18:26:35.875490] [18:26:35.875490] Loaded Index: blocks.5.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.5.mlp.fc1.bias from Saved Weights

[18:26:35.875499] [18:26:35.875499] Loaded Index: blocks.6.norm1.weight from Saved WeightsLoaded Index: blocks.5.mlp.fc2.weight from Saved Weights

[18:26:35.875507] [18:26:35.875507] Loaded Index: blocks.6.norm1.bias from Saved WeightsLoaded Index: blocks.5.mlp.fc2.bias from Saved Weights

er_blocks.1.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[18:26:35.875515] [18:26:35.875515] Loaded Index: blocks.6.attn.qkv.weight from Saved WeightsLoaded Index: blocks.6.norm1.weight from Saved Weights

[18:26:35.875464] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decod[18:26:35.875523] [18:26:35.875523] Loaded Index: blocks.6.attn.qkv.bias from Saved WeightsLoaded Index: blocks.6.norm1.bias from Saved Weights

er_blocks.1.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[18:26:35.875531] [18:26:35.875531] Loaded Index: blocks.6.attn.proj.weight from Saved WeightsLoaded Index: blocks.6.attn.qkv.weight from Saved Weights

[18:26:35.875540] [18:26:35.875541] Loaded Index: blocks.6.attn.proj.bias from Saved WeightsLoaded Index: blocks.6.attn.qkv.bias from Saved Weights

[18:26:35.875548] [18:26:35.875549] Loaded Index: blocks.6.norm2.weight from Saved WeightsLoaded Index: blocks.6.attn.proj.weight from Saved Weights

[18:26:35.875556] [18:26:35.875557] Loaded Index: blocks.6.norm2.bias from Saved WeightsLoaded Index: blocks.6.attn.proj.bias from Saved Weights

[18:26:35.875564] [18:26:35.875564] Loaded Index: blocks.6.norm2.weight from Saved WeightsLoaded Index: blocks.6.mlp.fc1.weight from Saved Weights

[18:26:35.875572] [18:26:35.875572] Loaded Index: blocks.6.norm2.bias from Saved WeightsLoaded Index: blocks.6.mlp.fc1.bias from Saved Weights

[18:26:35.875581] [18:26:35.875581] Loaded Index: blocks.6.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.6.mlp.fc2.weight from Saved Weights

[18:26:35.875589] [18:26:35.875589] Loaded Index: blocks.6.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.6.mlp.fc2.bias from Saved Weights

[18:26:35.875598] [18:26:35.875598] Loaded Index: blocks.6.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.7.norm1.weight from Saved Weights

[18:26:35.875605] [18:26:35.875605] Loaded Index: blocks.6.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.7.norm1.bias from Saved Weights

[18:26:35.875614] [18:26:35.875614] Loaded Index: blocks.7.norm1.weight from Saved WeightsLoaded Index: blocks.7.attn.qkv.weight from Saved Weights

[18:26:35.875622] [18:26:35.875623] Loaded Index: blocks.7.norm1.bias from Saved WeightsLoaded Index: blocks.7.attn.qkv.bias from Saved Weights

[18:26:35.875631] [18:26:35.875631] Loaded Index: blocks.7.attn.qkv.weight from Saved WeightsLoaded Index: blocks.7.attn.proj.weight from Saved Weights

[18:26:35.875639] [18:26:35.875639] Loaded Index: blocks.7.attn.qkv.bias from Saved WeightsLoaded Index: blocks.7.attn.proj.bias from Saved Weights

[18:26:35.875647] [18:26:35.875648] Loaded Index: blocks.7.attn.proj.weight from Saved WeightsLoaded Index: blocks.7.norm2.weight from Saved Weights

[18:26:35.875655] [18:26:35.875656] Loaded Index: blocks.7.attn.proj.bias from Saved Weights
[18:26:35.875564] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decodLoaded Index: blocks.7.norm2.bias from Saved Weights
er_blocks.1.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[18:26:35.875663] Loaded Index: blocks.7.norm2.weight from Saved Weights[18:26:35.875664] 
Loaded Index: blocks.7.mlp.fc1.weight from Saved Weights
[18:26:35.875670] Loaded Index: blocks.7.norm2.bias from Saved Weights[18:26:35.875671] 
Loaded Index: blocks.7.mlp.fc1.bias from Saved Weights
[18:26:35.875678] [18:26:35.875679] Loaded Index: blocks.7.mlp.fc1.weight from Saved Weights
Loaded Index: blocks.7.mlp.fc2.weight from Saved Weights
[18:26:35.875685] [18:26:35.875686] Loaded Index: blocks.7.mlp.fc1.bias from Saved Weights
Loaded Index: blocks.7.mlp.fc2.bias from Saved Weights
[18:26:35.875693] [18:26:35.875694] Loaded Index: blocks.7.mlp.fc2.weight from Saved Weights
Loaded Index: blocks.8.norm1.weight from Saved Weights
[18:26:35.875701] [18:26:35.875701] Loaded Index: blocks.7.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.8.norm1.bias from Saved Weights

[18:26:35.875710] [18:26:35.875710] Loaded Index: blocks.8.norm1.weight from Saved WeightsLoaded Index: blocks.8.attn.qkv.weight from Saved Weights

[18:26:35.875718] [18:26:35.875718] Loaded Index: blocks.8.norm1.bias from Saved WeightsLoaded Index: blocks.8.attn.qkv.bias from Saved Weights

[18:26:35.875727] [18:26:35.875727] Loaded Index: blocks.8.attn.qkv.weight from Saved WeightsLoaded Index: blocks.8.attn.proj.weight from Saved Weights

[18:26:35.875736] [18:26:35.875736] Loaded Index: blocks.8.attn.qkv.bias from Saved WeightsLoaded Index: blocks.8.attn.proj.bias from Saved Weights

[18:26:35.875743] [18:26:35.875744] Loaded Index: blocks.8.attn.proj.weight from Saved WeightsLoaded Index: blocks.8.norm2.weight from Saved Weights

[18:26:35.875752] [18:26:35.875752] Loaded Index: blocks.8.attn.proj.bias from Saved WeightsLoaded Index: blocks.8.norm2.bias from Saved Weights

[18:26:35.875759] [18:26:35.875759] Loaded Index: blocks.8.norm2.weight from Saved WeightsLoaded Index: blocks.8.mlp.fc1.weight from Saved Weights

[18:26:35.875767] [18:26:35.875767] Loaded Index: blocks.8.norm2.bias from Saved WeightsLoaded Index: blocks.8.mlp.fc1.bias from Saved Weights

[18:26:35.875775] [18:26:35.875775] Loaded Index: blocks.8.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.8.mlp.fc2.weight from Saved Weights

[18:26:35.875784] [18:26:35.875784] Loaded Index: blocks.8.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.8.mlp.fc2.bias from Saved Weights

[18:26:35.875793] [18:26:35.875793] Loaded Index: blocks.8.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.9.norm1.weight from Saved Weights

[18:26:35.875801] [18:26:35.875802] Loaded Index: blocks.8.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.9.norm1.bias from Saved Weights

[18:26:35.875809] [18:26:35.875809] Loaded Index: blocks.9.norm1.weight from Saved WeightsLoaded Index: blocks.9.attn.qkv.weight from Saved Weights

[18:26:35.875818] [18:26:35.875818] Loaded Index: blocks.9.norm1.bias from Saved WeightsLoaded Index: blocks.9.attn.qkv.bias from Saved Weights

[18:26:35.875825] [18:26:35.875826] Loaded Index: blocks.9.attn.qkv.weight from Saved WeightsLoaded Index: blocks.9.attn.proj.weight from Saved Weights

[18:26:35.875833] [18:26:35.875834] Loaded Index: blocks.9.attn.qkv.bias from Saved WeightsLoaded Index: blocks.9.attn.proj.bias from Saved Weights

[18:26:35.875841] [18:26:35.875841] Loaded Index: blocks.9.attn.proj.weight from Saved WeightsLoaded Index: blocks.9.norm2.weight from Saved Weights

[18:26:35.875850] [18:26:35.875850] Loaded Index: blocks.9.attn.proj.bias from Saved WeightsLoaded Index: blocks.9.norm2.bias from Saved Weights

[18:26:35.875858] [18:26:35.875859] Loaded Index: blocks.9.norm2.weight from Saved WeightsLoaded Index: blocks.9.mlp.fc1.weight from Saved Weights

[18:26:35.875867] [18:26:35.875867] Loaded Index: blocks.9.norm2.bias from Saved WeightsLoaded Index: blocks.9.mlp.fc1.bias from Saved Weights

[18:26:35.875875] [18:26:35.875875] Loaded Index: blocks.9.mlp.fc1.weight from Saved Weights
Loaded Index: blocks.9.mlp.fc2.weight from Saved Weights
[18:26:35.875882] Loaded Index: blocks.9.mlp.fc1.bias from Saved Weights[18:26:35.875883] 
Loaded Index: blocks.9.mlp.fc2.bias from Saved Weights
[18:26:35.875890] [18:26:35.875891] Loaded Index: blocks.9.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.10.norm1.weight from Saved Weights

[18:26:35.875898] Loaded Index: blocks.9.mlp.fc2.bias from Saved Weights[18:26:35.875899] 
Loaded Index: blocks.10.norm1.bias from Saved Weights
[18:26:35.875906] Loaded Index: blocks.10.norm1.weight from Saved Weights
[18:26:35.875907] Loaded Index: blocks.10.attn.qkv.weight from Saved Weights
[18:26:35.875913] Loaded Index: blocks.10.norm1.bias from Saved Weights
[18:26:35.875915] Loaded Index: blocks.10.attn.qkv.bias from Saved Weights
[18:26:35.875920] Loaded Index: blocks.10.attn.qkv.weight from Saved Weights
[18:26:35.875922] Loaded Index: blocks.10.attn.proj.weight from Saved Weights
[18:26:35.875927] Loaded Index: blocks.10.attn.qkv.bias from Saved Weights
[18:26:35.875930] Loaded Index: blocks.10.attn.proj.bias from Saved Weights
[18:26:35.875935] Loaded Index: blocks.10.attn.proj.weight from Saved Weights
[18:26:35.875937] Loaded Index: blocks.10.norm2.weight from Saved Weights
[18:26:35.875942] Loaded Index: blocks.10.attn.proj.bias from Saved Weights
[18:26:35.875945] Loaded Index: blocks.10.norm2.bias from Saved Weights
[18:26:35.875948] Loaded Index: blocks.10.norm2.weight from Saved Weights
[18:26:35.875952] Loaded Index: blocks.10.mlp.fc1.weight from Saved Weights
[18:26:35.875956] Loaded Index: blocks.10.norm2.bias from Saved Weights
[18:26:35.875959] Loaded Index: blocks.10.mlp.fc1.bias from Saved Weights
[18:26:35.875963] Loaded Index: blocks.10.mlp.fc1.weight from Saved Weights
[18:26:35.875966] Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights
[18:26:35.875971] Loaded Index: blocks.10.mlp.fc1.bias from Saved Weights
[18:26:35.875973] Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights
[18:26:35.875978] Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights
[18:26:35.875980] Loaded Index: blocks.11.norm1.weight from Saved Weights
[18:26:35.875985] Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights
[18:26:35.875988] Loaded Index: blocks.11.norm1.bias from Saved Weights
[18:26:35.875992] Loaded Index: blocks.11.norm1.weight from Saved Weights
[18:26:35.875995] Loaded Index: blocks.11.attn.qkv.weight from Saved Weights
[18:26:35.875999] Loaded Index: blocks.11.norm1.bias from Saved Weights
[18:26:35.876002] Loaded Index: blocks.11.attn.qkv.bias from Saved Weights
[18:26:35.876006] Loaded Index: blocks.11.attn.qkv.weight from Saved Weights
[18:26:35.876009] Loaded Index: blocks.11.attn.proj.weight from Saved Weights
[18:26:35.876014] Loaded Index: blocks.11.attn.qkv.bias from Saved Weights
[18:26:35.876016] Loaded Index: blocks.11.attn.proj.bias from Saved Weights
[18:26:35.876022] Loaded Index: blocks.11.attn.proj.weight from Saved Weights
[18:26:35.876023] Loaded Index: blocks.11.norm2.weight from Saved Weights
[18:26:35.876029] Loaded Index: blocks.11.attn.proj.bias from Saved Weights
[18:26:35.876030] Loaded Index: blocks.11.norm2.bias from Saved Weights
[18:26:35.876036] Loaded Index: blocks.11.norm2.weight from Saved Weights
[18:26:35.876038] Loaded Index: blocks.11.mlp.fc1.weight from Saved Weights
[18:26:35.876043] Loaded Index: blocks.11.norm2.bias from Saved Weights
[18:26:35.876046] Loaded Index: blocks.11.mlp.fc1.bias from Saved Weights
[18:26:35.876051] Loaded Index: blocks.11.mlp.fc1.weight from Saved Weights
[18:26:35.876054] Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights
[18:26:35.876058] Loaded Index: blocks.11.mlp.fc1.bias from Saved Weights
[18:26:35.876061] Loaded Index: blocks.11.mlp.fc2.bias from Saved Weights
[18:26:35.876065] Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights
[18:26:35.876067] norm.weight not found in Init Model
[18:26:35.876072] [18:26:35.876073] Loaded Index: blocks.11.mlp.fc2.bias from Saved Weights
norm.bias not found in Init Model
[18:26:35.876079] [18:26:35.876079] norm.weight not found in Init Model
decoder_embed.weight not found in Init Model
[18:26:35.876085] norm.bias not found in Init Model[18:26:35.876086] 
decoder_embed.bias not found in Init Model
[18:26:35.876092] [18:26:35.876093] decoder_embed.weight not found in Init Modeldecoder_blocks.0.norm1.weight not found in Init Model

[18:26:35.876099] [18:26:35.876100] decoder_embed.bias not found in Init Modeldecoder_blocks.0.norm1.bias not found in Init Model

[18:26:35.876106] [18:26:35.876106] decoder_blocks.0.norm1.weight not found in Init Modeldecoder_blocks.0.attn.qkv.weight not found in Init Model

[18:26:35.876113] [18:26:35.876113] decoder_blocks.0.norm1.bias not found in Init Model
decoder_blocks.0.attn.qkv.bias not found in Init Model
[18:26:35.876119] [18:26:35.876120] decoder_blocks.0.attn.qkv.weight not found in Init Model
decoder_blocks.0.attn.proj.weight not found in Init Model
[18:26:35.876125] [18:26:35.876126] decoder_blocks.0.attn.qkv.bias not found in Init Model
decoder_blocks.0.attn.proj.bias not found in Init Model
[18:26:35.876131] [18:26:35.876132] decoder_blocks.0.attn.proj.weight not found in Init Model
decoder_blocks.0.norm2.weight not found in Init Model
[18:26:35.876138] decoder_blocks.0.attn.proj.bias not found in Init Model[18:26:35.876139] 
decoder_blocks.0.norm2.bias not found in Init Model
[18:26:35.876144] decoder_blocks.0.norm2.weight not found in Init Model[18:26:35.876145] 
decoder_blocks.0.mlp.fc1.weight not found in Init Model
[18:26:35.876150] decoder_blocks.0.norm2.bias not found in Init Model
[18:26:35.876152] decoder_blocks.0.mlp.fc1.bias not found in Init Model
[18:26:35.876156] decoder_blocks.0.mlp.fc1.weight not found in Init Model
[18:26:35.876158] decoder_blocks.0.mlp.fc2.weight not found in Init Model
[18:26:35.876162] decoder_blocks.0.mlp.fc1.bias not found in Init Model
[18:26:35.876164] decoder_blocks.0.mlp.fc2.bias not found in Init Model
[18:26:35.876168] decoder_blocks.0.mlp.fc2.weight not found in Init Model
[18:26:35.876171] decoder_blocks.1.norm1.weight not found in Init Model
[18:26:35.876174] decoder_blocks.0.mlp.fc2.bias not found in Init Model
[18:26:35.876177] decoder_blocks.1.norm1.bias not found in Init Model
[18:26:35.876180] decoder_blocks.1.norm1.weight not found in Init Model
[18:26:35.876183] decoder_blocks.1.attn.qkv.weight not found in Init Model
[18:26:35.876187] decoder_blocks.1.norm1.bias not found in Init Model
[18:26:35.876189] decoder_blocks.1.attn.qkv.bias not found in Init Model
[18:26:35.876193] decoder_blocks.1.attn.qkv.weight not found in Init Model
[18:26:35.876195] decoder_blocks.1.attn.proj.weight not found in Init Model
[18:26:35.876199] decoder_blocks.1.attn.qkv.bias not found in Init Model[18:26:35.876201] 
decoder_blocks.1.attn.proj.bias not found in Init Model
[18:26:35.876205] decoder_blocks.1.attn.proj.weight not found in Init Model[18:26:35.876207] 
decoder_blocks.1.norm2.weight not found in Init Model
[18:26:35.876212] decoder_blocks.1.attn.proj.bias not found in Init Model[18:26:35.876214] 
decoder_blocks.1.norm2.bias not found in Init Model
[18:26:35.876218] decoder_blocks.1.norm2.weight not found in Init Model[18:26:35.876220] 
decoder_blocks.1.mlp.fc1.weight not found in Init Model
[18:26:35.876225] decoder_blocks.1.norm2.bias not found in Init Model[18:26:35.876226] 
decoder_blocks.1.mlp.fc1.bias not found in Init Model
[18:26:35.876231] decoder_blocks.1.mlp.fc1.weight not found in Init Model
[18:26:35.876233] decoder_blocks.1.mlp.fc2.weight not found in Init Model
[18:26:35.876237] decoder_blocks.1.mlp.fc1.bias not found in Init Model
[18:26:35.876239] decoder_blocks.1.mlp.fc2.bias not found in Init Model
[18:26:35.876243] decoder_blocks.1.mlp.fc2.weight not found in Init Model[18:26:35.876245] 
decoder_norm.weight not found in Init Model
[18:26:35.876250] [18:26:35.876251] decoder_blocks.1.mlp.fc2.bias not found in Init Model
decoder_norm.bias not found in Init Model
[18:26:35.876256] [18:26:35.876257] decoder_norm.weight not found in Init Model
decoder_pred.weight not found in Init Model
[18:26:35.876262] [18:26:35.876263] decoder_norm.bias not found in Init Model
decoder_pred.bias not found in Init Model
[18:26:35.876269] decoder_pred.weight not found in Init Model
[18:26:35.876275] decoder_pred.bias not found in Init Model
[18:26:35.876303] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[18:26:35.893605] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[18:26:35.894737] Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth
[18:26:35.895328] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[18:26:35.895328] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
[18:26:35.895722] Loaded Index: cls_token from Saved Weights
[18:26:35.895741] Loaded Index: pos_embed from Saved Weights
[18:26:35.895748] mask_token not found in Init Model
[18:26:35.895755] [18:26:35.895743] decoder_pos_embed not found in Init Model
[18:26:35.895764] Loaded Index: patch_embed.proj.weight from Saved Weights
[18:26:35.895772] Loaded Index: patch_embed.proj.bias from Saved Weights
[18:26:35.895780] Loaded Index: blocks.0.norm1.weight from Saved Weights
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
_IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])[18:26:35.895787] 
Loaded Index: blocks.0.norm1.bias from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.895795] Loaded Index: blocks.0.attn.qkv.weight from Saved Weights
[18:26:35.895802] Loaded Index: blocks.0.attn.qkv.bias from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)
[18:26:35.895354] number of params (M): 21.67
[18:26:35.895367] base lr: 2.50e-04
[18:26:35.895374] actual lr: 3.75e-04
[18:26:35.895381] accumulate grad iterations: 1
[18:26:35.895810] Loaded Index: blocks.0.attn.proj.weight from Saved Weights
[18:26:35.895387] effective batch size: 384
[18:26:35.895817] Loaded Index: blocks.0.attn.proj.bias from Saved Weights
[18:26:35.895650] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
[18:26:35.895824] Loaded Index: blocks.0.norm2.weight from Saved Weights
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
[18:26:35.895832] Loaded Index: blocks.0.norm2.bias from Saved Weights
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
[18:26:35.895840] Loaded Index: blocks.0.mlp.fc1.weight from Saved Weights
[18:26:35.895847] Loaded Index: blocks.0.mlp.fc1.bias from Saved Weights
[18:26:35.895855] Loaded Index: blocks.0.mlp.fc2.weight from Saved Weights
[18:26:35.895862] Loaded Index: blocks.0.mlp.fc2.bias from Saved Weights
[18:26:35.895869] Loaded Index: blocks.1.norm1.weight from Saved Weights
[18:26:35.895876] Loaded Index: blocks.1.norm1.bias from Saved Weights
[18:26:35.895883] Loaded Index: blocks.1.attn.qkv.weight from Saved Weights
[18:26:35.895890] Loaded Index: blocks.1.attn.qkv.bias from Saved Weights
[18:26:35.895897] Loaded Index: blocks.1.attn.proj.weight from Saved Weights
[18:26:35.895904] Loaded Index: blocks.1.attn.proj.bias from Saved Weights
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
[18:26:35.895911] Loaded Index: blocks.1.norm2.weight from Saved Weights
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
[18:26:35.895918] Loaded Index: blocks.1.norm2.bias from Saved Weights
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
[18:26:35.895925] Loaded Index: blocks.1.mlp.fc1.weight from Saved Weights
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
[18:26:35.895933] Loaded Index: blocks.1.mlp.fc1.bias from Saved Weights
[18:26:35.895941] Loaded Index: blocks.1.mlp.fc2.weight from Saved Weights
[18:26:35.895948] Loaded Index: blocks.1.mlp.fc2.bias from Saved Weights
[18:26:35.895955] Loaded Index: blocks.2.norm1.weight from Saved Weights
[18:26:35.895962] Loaded Index: blocks.2.norm1.bias from Saved Weights
[18:26:35.895969] Loaded Index: blocks.2.attn.qkv.weight from Saved Weights
[18:26:35.895976] Loaded Index: blocks.2.attn.qkv.bias from Saved Weights
[18:26:35.895984] Loaded Index: blocks.2.attn.proj.weight from Saved Weights
[18:26:35.895991] Loaded Index: blocks.2.attn.proj.bias from Saved Weights
[18:26:35.895998] Loaded Index: blocks.2.norm2.weight from Saved Weights
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
[18:26:35.896005] Loaded Index: blocks.2.norm2.bias from Saved Weights
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
[18:26:35.896012] Loaded Index: blocks.2.mlp.fc1.weight from Saved Weights
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
[18:26:35.896019] Loaded Index: blocks.2.mlp.fc1.bias from Saved Weights
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.896028] Loaded Index: blocks.2.mlp.fc2.weight from Saved Weights
[18:26:35.896035] Loaded Index: blocks.2.mlp.fc2.bias from Saved Weights
[18:26:35.896042] Loaded Index: blocks.3.norm1.weight from Saved Weights
[18:26:35.896049] Loaded Index: blocks.3.norm1.bias from Saved Weights
[18:26:35.896056] Loaded Index: blocks.3.attn.qkv.weight from Saved Weights
[18:26:35.896063] Loaded Index: blocks.3.attn.qkv.bias from Saved Weights
[18:26:35.896070] Loaded Index: blocks.3.attn.proj.weight from Saved Weights
[18:26:35.896077] Loaded Index: blocks.3.attn.proj.bias from Saved Weights
[18:26:35.896084] Loaded Index: blocks.3.norm2.weight from Saved Weights
[18:26:35.896091] Loaded Index: blocks.3.norm2.bias from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.896098] Loaded Index: blocks.3.mlp.fc1.weight from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.896106] Loaded Index: blocks.3.mlp.fc1.bias from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)[18:26:35.895658] 
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
[18:26:35.896113] Loaded Index: blocks.3.mlp.fc2.weight from Saved Weights
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
[18:26:35.896120] Loaded Index: blocks.3.mlp.fc2.bias from Saved Weights
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
[18:26:35.896128] Loaded Index: blocks.4.norm1.weight from Saved Weights
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
[18:26:35.896135] Loaded Index: blocks.4.norm1.bias from Saved Weights
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
[18:26:35.896143] Loaded Index: blocks.4.attn.qkv.weight from Saved Weights
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
[18:26:35.896150] Loaded Index: blocks.4.attn.qkv.bias from Saved Weights
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.896158] Loaded Index: blocks.4.attn.proj.weight from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.896165] Loaded Index: blocks.4.attn.proj.bias from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.896173] Loaded Index: blocks.4.norm2.weight from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.896180] Loaded Index: blocks.4.norm2.bias from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.896188] Loaded Index: blocks.4.mlp.fc1.weight from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
[18:26:35.896195] Loaded Index: blocks.4.mlp.fc1.bias from Saved Weights
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
[18:26:35.896203] Loaded Index: blocks.4.mlp.fc2.weight from Saved Weights
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
[18:26:35.896211] Loaded Index: blocks.4.mlp.fc2.bias from Saved Weights
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
[18:26:35.896218] Loaded Index: blocks.5.norm1.weight from Saved Weights
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)
[18:26:35.895678] number of params (M): 21.67
[18:26:35.895682] number of params (M): 21.67
[18:26:35.895691] base lr: 2.50e-04
[18:26:35.895698] [18:26:35.895698] actual lr: 3.75e-04base lr: 2.50e-04

[18:26:35.895705] [18:26:35.895706] accumulate grad iterations: 1
actual lr: 3.75e-04
[18:26:35.895711] effective batch size: 384[18:26:35.895712] 
accumulate grad iterations: 1
[18:26:35.895719] effective batch size: 384
[18:26:35.896225] Loaded Index: blocks.5.norm1.bias from Saved Weights
[18:26:35.896276] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
[18:26:35.896233] Loaded Index: blocks.5.attn.qkv.weight from Saved Weights
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
[18:26:35.896240] Loaded Index: blocks.5.attn.qkv.bias from Saved Weights
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
[18:26:35.896248] Loaded Index: blocks.5.attn.proj.weight from Saved Weights
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
[18:26:35.896255] Loaded Index: blocks.5.attn.proj.bias from Saved Weights
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
[18:26:35.896263] Loaded Index: blocks.5.norm2.weight from Saved Weights
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
[18:26:35.896270] Loaded Index: blocks.5.norm2.bias from Saved Weights
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
[18:26:35.896278] Loaded Index: blocks.5.mlp.fc1.weight from Saved Weights
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
[18:26:35.896285] Loaded Index: blocks.5.mlp.fc1.bias from Saved Weights
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
[18:26:35.896292] Loaded Index: blocks.5.mlp.fc2.weight from Saved Weights
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
[18:26:35.896300] Loaded Index: blocks.5.mlp.fc2.bias from Saved Weights
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.896307] Loaded Index: blocks.6.norm1.weight from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.896315] Loaded Index: blocks.6.norm1.bias from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
[18:26:35.896322] Loaded Index: blocks.6.attn.qkv.weight from Saved Weights
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)
[18:26:35.896302] number of params (M): 21.67
[18:26:35.896317] base lr: 2.50e-04
[18:26:35.896324] actual lr: 3.75e-04
[18:26:35.896331] accumulate grad iterations: 1
[18:26:35.896330] Loaded Index: blocks.6.attn.qkv.bias from Saved Weights
[18:26:35.896337] effective batch size: 384
[18:26:35.896337] Loaded Index: blocks.6.attn.proj.weight from Saved Weights
[18:26:35.896345] Loaded Index: blocks.6.attn.proj.bias from Saved Weights
[18:26:35.896353] Loaded Index: blocks.6.norm2.weight from Saved Weights
[18:26:35.896361] Loaded Index: blocks.6.norm2.bias from Saved Weights
[18:26:35.896368] Loaded Index: blocks.6.mlp.fc1.weight from Saved Weights
[18:26:35.896376] Loaded Index: blocks.6.mlp.fc1.bias from Saved Weights
[18:26:35.896384] Loaded Index: blocks.6.mlp.fc2.weight from Saved Weights
[18:26:35.896391] Loaded Index: blocks.6.mlp.fc2.bias from Saved Weights
[18:26:35.896399] Loaded Index: blocks.7.norm1.weight from Saved Weights
[18:26:35.896406] Loaded Index: blocks.7.norm1.bias from Saved Weights
[18:26:35.896414] Loaded Index: blocks.7.attn.qkv.weight from Saved Weights
[18:26:35.896422] Loaded Index: blocks.7.attn.qkv.bias from Saved Weights
[18:26:35.896429] Loaded Index: blocks.7.attn.proj.weight from Saved Weights
[18:26:35.896436] Loaded Index: blocks.7.attn.proj.bias from Saved Weights
[18:26:35.896444] Loaded Index: blocks.7.norm2.weight from Saved Weights
[18:26:35.896452] Loaded Index: blocks.7.norm2.bias from Saved Weights
[18:26:35.896459] Loaded Index: blocks.7.mlp.fc1.weight from Saved Weights
[18:26:35.896467] Loaded Index: blocks.7.mlp.fc1.bias from Saved Weights
[18:26:35.896474] Loaded Index: blocks.7.mlp.fc2.weight from Saved Weights
[18:26:35.896483] Loaded Index: blocks.7.mlp.fc2.bias from Saved Weights
[18:26:35.896491] Loaded Index: blocks.8.norm1.weight from Saved Weights
[18:26:35.896498] Loaded Index: blocks.8.norm1.bias from Saved Weights
[18:26:35.896507] Loaded Index: blocks.8.attn.qkv.weight from Saved Weights
[18:26:35.896514] Loaded Index: blocks.8.attn.qkv.bias from Saved Weights
[18:26:35.896522] Loaded Index: blocks.8.attn.proj.weight from Saved Weights
[18:26:35.896529] Loaded Index: blocks.8.attn.proj.bias from Saved Weights
[18:26:35.896537] Loaded Index: blocks.8.norm2.weight from Saved Weights
[18:26:35.896544] Loaded Index: blocks.8.norm2.bias from Saved Weights
[18:26:35.896552] Loaded Index: blocks.8.mlp.fc1.weight from Saved Weights
[18:26:35.896559] Loaded Index: blocks.8.mlp.fc1.bias from Saved Weights
[18:26:35.896567] Loaded Index: blocks.8.mlp.fc2.weight from Saved Weights
[18:26:35.896575] Loaded Index: blocks.8.mlp.fc2.bias from Saved Weights
[18:26:35.896582] Loaded Index: blocks.9.norm1.weight from Saved Weights
[18:26:35.896590] Loaded Index: blocks.9.norm1.bias from Saved Weights
[18:26:35.896597] Loaded Index: blocks.9.attn.qkv.weight from Saved Weights
[18:26:35.896606] Loaded Index: blocks.9.attn.qkv.bias from Saved Weights
[18:26:35.896613] Loaded Index: blocks.9.attn.proj.weight from Saved Weights
[18:26:35.896622] Loaded Index: blocks.9.attn.proj.bias from Saved Weights
[18:26:35.896629] Loaded Index: blocks.9.norm2.weight from Saved Weights
[18:26:35.896637] Loaded Index: blocks.9.norm2.bias from Saved Weights
[18:26:35.896645] Loaded Index: blocks.9.mlp.fc1.weight from Saved Weights
[18:26:35.896652] Loaded Index: blocks.9.mlp.fc1.bias from Saved Weights
[18:26:35.896660] Loaded Index: blocks.9.mlp.fc2.weight from Saved Weights
[18:26:35.896668] Loaded Index: blocks.9.mlp.fc2.bias from Saved Weights
[18:26:35.896676] Loaded Index: blocks.10.norm1.weight from Saved Weights
[18:26:35.896683] Loaded Index: blocks.10.norm1.bias from Saved Weights
[18:26:35.896691] Loaded Index: blocks.10.attn.qkv.weight from Saved Weights
[18:26:35.896699] Loaded Index: blocks.10.attn.qkv.bias from Saved Weights
[18:26:35.896706] Loaded Index: blocks.10.attn.proj.weight from Saved Weights
[18:26:35.896714] Loaded Index: blocks.10.attn.proj.bias from Saved Weights
[18:26:35.896721] Loaded Index: blocks.10.norm2.weight from Saved Weights
[18:26:35.896728] Loaded Index: blocks.10.norm2.bias from Saved Weights
[18:26:35.896735] Loaded Index: blocks.10.mlp.fc1.weight from Saved Weights
[18:26:35.896743] Loaded Index: blocks.10.mlp.fc1.bias from Saved Weights
[18:26:35.896751] Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights
[18:26:35.896758] Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights
[18:26:35.896765] Loaded Index: blocks.11.norm1.weight from Saved Weights
[18:26:35.896773] Loaded Index: blocks.11.norm1.bias from Saved Weights
[18:26:35.896781] Loaded Index: blocks.11.attn.qkv.weight from Saved Weights
[18:26:35.896788] Loaded Index: blocks.11.attn.qkv.bias from Saved Weights
[18:26:35.896796] Loaded Index: blocks.11.attn.proj.weight from Saved Weights
[18:26:35.896803] Loaded Index: blocks.11.attn.proj.bias from Saved Weights
[18:26:35.896811] Loaded Index: blocks.11.norm2.weight from Saved Weights
[18:26:35.896818] Loaded Index: blocks.11.norm2.bias from Saved Weights
[18:26:35.896826] Loaded Index: blocks.11.mlp.fc1.weight from Saved Weights
[18:26:35.896834] Loaded Index: blocks.11.mlp.fc1.bias from Saved Weights
[18:26:35.896842] Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights
[18:26:35.896850] Loaded Index: blocks.11.mlp.fc2.bias from Saved Weights
[18:26:35.896856] norm.weight not found in Init Model
[18:26:35.896863] norm.bias not found in Init Model
[18:26:35.896869] decoder_embed.weight not found in Init Model
[18:26:35.896875] decoder_embed.bias not found in Init Model
[18:26:35.896882] decoder_blocks.0.norm1.weight not found in Init Model
[18:26:35.896888] decoder_blocks.0.norm1.bias not found in Init Model
[18:26:35.896895] decoder_blocks.0.attn.qkv.weight not found in Init Model
[18:26:35.896901] decoder_blocks.0.attn.qkv.bias not found in Init Model
[18:26:35.896908] decoder_blocks.0.attn.proj.weight not found in Init Model
[18:26:35.896914] decoder_blocks.0.attn.proj.bias not found in Init Model
[18:26:35.896920] decoder_blocks.0.norm2.weight not found in Init Model
[18:26:35.896927] decoder_blocks.0.norm2.bias not found in Init Model
[18:26:35.896933] decoder_blocks.0.mlp.fc1.weight not found in Init Model
[18:26:35.896939] decoder_blocks.0.mlp.fc1.bias not found in Init Model
[18:26:35.896945] decoder_blocks.0.mlp.fc2.weight not found in Init Model
[18:26:35.896951] decoder_blocks.0.mlp.fc2.bias not found in Init Model
[18:26:35.896957] decoder_blocks.1.norm1.weight not found in Init Model
[18:26:35.896963] decoder_blocks.1.norm1.bias not found in Init Model
[18:26:35.896969] decoder_blocks.1.attn.qkv.weight not found in Init Model
[18:26:35.896976] decoder_blocks.1.attn.qkv.bias not found in Init Model
[18:26:35.896982] decoder_blocks.1.attn.proj.weight not found in Init Model
[18:26:35.896988] decoder_blocks.1.attn.proj.bias not found in Init Model
[18:26:35.896994] decoder_blocks.1.norm2.weight not found in Init Model
[18:26:35.897001] decoder_blocks.1.norm2.bias not found in Init Model
[18:26:35.897007] decoder_blocks.1.mlp.fc1.weight not found in Init Model
[18:26:35.897014] decoder_blocks.1.mlp.fc1.bias not found in Init Model
[18:26:35.897020] decoder_blocks.1.mlp.fc2.weight not found in Init Model
[18:26:35.897025] decoder_blocks.1.mlp.fc2.bias not found in Init Model
[18:26:35.897032] decoder_norm.weight not found in Init Model
[18:26:35.897038] decoder_norm.bias not found in Init Model
[18:26:35.897045] decoder_pred.weight not found in Init Model
[18:26:35.897051] decoder_pred.bias not found in Init Model
[18:26:35.869312] Loaded Index: blocks.0.norm1.bias from Saved Weights
[18:26:35.869314] Loaded Index: blocks.1.norm1.weight from Saved Weights
[18:26:35.869317] Loaded Index: blocks.3.norm1.bias from Saved Weights
[18:26:35.869320] Loaded Index: blocks.0.attn.qkv.weight from Saved Weights[18:26:35.869321] 
Loaded Index: blocks.1.norm1.bias from Saved Weights
[18:26:35.869324] Loaded Index: blocks.3.attn.qkv.weight from Saved Weights
[18:26:35.869328] [18:26:35.869329] Loaded Index: blocks.0.attn.qkv.bias from Saved Weights
Loaded Index: blocks.1.attn.qkv.weight from Saved Weights
[18:26:35.869331] Loaded Index: blocks.3.attn.qkv.bias from Saved Weights
[18:26:35.869336] [18:26:35.869336] Loaded Index: blocks.0.attn.proj.weight from Saved Weights
Loaded Index: blocks.1.attn.qkv.bias from Saved Weights
[18:26:35.869338] Loaded Index: blocks.3.attn.proj.weight from Saved Weights
[18:26:35.869343] [18:26:35.869344] Loaded Index: blocks.0.attn.proj.bias from Saved Weights
Loaded Index: blocks.1.attn.proj.weight from Saved Weights[18:26:35.869345] 
Loaded Index: blocks.3.attn.proj.bias from Saved Weights
[18:26:35.869351] [18:26:35.869352] Loaded Index: blocks.0.norm2.weight from Saved WeightsLoaded Index: blocks.1.attn.proj.bias from Saved Weights[18:26:35.869353] 

Loaded Index: blocks.3.norm2.weight from Saved Weights
[18:26:35.869360] [18:26:35.869360] [18:26:35.869360] Loaded Index: blocks.0.norm2.bias from Saved WeightsLoaded Index: blocks.1.norm2.weight from Saved Weights

Loaded Index: blocks.3.norm2.bias from Saved Weights
[18:26:35.869368] [18:26:35.869368] [18:26:35.869369] Loaded Index: blocks.0.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.1.norm2.bias from Saved WeightsLoaded Index: blocks.3.mlp.fc1.weight from Saved Weights


[18:26:35.869378] [18:26:35.869378] [18:26:35.869378] Loaded Index: blocks.0.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.1.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.3.mlp.fc1.bias from Saved Weights


[18:26:35.869387] [18:26:35.869388] [18:26:35.869387] Loaded Index: blocks.0.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.3.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.1.mlp.fc1.bias from Saved Weights


[18:26:35.869397] Loaded Index: blocks.0.mlp.fc2.bias from Saved Weights[18:26:35.869398] [18:26:35.869398] 
Loaded Index: blocks.1.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.3.mlp.fc2.bias from Saved Weights

[18:26:35.869405] Loaded Index: blocks.1.norm1.weight from Saved Weights
[18:26:35.869407] [18:26:35.869407] Loaded Index: blocks.4.norm1.weight from Saved WeightsLoaded Index: blocks.1.mlp.fc2.bias from Saved Weights

[18:26:35.869413] Loaded Index: blocks.1.norm1.bias from Saved Weights
[18:26:35.869415] [18:26:35.869415] Loaded Index: blocks.4.norm1.bias from Saved WeightsLoaded Index: blocks.2.norm1.weight from Saved Weights

[18:26:35.869420] Loaded Index: blocks.1.attn.qkv.weight from Saved Weights
[18:26:35.869423] [18:26:35.869423] Loaded Index: blocks.4.attn.qkv.weight from Saved WeightsLoaded Index: blocks.2.norm1.bias from Saved Weights

[18:26:35.869428] Loaded Index: blocks.1.attn.qkv.bias from Saved Weights
[18:26:35.869431] [18:26:35.869431] Loaded Index: blocks.4.attn.qkv.bias from Saved WeightsLoaded Index: blocks.2.attn.qkv.weight from Saved Weights

[18:26:35.869435] Loaded Index: blocks.1.attn.proj.weight from Saved Weights
[18:26:35.869438] [18:26:35.869438] Loaded Index: blocks.4.attn.proj.weight from Saved WeightsLoaded Index: blocks.2.attn.qkv.bias from Saved Weights

[18:26:35.869443] Loaded Index: blocks.1.attn.proj.bias from Saved Weights
[18:26:35.869445] Loaded Index: blocks.4.attn.proj.bias from Saved Weights[18:26:35.869447] 
Loaded Index: blocks.2.attn.proj.weight from Saved Weights
[18:26:35.869450] Loaded Index: blocks.1.norm2.weight from Saved Weights
[18:26:35.869453] Loaded Index: blocks.4.norm2.weight from Saved Weights[18:26:35.869454] 
Loaded Index: blocks.2.attn.proj.bias from Saved Weights
[18:26:35.869457] Loaded Index: blocks.1.norm2.bias from Saved Weights
[18:26:35.869460] Loaded Index: blocks.4.norm2.bias from Saved Weights[18:26:35.869462] 
Loaded Index: blocks.2.norm2.weight from Saved Weights
[18:26:35.869465] Loaded Index: blocks.1.mlp.fc1.weight from Saved Weights
[18:26:35.869468] Loaded Index: blocks.4.mlp.fc1.weight from Saved Weights[18:26:35.869469] 
Loaded Index: blocks.2.norm2.bias from Saved Weights
[18:26:35.869472] Loaded Index: blocks.1.mlp.fc1.bias from Saved Weights
[18:26:35.869475] Loaded Index: blocks.4.mlp.fc1.bias from Saved Weights[18:26:35.869477] 
Loaded Index: blocks.2.mlp.fc1.weight from Saved Weights
[18:26:35.869479] Loaded Index: blocks.1.mlp.fc2.weight from Saved Weights
[18:26:35.869483] Loaded Index: blocks.4.mlp.fc2.weight from Saved Weights[18:26:35.869484] 
Loaded Index: blocks.2.mlp.fc1.bias from Saved Weights
[18:26:35.869486] Loaded Index: blocks.1.mlp.fc2.bias from Saved Weights
[18:26:35.869491] Loaded Index: blocks.4.mlp.fc2.bias from Saved Weights[18:26:35.869492] 
Loaded Index: blocks.2.mlp.fc2.weight from Saved Weights[18:26:35.869493] 
Loaded Index: blocks.2.norm1.weight from Saved Weights
[18:26:35.869498] Loaded Index: blocks.5.norm1.weight from Saved Weights
[18:26:35.869500] Loaded Index: blocks.2.mlp.fc2.bias from Saved Weights[18:26:35.869501] 
Loaded Index: blocks.2.norm1.bias from Saved Weights
[18:26:35.869505] Loaded Index: blocks.5.norm1.bias from Saved Weights
[18:26:35.869507] Loaded Index: blocks.3.norm1.weight from Saved Weights
[18:26:35.869509] Loaded Index: blocks.2.attn.qkv.weight from Saved Weights
[18:26:35.869513] Loaded Index: blocks.5.attn.qkv.weight from Saved Weights[18:26:35.869515] 
Loaded Index: blocks.3.norm1.bias from Saved Weights
[18:26:35.869517] Loaded Index: blocks.2.attn.qkv.bias from Saved Weights
[18:26:35.869521] Loaded Index: blocks.5.attn.qkv.bias from Saved Weights
[18:26:35.869522] Loaded Index: blocks.3.attn.qkv.weight from Saved Weights
[18:26:35.869525] Loaded Index: blocks.2.attn.proj.weight from Saved Weights
[18:26:35.869528] Loaded Index: blocks.5.attn.proj.weight from Saved Weights
[18:26:35.869530] Loaded Index: blocks.3.attn.qkv.bias from Saved Weights
[18:26:35.869532] Loaded Index: blocks.2.attn.proj.bias from Saved Weights
[18:26:35.869535] Loaded Index: blocks.5.attn.proj.bias from Saved Weights
[18:26:35.869537] Loaded Index: blocks.3.attn.proj.weight from Saved Weights[18:26:35.869539] 
Loaded Index: blocks.2.norm2.weight from Saved Weights
[18:26:35.869542] Loaded Index: blocks.5.norm2.weight from Saved Weights
[18:26:35.869545] Loaded Index: blocks.3.attn.proj.bias from Saved Weights[18:26:35.869547] 
Loaded Index: blocks.2.norm2.bias from Saved Weights
[18:26:35.869550] Loaded Index: blocks.5.norm2.bias from Saved Weights
[18:26:35.869553] Loaded Index: blocks.3.norm2.weight from Saved Weights
[18:26:35.869554] Loaded Index: blocks.2.mlp.fc1.weight from Saved Weights
[18:26:35.869557] Loaded Index: blocks.5.mlp.fc1.weight from Saved Weights
[18:26:35.869560] Loaded Index: blocks.3.norm2.bias from Saved Weights
[18:26:35.869561] Loaded Index: blocks.2.mlp.fc1.bias from Saved Weights
[18:26:35.869565] Loaded Index: blocks.5.mlp.fc1.bias from Saved Weights
[18:26:35.869567] Loaded Index: blocks.3.mlp.fc1.weight from Saved Weights
[18:26:35.869569] Loaded Index: blocks.2.mlp.fc2.weight from Saved Weights
[18:26:35.869572] Loaded Index: blocks.5.mlp.fc2.weight from Saved Weights
[18:26:35.869574] Loaded Index: blocks.3.mlp.fc1.bias from Saved Weights
[18:26:35.869576] Loaded Index: blocks.2.mlp.fc2.bias from Saved Weights
[18:26:35.869579] Loaded Index: blocks.5.mlp.fc2.bias from Saved Weights
[18:26:35.869581] Loaded Index: blocks.3.mlp.fc2.weight from Saved Weights
[18:26:35.869584] Loaded Index: blocks.3.norm1.weight from Saved Weights
[18:26:35.869585] Loaded Index: blocks.6.norm1.weight from Saved Weights
[18:26:35.869588] Loaded Index: blocks.3.mlp.fc2.bias from Saved Weights
[18:26:35.869591] Loaded Index: blocks.3.norm1.bias from Saved Weights
[18:26:35.869592] Loaded Index: blocks.6.norm1.bias from Saved Weights
[18:26:35.869595] Loaded Index: blocks.4.norm1.weight from Saved Weights
[18:26:35.869598] Loaded Index: blocks.3.attn.qkv.weight from Saved Weights
[18:26:35.869599] Loaded Index: blocks.6.attn.qkv.weight from Saved Weights
[18:26:35.869602] Loaded Index: blocks.4.norm1.bias from Saved Weights
[18:26:35.869605] Loaded Index: blocks.3.attn.qkv.bias from Saved Weights
[18:26:35.869607] Loaded Index: blocks.6.attn.qkv.bias from Saved Weights
[18:26:35.869610] Loaded Index: blocks.4.attn.qkv.weight from Saved Weights
[18:26:35.869612] Loaded Index: blocks.3.attn.proj.weight from Saved Weights
[18:26:35.869614] Loaded Index: blocks.6.attn.proj.weight from Saved Weights
[18:26:35.869617] Loaded Index: blocks.4.attn.qkv.bias from Saved Weights
[18:26:35.869619] Loaded Index: blocks.3.attn.proj.bias from Saved Weights
[18:26:35.869621] Loaded Index: blocks.6.attn.proj.bias from Saved Weights
[18:26:35.869624] Loaded Index: blocks.4.attn.proj.weight from Saved Weights
[18:26:35.869626] Loaded Index: blocks.3.norm2.weight from Saved Weights[18:26:35.869628] 
Loaded Index: blocks.6.norm2.weight from Saved Weights
[18:26:35.869631] Loaded Index: blocks.4.attn.proj.bias from Saved Weights
[18:26:35.869635] [18:26:35.869635] Loaded Index: blocks.3.norm2.bias from Saved WeightsLoaded Index: blocks.6.norm2.bias from Saved Weights

[18:26:35.869638] Loaded Index: blocks.4.norm2.weight from Saved Weights
[18:26:35.869643] [18:26:35.869643] Loaded Index: blocks.3.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.6.mlp.fc1.weight from Saved Weights
[18:26:35.869645] 
Loaded Index: blocks.4.norm2.bias from Saved Weights
[18:26:35.869651] [18:26:35.869651] Loaded Index: blocks.3.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.6.mlp.fc1.bias from Saved Weights

[18:26:35.869653] Loaded Index: blocks.4.mlp.fc1.weight from Saved Weights
[18:26:35.869658] [18:26:35.869658] Loaded Index: blocks.6.mlp.fc2.weight from Saved Weights[18:26:35.869666] 
Loaded Index: blocks.3.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.4.mlp.fc1.bias from Saved Weights

[18:26:35.869673] [18:26:35.869674] Loaded Index: blocks.6.mlp.fc2.bias from Saved Weights[18:26:35.869674] 
Loaded Index: blocks.3.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.4.mlp.fc2.weight from Saved Weights

[18:26:35.869681] Loaded Index: blocks.7.norm1.weight from Saved Weights[18:26:35.869682] 
[18:26:35.869683] Loaded Index: blocks.4.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.4.norm1.weight from Saved Weights

[18:26:35.869689] Loaded Index: blocks.7.norm1.bias from Saved Weights[18:26:35.869690] 
[18:26:35.869691] Loaded Index: blocks.5.norm1.weight from Saved WeightsLoaded Index: blocks.4.norm1.bias from Saved Weights

[18:26:35.869696] Loaded Index: blocks.7.attn.qkv.weight from Saved Weights
[18:26:35.869698] [18:26:35.869699] Loaded Index: blocks.5.norm1.bias from Saved WeightsLoaded Index: blocks.4.attn.qkv.weight from Saved Weights

[18:26:35.869704] Loaded Index: blocks.7.attn.qkv.bias from Saved Weights
[18:26:35.869706] [18:26:35.869707] Loaded Index: blocks.5.attn.qkv.weight from Saved WeightsLoaded Index: blocks.4.attn.qkv.bias from Saved Weights

[18:26:35.869711] Loaded Index: blocks.7.attn.proj.weight from Saved Weights
[18:26:35.869714] [18:26:35.869714] Loaded Index: blocks.5.attn.qkv.bias from Saved WeightsLoaded Index: blocks.4.attn.proj.weight from Saved Weights

[18:26:35.869718] Loaded Index: blocks.7.attn.proj.bias from Saved Weights
[18:26:35.869721] [18:26:35.869722] Loaded Index: blocks.5.attn.proj.weight from Saved WeightsLoaded Index: blocks.4.attn.proj.bias from Saved Weights

[18:26:35.869725] Loaded Index: blocks.7.norm2.weight from Saved Weights
[18:26:35.869729] [18:26:35.869729] Loaded Index: blocks.5.attn.proj.bias from Saved WeightsLoaded Index: blocks.4.norm2.weight from Saved Weights

[18:26:35.869733] Loaded Index: blocks.7.norm2.bias from Saved Weights
[18:26:35.869736] [18:26:35.869736] Loaded Index: blocks.5.norm2.weight from Saved WeightsLoaded Index: blocks.4.norm2.bias from Saved Weights

[18:26:35.869740] Loaded Index: blocks.7.mlp.fc1.weight from Saved Weights
[18:26:35.869744] [18:26:35.869744] Loaded Index: blocks.5.norm2.bias from Saved WeightsLoaded Index: blocks.4.mlp.fc1.weight from Saved Weights

[18:26:35.869748] Loaded Index: blocks.7.mlp.fc1.bias from Saved Weights
[18:26:35.869751] [18:26:35.869751] Loaded Index: blocks.5.mlp.fc1.weight from Saved Weights
Loaded Index: blocks.4.mlp.fc1.bias from Saved Weights
[18:26:35.869755] Loaded Index: blocks.7.mlp.fc2.weight from Saved Weights
[18:26:35.869759] [18:26:35.869760] Loaded Index: blocks.5.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.4.mlp.fc2.weight from Saved Weights

[18:26:35.869762] Loaded Index: blocks.7.mlp.fc2.bias from Saved Weights
[18:26:35.869767] [18:26:35.869767] Loaded Index: blocks.5.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.4.mlp.fc2.bias from Saved Weights

[18:26:35.869769] Loaded Index: blocks.8.norm1.weight from Saved Weights
[18:26:35.869775] [18:26:35.869775] Loaded Index: blocks.5.mlp.fc2.bias from Saved Weights
Loaded Index: blocks.5.norm1.weight from Saved Weights[18:26:35.869776] 
Loaded Index: blocks.8.norm1.bias from Saved Weights
[18:26:35.869782] Loaded Index: blocks.6.norm1.weight from Saved Weights[18:26:35.869783] 
[18:26:35.869784] Loaded Index: blocks.5.norm1.bias from Saved Weights
Loaded Index: blocks.8.attn.qkv.weight from Saved Weights
[18:26:35.869790] Loaded Index: blocks.6.norm1.bias from Saved Weights[18:26:35.869791] 
[18:26:35.869792] Loaded Index: blocks.5.attn.qkv.weight from Saved Weights
Loaded Index: blocks.8.attn.qkv.bias from Saved Weights
[18:26:35.869797] Loaded Index: blocks.6.attn.qkv.weight from Saved Weights[18:26:35.869798] 
[18:26:35.869799] Loaded Index: blocks.5.attn.qkv.bias from Saved Weights
Loaded Index: blocks.8.attn.proj.weight from Saved Weights
[18:26:35.869805] [18:26:35.869806] Loaded Index: blocks.6.attn.qkv.bias from Saved Weights[18:26:35.869807] 
Loaded Index: blocks.5.attn.proj.weight from Saved WeightsLoaded Index: blocks.8.attn.proj.bias from Saved Weights

[18:26:35.869813] Loaded Index: blocks.6.attn.proj.weight from Saved Weights
[18:26:35.869815] [18:26:35.869815] Loaded Index: blocks.8.norm2.weight from Saved WeightsLoaded Index: blocks.5.attn.proj.bias from Saved Weights

[18:26:35.869821] Loaded Index: blocks.6.attn.proj.bias from Saved Weights
[18:26:35.869823] [18:26:35.869823] Loaded Index: blocks.8.norm2.bias from Saved WeightsLoaded Index: blocks.5.norm2.weight from Saved Weights

[18:26:35.869828] Loaded Index: blocks.6.norm2.weight from Saved Weights
[18:26:35.869830] [18:26:35.869831] Loaded Index: blocks.8.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.5.norm2.bias from Saved Weights

[18:26:35.869835] Loaded Index: blocks.6.norm2.bias from Saved Weights
[18:26:35.869838] [18:26:35.869839] Loaded Index: blocks.8.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.5.mlp.fc1.weight from Saved Weights

[18:26:35.869842] Loaded Index: blocks.6.mlp.fc1.weight from Saved Weights
[18:26:35.869846] [18:26:35.869846] Loaded Index: blocks.5.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.8.mlp.fc2.weight from Saved Weights

[18:26:35.869849] Loaded Index: blocks.6.mlp.fc1.bias from Saved Weights
[18:26:35.869854] [18:26:35.869854] Loaded Index: blocks.5.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.8.mlp.fc2.bias from Saved Weights
[18:26:35.869857] 
Loaded Index: blocks.6.mlp.fc2.weight from Saved Weights
[18:26:35.869862] [18:26:35.869863] Loaded Index: blocks.5.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.9.norm1.weight from Saved Weights
[18:26:35.869864] 
Loaded Index: blocks.6.mlp.fc2.bias from Saved Weights
[18:26:35.869870] [18:26:35.869871] Loaded Index: blocks.6.norm1.weight from Saved Weights
Loaded Index: blocks.9.norm1.bias from Saved Weights[18:26:35.869872] 
Loaded Index: blocks.7.norm1.weight from Saved Weights
[18:26:35.869878] [18:26:35.869879] Loaded Index: blocks.6.norm1.bias from Saved Weights
Loaded Index: blocks.9.attn.qkv.weight from Saved Weights
[18:26:35.869880] Loaded Index: blocks.7.norm1.bias from Saved Weights
[18:26:35.869886] [18:26:35.869887] Loaded Index: blocks.6.attn.qkv.weight from Saved Weights
Loaded Index: blocks.9.attn.qkv.bias from Saved Weights
[18:26:35.869889] Loaded Index: blocks.7.attn.qkv.weight from Saved Weights
[18:26:35.869893] [18:26:35.869894] Loaded Index: blocks.6.attn.qkv.bias from Saved Weights
Loaded Index: blocks.9.attn.proj.weight from Saved Weights[18:26:35.869896] 
Loaded Index: blocks.7.attn.qkv.bias from Saved Weights
[18:26:35.869901] Loaded Index: blocks.6.attn.proj.weight from Saved Weights[18:26:35.869902] 
Loaded Index: blocks.9.attn.proj.bias from Saved Weights[18:26:35.869903] 
Loaded Index: blocks.7.attn.proj.weight from Saved Weights
[18:26:35.869908] Loaded Index: blocks.6.attn.proj.bias from Saved Weights[18:26:35.869909] 
[18:26:35.869910] Loaded Index: blocks.9.norm2.weight from Saved WeightsLoaded Index: blocks.7.attn.proj.bias from Saved Weights

[18:26:35.869916] Loaded Index: blocks.6.norm2.weight from Saved Weights
[18:26:35.869918] [18:26:35.869918] Loaded Index: blocks.9.norm2.bias from Saved WeightsLoaded Index: blocks.7.norm2.weight from Saved Weights

[18:26:35.869923] Loaded Index: blocks.6.norm2.bias from Saved Weights
[18:26:35.869925] [18:26:35.869926] Loaded Index: blocks.9.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.7.norm2.bias from Saved Weights

[18:26:35.869930] Loaded Index: blocks.6.mlp.fc1.weight from Saved Weights
[18:26:35.869933] [18:26:35.869934] Loaded Index: blocks.9.mlp.fc1.bias from Saved WeightsLoaded Index: blocks.7.mlp.fc1.weight from Saved Weights

[18:26:35.869937] Loaded Index: blocks.6.mlp.fc1.bias from Saved Weights
[18:26:35.869941] [18:26:35.869941] Loaded Index: blocks.9.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.7.mlp.fc1.bias from Saved Weights

[18:26:35.869945] Loaded Index: blocks.6.mlp.fc2.weight from Saved Weights
[18:26:35.869948] [18:26:35.869949] Loaded Index: blocks.9.mlp.fc2.bias from Saved Weights
Loaded Index: blocks.7.mlp.fc2.weight from Saved Weights
[18:26:35.869952] Loaded Index: blocks.6.mlp.fc2.bias from Saved Weights
[18:26:35.869956] [18:26:35.869956] Loaded Index: blocks.10.norm1.weight from Saved WeightsLoaded Index: blocks.7.mlp.fc2.bias from Saved Weights

[18:26:35.869959] Loaded Index: blocks.7.norm1.weight from Saved Weights
[18:26:35.869963] [18:26:35.869964] Loaded Index: blocks.10.norm1.bias from Saved WeightsLoaded Index: blocks.8.norm1.weight from Saved Weights

[18:26:35.869967] Loaded Index: blocks.7.norm1.bias from Saved Weights
[18:26:35.869971] [18:26:35.869972] Loaded Index: blocks.8.norm1.bias from Saved WeightsLoaded Index: blocks.10.attn.qkv.weight from Saved Weights

[18:26:35.869975] Loaded Index: blocks.7.attn.qkv.weight from Saved Weights
[18:26:35.869980] [18:26:35.869980] Loaded Index: blocks.10.attn.qkv.bias from Saved WeightsLoaded Index: blocks.8.attn.qkv.weight from Saved Weights

[18:26:35.869982] Loaded Index: blocks.7.attn.qkv.bias from Saved Weights
[18:26:35.869987] [18:26:35.869988] Loaded Index: blocks.10.attn.proj.weight from Saved Weights
Loaded Index: blocks.8.attn.qkv.bias from Saved Weights
[18:26:35.869990] Loaded Index: blocks.7.attn.proj.weight from Saved Weights
[18:26:35.869995] Loaded Index: blocks.10.attn.proj.bias from Saved Weights[18:26:35.869996] 
Loaded Index: blocks.8.attn.proj.weight from Saved Weights[18:26:35.869997] 
Loaded Index: blocks.7.attn.proj.bias from Saved Weights
[18:26:35.870002] Loaded Index: blocks.10.norm2.weight from Saved Weights[18:26:35.870004] 
Loaded Index: blocks.8.attn.proj.bias from Saved Weights[18:26:35.870005] 
Loaded Index: blocks.7.norm2.weight from Saved Weights
[18:26:35.870010] Loaded Index: blocks.10.norm2.bias from Saved Weights
[18:26:35.870011] Loaded Index: blocks.8.norm2.weight from Saved Weights[18:26:35.870013] 
Loaded Index: blocks.7.norm2.bias from Saved Weights
[18:26:35.870017] Loaded Index: blocks.10.mlp.fc1.weight from Saved Weights
[18:26:35.870019] Loaded Index: blocks.8.norm2.bias from Saved Weights[18:26:35.870020] 
Loaded Index: blocks.7.mlp.fc1.weight from Saved Weights
[18:26:35.870024] Loaded Index: blocks.10.mlp.fc1.bias from Saved Weights
[18:26:35.870026] [18:26:35.870027] Loaded Index: blocks.8.mlp.fc1.weight from Saved Weights
Loaded Index: blocks.7.mlp.fc1.bias from Saved Weights
[18:26:35.870031] Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights
[18:26:35.870034] Loaded Index: blocks.8.mlp.fc1.bias from Saved Weights[18:26:35.870036] 
Loaded Index: blocks.7.mlp.fc2.weight from Saved Weights
[18:26:35.870038] Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights
[18:26:35.870042] [18:26:35.870043] Loaded Index: blocks.8.mlp.fc2.weight from Saved WeightsLoaded Index: blocks.7.mlp.fc2.bias from Saved Weights

[18:26:35.870046] Loaded Index: blocks.11.norm1.weight from Saved Weights
[18:26:35.870051] [18:26:35.870051] Loaded Index: blocks.8.mlp.fc2.bias from Saved WeightsLoaded Index: blocks.8.norm1.weight from Saved Weights
[18:26:35.870052] 
Loaded Index: blocks.11.norm1.bias from Saved Weights
[18:26:35.870058] [18:26:35.870059] Loaded Index: blocks.9.norm1.weight from Saved Weights
Loaded Index: blocks.8.norm1.bias from Saved Weights[18:26:35.870060] 
Loaded Index: blocks.11.attn.qkv.weight from Saved Weights
[18:26:35.870065] Loaded Index: blocks.9.norm1.bias from Saved Weights
[18:26:35.870067] [18:26:35.870067] Loaded Index: blocks.8.attn.qkv.weight from Saved WeightsLoaded Index: blocks.11.attn.qkv.bias from Saved Weights

[18:26:35.870072] Loaded Index: blocks.9.attn.qkv.weight from Saved Weights
[18:26:35.870075] [18:26:35.870075] Loaded Index: blocks.8.attn.qkv.bias from Saved WeightsLoaded Index: blocks.11.attn.proj.weight from Saved Weights

[18:26:35.870079] Loaded Index: blocks.9.attn.qkv.bias from Saved Weights
[18:26:35.870082] [18:26:35.870082] Loaded Index: blocks.11.attn.proj.bias from Saved WeightsLoaded Index: blocks.8.attn.proj.weight from Saved Weights

[18:26:35.870087] Loaded Index: blocks.9.attn.proj.weight from Saved Weights
[18:26:35.870089] [18:26:35.870090] Loaded Index: blocks.11.norm2.weight from Saved Weights
Loaded Index: blocks.8.attn.proj.bias from Saved Weights
[18:26:35.870095] Loaded Index: blocks.9.attn.proj.bias from Saved Weights
[18:26:35.870097] Loaded Index: blocks.11.norm2.bias from Saved Weights[18:26:35.870098] 
Loaded Index: blocks.8.norm2.weight from Saved Weights
[18:26:35.870102] Loaded Index: blocks.9.norm2.weight from Saved Weights
[18:26:35.870105] Loaded Index: blocks.11.mlp.fc1.weight from Saved Weights[18:26:35.870106] 
Loaded Index: blocks.8.norm2.bias from Saved Weights
[18:26:35.870109] Loaded Index: blocks.9.norm2.bias from Saved Weights
[18:26:35.870112] [18:26:35.870114] Loaded Index: blocks.11.mlp.fc1.bias from Saved Weights
Loaded Index: blocks.8.mlp.fc1.weight from Saved Weights
[18:26:35.870117] Loaded Index: blocks.9.mlp.fc1.weight from Saved Weights
[18:26:35.870120] Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights[18:26:35.870122] 
Loaded Index: blocks.8.mlp.fc1.bias from Saved Weights
[18:26:35.870124] Loaded Index: blocks.9.mlp.fc1.bias from Saved Weights
[18:26:35.870128] Loaded Index: blocks.11.mlp.fc2.bias from Saved Weights
[18:26:35.870130] Loaded Index: blocks.8.mlp.fc2.weight from Saved Weights[18:26:35.870131] 
Loaded Index: blocks.9.mlp.fc2.weight from Saved Weights
[18:26:35.870134] norm.weight not found in Init Model
[18:26:35.870137] Loaded Index: blocks.8.mlp.fc2.bias from Saved Weights[18:26:35.870139] 
Loaded Index: blocks.9.mlp.fc2.bias from Saved Weights[18:26:35.870140] 
norm.bias not found in Init Model
[18:26:35.870145] Loaded Index: blocks.9.norm1.weight from Saved Weights[18:26:35.870146] 
[18:26:35.870146] Loaded Index: blocks.10.norm1.weight from Saved Weights
decoder_embed.weight not found in Init Model
[18:26:35.870153] [18:26:35.870154] Loaded Index: blocks.9.norm1.bias from Saved Weights[18:26:35.870154] decoder_embed.bias not found in Init Model

Loaded Index: blocks.10.norm1.bias from Saved Weights
[18:26:35.870160] [18:26:35.870161] decoder_blocks.0.norm1.weight not found in Init Model[18:26:35.870162] 
Loaded Index: blocks.9.attn.qkv.weight from Saved Weights
Loaded Index: blocks.10.attn.qkv.weight from Saved Weights
[18:26:35.870167] decoder_blocks.0.norm1.bias not found in Init Model
[18:26:35.870169] [18:26:35.870170] Loaded Index: blocks.9.attn.qkv.bias from Saved Weights
Loaded Index: blocks.10.attn.qkv.bias from Saved Weights
[18:26:35.870173] decoder_blocks.0.attn.qkv.weight not found in Init Model
[18:26:35.870176] [18:26:35.870177] Loaded Index: blocks.9.attn.proj.weight from Saved WeightsLoaded Index: blocks.10.attn.proj.weight from Saved Weights
[18:26:35.870179] 
decoder_blocks.0.attn.qkv.bias not found in Init Model
[18:26:35.870184] [18:26:35.870185] [18:26:35.870185] Loaded Index: blocks.9.attn.proj.bias from Saved WeightsLoaded Index: blocks.10.attn.proj.bias from Saved Weights
decoder_blocks.0.attn.proj.weight not found in Init Model

[18:26:35.870192] [18:26:35.870192] [18:26:35.870193] Loaded Index: blocks.9.norm2.weight from Saved Weightsdecoder_blocks.0.attn.proj.bias not found in Init ModelLoaded Index: blocks.10.norm2.weight from Saved Weights


[18:26:35.870200] decoder_blocks.0.norm2.weight not found in Init Model
[18:26:35.870202] [18:26:35.870202] Loaded Index: blocks.9.norm2.bias from Saved WeightsLoaded Index: blocks.10.norm2.bias from Saved Weights

[18:26:35.870206] decoder_blocks.0.norm2.bias not found in Init Model
[18:26:35.870210] [18:26:35.870211] Loaded Index: blocks.9.mlp.fc1.weight from Saved WeightsLoaded Index: blocks.10.mlp.fc1.weight from Saved Weights[18:26:35.870212] 

decoder_blocks.0.mlp.fc1.weight not found in Init Model
[18:26:35.870218] [18:26:35.870219] [18:26:35.870219] Loaded Index: blocks.9.mlp.fc1.bias from Saved Weightsdecoder_blocks.0.mlp.fc1.bias not found in Init Model

Loaded Index: blocks.10.mlp.fc1.bias from Saved Weights
[18:26:35.870225] decoder_blocks.0.mlp.fc2.weight not found in Init Model[18:26:35.870226] 
Loaded Index: blocks.9.mlp.fc2.weight from Saved Weights[18:26:35.870228] 
Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights
[18:26:35.870232] decoder_blocks.0.mlp.fc2.bias not found in Init Model
[18:26:35.870234] Loaded Index: blocks.9.mlp.fc2.bias from Saved Weights[18:26:35.870235] 
Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights[18:26:35.870237] 
decoder_blocks.1.norm1.weight not found in Init Model
[18:26:35.870242] Loaded Index: blocks.10.norm1.weight from Saved Weights
[18:26:35.870243] [18:26:35.870244] decoder_blocks.1.norm1.bias not found in Init ModelLoaded Index: blocks.11.norm1.weight from Saved Weights

[18:26:35.870249] Loaded Index: blocks.10.norm1.bias from Saved Weights[18:26:35.870251] 
[18:26:35.870252] decoder_blocks.1.attn.qkv.weight not found in Init Model
Loaded Index: blocks.11.norm1.bias from Saved Weights
[18:26:35.870257] [18:26:35.870257] Loaded Index: blocks.10.attn.qkv.weight from Saved Weightsdecoder_blocks.1.attn.qkv.bias not found in Init Model
[18:26:35.870259] 
Loaded Index: blocks.11.attn.qkv.weight from Saved Weights
[18:26:35.870265] [18:26:35.870265] decoder_blocks.1.attn.proj.weight not found in Init ModelLoaded Index: blocks.10.attn.qkv.bias from Saved Weights

[18:26:35.870267] Loaded Index: blocks.11.attn.qkv.bias from Saved Weights
[18:26:35.870271] decoder_blocks.1.attn.proj.bias not found in Init Model
[18:26:35.870273] Loaded Index: blocks.10.attn.proj.weight from Saved Weights
[18:26:35.870275] Loaded Index: blocks.11.attn.proj.weight from Saved Weights
[18:26:35.870277] decoder_blocks.1.norm2.weight not found in Init Model
[18:26:35.870281] Loaded Index: blocks.10.attn.proj.bias from Saved Weights
[18:26:35.870282] [18:26:35.870283] Loaded Index: blocks.11.attn.proj.bias from Saved Weightsdecoder_blocks.1.norm2.bias not found in Init Model

[18:26:35.870288] Loaded Index: blocks.10.norm2.weight from Saved Weights
[18:26:35.870290] decoder_blocks.1.mlp.fc1.weight not found in Init Model[18:26:35.870291] 
Loaded Index: blocks.11.norm2.weight from Saved Weights
[18:26:35.870295] Loaded Index: blocks.10.norm2.bias from Saved Weights[18:26:35.870296] 
decoder_blocks.1.mlp.fc1.bias not found in Init Model
[18:26:35.870299] Loaded Index: blocks.11.norm2.bias from Saved Weights
[18:26:35.870302] [18:26:35.870302] Loaded Index: blocks.10.mlp.fc1.weight from Saved Weightsdecoder_blocks.1.mlp.fc2.weight not found in Init Model

[18:26:35.870306] Loaded Index: blocks.11.mlp.fc1.weight from Saved Weights
[18:26:35.870309] decoder_blocks.1.mlp.fc2.bias not found in Init Model
[18:26:35.870310] Loaded Index: blocks.10.mlp.fc1.bias from Saved Weights
[18:26:35.870313] Loaded Index: blocks.11.mlp.fc1.bias from Saved Weights
[18:26:35.870315] decoder_norm.weight not found in Init Model
[18:26:35.870319] Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights
[18:26:35.870320] [18:26:35.870321] Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights
decoder_norm.bias not found in Init Model
[18:26:35.870326] Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights
[18:26:35.870327] [18:26:35.870327] decoder_pred.weight not found in Init ModelLoaded Index: blocks.11.mlp.fc2.bias from Saved Weights

[18:26:35.870333] [18:26:35.870334] Loaded Index: blocks.11.norm1.weight from Saved Weights[18:26:35.870334] 
decoder_pred.bias not found in Init Modelnorm.weight not found in Init Model

[18:26:35.870340] [18:26:35.870341] Loaded Index: blocks.11.norm1.bias from Saved Weights
norm.bias not found in Init Model
[18:26:35.870348] [18:26:35.870348] decoder_embed.weight not found in Init ModelLoaded Index: blocks.11.attn.qkv.weight from Saved Weights

[18:26:35.870354] decoder_embed.bias not found in Init Model
[18:26:35.870355] Loaded Index: blocks.11.attn.qkv.bias from Saved Weights
[18:26:35.870360] decoder_blocks.0.norm1.weight not found in Init Model
[18:26:35.870363] Loaded Index: blocks.11.attn.proj.weight from Saved Weights
[18:26:35.870366] decoder_blocks.0.norm1.bias not found in Init Model
[18:26:35.870371] Loaded Index: blocks.11.attn.proj.bias from Saved Weights
[18:26:35.870373] decoder_blocks.0.attn.qkv.weight not found in Init Model
[18:26:35.870378] [18:26:35.870379] Loaded Index: blocks.11.norm2.weight from Saved Weights
decoder_blocks.0.attn.qkv.bias not found in Init Model
[18:26:35.870385] [18:26:35.870385] decoder_blocks.0.attn.proj.weight not found in Init ModelLoaded Index: blocks.11.norm2.bias from Saved Weights

[18:26:35.870392] decoder_blocks.0.attn.proj.bias not found in Init Model
[18:26:35.870393] Loaded Index: blocks.11.mlp.fc1.weight from Saved Weights
[18:26:35.870398] decoder_blocks.0.norm2.weight not found in Init Model
[18:26:35.870401] Loaded Index: blocks.11.mlp.fc1.bias from Saved Weights
[18:26:35.870404] decoder_blocks.0.norm2.bias not found in Init Model
[18:26:35.870408] Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights[18:26:35.870410] 
decoder_blocks.0.mlp.fc1.weight not found in Init Model
[18:26:35.870416] [18:26:35.870416] Loaded Index: blocks.11.mlp.fc2.bias from Saved Weightsdecoder_blocks.0.mlp.fc1.bias not found in Init Model

[18:26:35.870423] [18:26:35.870423] norm.weight not found in Init Modeldecoder_blocks.0.mlp.fc2.weight not found in Init Model

[18:26:35.870430] [18:26:35.870430] norm.bias not found in Init Modeldecoder_blocks.0.mlp.fc2.bias not found in Init Model

[18:26:35.870437] [18:26:35.870437] decoder_embed.weight not found in Init Modeldecoder_blocks.1.norm1.weight not found in Init Model

[18:26:35.870444] [18:26:35.870444] decoder_embed.bias not found in Init Modeldecoder_blocks.1.norm1.bias not found in Init Model

[18:26:35.870450] [18:26:35.870451] decoder_blocks.0.norm1.weight not found in Init Modeldecoder_blocks.1.attn.qkv.weight not found in Init Model

[18:26:35.870457] [18:26:35.870457] decoder_blocks.0.norm1.bias not found in Init Modeldecoder_blocks.1.attn.qkv.bias not found in Init Model

[18:26:35.870464] [18:26:35.870464] decoder_blocks.1.attn.proj.weight not found in Init Modeldecoder_blocks.0.attn.qkv.weight not found in Init Model

[18:26:35.870471] [18:26:35.870471] decoder_blocks.0.attn.qkv.bias not found in Init Modeldecoder_blocks.1.attn.proj.bias not found in Init Model

[18:26:35.870477] [18:26:35.870478] decoder_blocks.0.attn.proj.weight not found in Init Modeldecoder_blocks.1.norm2.weight not found in Init Model

[18:26:35.870484] [18:26:35.870485] decoder_blocks.0.attn.proj.bias not found in Init Modeldecoder_blocks.1.norm2.bias not found in Init Model

[18:26:35.870491] [18:26:35.870491] decoder_blocks.0.norm2.weight not found in Init Modeldecoder_blocks.1.mlp.fc1.weight not found in Init Model

[18:26:35.870498] [18:26:35.870498] decoder_blocks.0.norm2.bias not found in Init Modeldecoder_blocks.1.mlp.fc1.bias not found in Init Model

[18:26:35.870505] [18:26:35.870505] decoder_blocks.0.mlp.fc1.weight not found in Init Modeldecoder_blocks.1.mlp.fc2.weight not found in Init Model

[18:26:35.870512] [18:26:35.870512] decoder_blocks.1.mlp.fc2.bias not found in Init Modeldecoder_blocks.0.mlp.fc1.bias not found in Init Model

[18:26:35.870519] [18:26:35.870520] decoder_norm.weight not found in Init Modeldecoder_blocks.0.mlp.fc2.weight not found in Init Model

[18:26:35.870526] [18:26:35.870526] decoder_norm.bias not found in Init Modeldecoder_blocks.0.mlp.fc2.bias not found in Init Model

[18:26:35.870533] [18:26:35.870533] decoder_pred.weight not found in Init Modeldecoder_blocks.1.norm1.weight not found in Init Model

[18:26:35.870539] [18:26:35.870540] decoder_pred.bias not found in Init Modeldecoder_blocks.1.norm1.bias not found in Init Model

[18:26:35.870546] decoder_blocks.1.attn.qkv.weight not found in Init Model
[18:26:35.870552] decoder_blocks.1.attn.qkv.bias not found in Init Model
[18:26:35.870559] decoder_blocks.1.attn.proj.weight not found in Init Model
[18:26:35.870565] decoder_blocks.1.attn.proj.bias not found in Init Model
[18:26:35.870572] decoder_blocks.1.norm2.weight not found in Init Model
[18:26:35.870577] decoder_blocks.1.norm2.bias not found in Init Model
[18:26:35.870583] decoder_blocks.1.mlp.fc1.weight not found in Init Model
[18:26:35.870589] decoder_blocks.1.mlp.fc1.bias not found in Init Model
[18:26:35.870595] decoder_blocks.1.mlp.fc2.weight not found in Init Model
[18:26:35.870601] decoder_blocks.1.mlp.fc2.bias not found in Init Model
[18:26:35.870607] decoder_norm.weight not found in Init Model
[18:26:35.870612] decoder_norm.bias not found in Init Model
[18:26:35.870618] decoder_pred.weight not found in Init Model
[18:26:35.870624] decoder_pred.bias not found in Init Model
[18:26:35.884097] Load pre-trained checkpoint from: /mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth
[18:26:35.885085] Loaded Index: cls_token from Saved Weights
[18:26:35.885102] Loaded Index: pos_embed from Saved Weights
[18:26:35.885109] mask_token not found in Init Model
[18:26:35.885116] decoder_pos_embed not found in Init Model
[18:26:35.885124] Loaded Index: patch_embed.proj.weight from Saved Weights
[18:26:35.885132] Loaded Index: patch_embed.proj.bias from Saved Weights
[18:26:35.885140] Loaded Index: blocks.0.norm1.weight from Saved Weights
[18:26:35.885147] Loaded Index: blocks.0.norm1.bias from Saved Weights
[18:26:35.885155] Loaded Index: blocks.0.attn.qkv.weight from Saved Weights
[18:26:35.885162] Loaded Index: blocks.0.attn.qkv.bias from Saved Weights
[18:26:35.885170] Loaded Index: blocks.0.attn.proj.weight from Saved Weights
[18:26:35.885177] Loaded Index: blocks.0.attn.proj.bias from Saved Weights
[18:26:35.885184] Loaded Index: blocks.0.norm2.weight from Saved Weights
[18:26:35.885191] Loaded Index: blocks.0.norm2.bias from Saved Weights
[18:26:35.885199] Loaded Index: blocks.0.mlp.fc1.weight from Saved Weights
[18:26:35.885207] Loaded Index: blocks.0.mlp.fc1.bias from Saved Weights
[18:26:35.885214] Loaded Index: blocks.0.mlp.fc2.weight from Saved Weights
[18:26:35.885221] Loaded Index: blocks.0.mlp.fc2.bias from Saved Weights
[18:26:35.885229] Loaded Index: blocks.1.norm1.weight from Saved Weights
[18:26:35.885237] Loaded Index: blocks.1.norm1.bias from Saved Weights
[18:26:35.885244] Loaded Index: blocks.1.attn.qkv.weight from Saved Weights
[18:26:35.885251] Loaded Index: blocks.1.attn.qkv.bias from Saved Weights
[18:26:35.885259] Loaded Index: blocks.1.attn.proj.weight from Saved Weights
[18:26:35.885267] Loaded Index: blocks.1.attn.proj.bias from Saved Weights
[18:26:35.885274] Loaded Index: blocks.1.norm2.weight from Saved Weights
[18:26:35.885282] Loaded Index: blocks.1.norm2.bias from Saved Weights
[18:26:35.885289] Loaded Index: blocks.1.mlp.fc1.weight from Saved Weights
[18:26:35.885297] Loaded Index: blocks.1.mlp.fc1.bias from Saved Weights
[18:26:35.885305] Loaded Index: blocks.1.mlp.fc2.weight from Saved Weights
[18:26:35.885313] Loaded Index: blocks.1.mlp.fc2.bias from Saved Weights
[18:26:35.885320] Loaded Index: blocks.2.norm1.weight from Saved Weights
[18:26:35.885327] Loaded Index: blocks.2.norm1.bias from Saved Weights
[18:26:35.885335] Loaded Index: blocks.2.attn.qkv.weight from Saved Weights
[18:26:35.885342] Loaded Index: blocks.2.attn.qkv.bias from Saved Weights
[18:26:35.885350] Loaded Index: blocks.2.attn.proj.weight from Saved Weights
[18:26:35.885358] Loaded Index: blocks.2.attn.proj.bias from Saved Weights
[18:26:35.885365] Loaded Index: blocks.2.norm2.weight from Saved Weights
[18:26:35.885372] Loaded Index: blocks.2.norm2.bias from Saved Weights
[18:26:35.885380] Loaded Index: blocks.2.mlp.fc1.weight from Saved Weights
[18:26:35.885387] Loaded Index: blocks.2.mlp.fc1.bias from Saved Weights
[18:26:35.885395] Loaded Index: blocks.2.mlp.fc2.weight from Saved Weights
[18:26:35.885403] Loaded Index: blocks.2.mlp.fc2.bias from Saved Weights
[18:26:35.885411] Loaded Index: blocks.3.norm1.weight from Saved Weights
[18:26:35.885419] Loaded Index: blocks.3.norm1.bias from Saved Weights
[18:26:35.885427] Loaded Index: blocks.3.attn.qkv.weight from Saved Weights
[18:26:35.885434] Loaded Index: blocks.3.attn.qkv.bias from Saved Weights
[18:26:35.885442] Loaded Index: blocks.3.attn.proj.weight from Saved Weights
[18:26:35.885450] Loaded Index: blocks.3.attn.proj.bias from Saved Weights
[18:26:35.885457] Loaded Index: blocks.3.norm2.weight from Saved Weights
[18:26:35.885465] Loaded Index: blocks.3.norm2.bias from Saved Weights
[18:26:35.885473] Loaded Index: blocks.3.mlp.fc1.weight from Saved Weights
[18:26:35.885481] Loaded Index: blocks.3.mlp.fc1.bias from Saved Weights
[18:26:35.885488] Loaded Index: blocks.3.mlp.fc2.weight from Saved Weights
[18:26:35.885496] Loaded Index: blocks.3.mlp.fc2.bias from Saved Weights
[18:26:35.885503] Loaded Index: blocks.4.norm1.weight from Saved Weights
[18:26:35.885511] Loaded Index: blocks.4.norm1.bias from Saved Weights
[18:26:35.885518] Loaded Index: blocks.4.attn.qkv.weight from Saved Weights
[18:26:35.885525] Loaded Index: blocks.4.attn.qkv.bias from Saved Weights
[18:26:35.885533] Loaded Index: blocks.4.attn.proj.weight from Saved Weights
[18:26:35.885540] Loaded Index: blocks.4.attn.proj.bias from Saved Weights
[18:26:35.885548] Loaded Index: blocks.4.norm2.weight from Saved Weights
[18:26:35.885556] Loaded Index: blocks.4.norm2.bias from Saved Weights
[18:26:35.885563] Loaded Index: blocks.4.mlp.fc1.weight from Saved Weights
[18:26:35.885571] Loaded Index: blocks.4.mlp.fc1.bias from Saved Weights
[18:26:35.885579] Loaded Index: blocks.4.mlp.fc2.weight from Saved Weights
[18:26:35.885586] Loaded Index: blocks.4.mlp.fc2.bias from Saved Weights
[18:26:35.885593] Loaded Index: blocks.5.norm1.weight from Saved Weights
[18:26:35.885601] Loaded Index: blocks.5.norm1.bias from Saved Weights
[18:26:35.885608] Loaded Index: blocks.5.attn.qkv.weight from Saved Weights
[18:26:35.885615] Loaded Index: blocks.5.attn.qkv.bias from Saved Weights
[18:26:35.885623] Loaded Index: blocks.5.attn.proj.weight from Saved Weights
[18:26:35.885631] Loaded Index: blocks.5.attn.proj.bias from Saved Weights
[18:26:35.885638] Loaded Index: blocks.5.norm2.weight from Saved Weights
[18:26:35.885645] Loaded Index: blocks.5.norm2.bias from Saved Weights
[18:26:35.885653] Loaded Index: blocks.5.mlp.fc1.weight from Saved Weights
[18:26:35.885666] Loaded Index: blocks.5.mlp.fc1.bias from Saved Weights
[18:26:35.885674] Loaded Index: blocks.5.mlp.fc2.weight from Saved Weights
[18:26:35.885682] Loaded Index: blocks.5.mlp.fc2.bias from Saved Weights
[18:26:35.885690] Loaded Index: blocks.6.norm1.weight from Saved Weights
[18:26:35.885697] Loaded Index: blocks.6.norm1.bias from Saved Weights
[18:26:35.885705] Loaded Index: blocks.6.attn.qkv.weight from Saved Weights
[18:26:35.885713] Loaded Index: blocks.6.attn.qkv.bias from Saved Weights
[18:26:35.885720] Loaded Index: blocks.6.attn.proj.weight from Saved Weights
[18:26:35.885728] Loaded Index: blocks.6.attn.proj.bias from Saved Weights
[18:26:35.885735] Loaded Index: blocks.6.norm2.weight from Saved Weights
[18:26:35.885743] Loaded Index: blocks.6.norm2.bias from Saved Weights
[18:26:35.885750] Loaded Index: blocks.6.mlp.fc1.weight from Saved Weights
[18:26:35.885757] Loaded Index: blocks.6.mlp.fc1.bias from Saved Weights
[18:26:35.885765] Loaded Index: blocks.6.mlp.fc2.weight from Saved Weights
[18:26:35.885773] Loaded Index: blocks.6.mlp.fc2.bias from Saved Weights
[18:26:35.885781] Loaded Index: blocks.7.norm1.weight from Saved Weights
[18:26:35.885788] Loaded Index: blocks.7.norm1.bias from Saved Weights
[18:26:35.885796] Loaded Index: blocks.7.attn.qkv.weight from Saved Weights
[18:26:35.885804] Loaded Index: blocks.7.attn.qkv.bias from Saved Weights
[18:26:35.885812] Loaded Index: blocks.7.attn.proj.weight from Saved Weights
[18:26:35.885819] Loaded Index: blocks.7.attn.proj.bias from Saved Weights
[18:26:35.885827] Loaded Index: blocks.7.norm2.weight from Saved Weights
[18:26:35.885834] Loaded Index: blocks.7.norm2.bias from Saved Weights
[18:26:35.885842] Loaded Index: blocks.7.mlp.fc1.weight from Saved Weights
[18:26:35.885849] Loaded Index: blocks.7.mlp.fc1.bias from Saved Weights
[18:26:35.885857] Loaded Index: blocks.7.mlp.fc2.weight from Saved Weights
[18:26:35.885865] Loaded Index: blocks.7.mlp.fc2.bias from Saved Weights
[18:26:35.885872] Loaded Index: blocks.8.norm1.weight from Saved Weights
[18:26:35.885880] Loaded Index: blocks.8.norm1.bias from Saved Weights
[18:26:35.885888] Loaded Index: blocks.8.attn.qkv.weight from Saved Weights
[18:26:35.885895] Loaded Index: blocks.8.attn.qkv.bias from Saved Weights
[18:26:35.885903] Loaded Index: blocks.8.attn.proj.weight from Saved Weights
[18:26:35.885911] Loaded Index: blocks.8.attn.proj.bias from Saved Weights
[18:26:35.885918] Loaded Index: blocks.8.norm2.weight from Saved Weights
[18:26:35.885925] Loaded Index: blocks.8.norm2.bias from Saved Weights
[18:26:35.885933] Loaded Index: blocks.8.mlp.fc1.weight from Saved Weights
[18:26:35.885941] Loaded Index: blocks.8.mlp.fc1.bias from Saved Weights
[18:26:35.885949] Loaded Index: blocks.8.mlp.fc2.weight from Saved Weights
[18:26:35.885956] Loaded Index: blocks.8.mlp.fc2.bias from Saved Weights
[18:26:35.885963] Loaded Index: blocks.9.norm1.weight from Saved Weights
[18:26:35.885971] Loaded Index: blocks.9.norm1.bias from Saved Weights
[18:26:35.885978] Loaded Index: blocks.9.attn.qkv.weight from Saved Weights
[18:26:35.885986] Loaded Index: blocks.9.attn.qkv.bias from Saved Weights
[18:26:35.885993] Loaded Index: blocks.9.attn.proj.weight from Saved Weights
[18:26:35.886000] Loaded Index: blocks.9.attn.proj.bias from Saved Weights
[18:26:35.886008] Loaded Index: blocks.9.norm2.weight from Saved Weights
[18:26:35.886015] Loaded Index: blocks.9.norm2.bias from Saved Weights
[18:26:35.886023] Loaded Index: blocks.9.mlp.fc1.weight from Saved Weights
[18:26:35.886030] Loaded Index: blocks.9.mlp.fc1.bias from Saved Weights
[18:26:35.886038] Loaded Index: blocks.9.mlp.fc2.weight from Saved Weights
[18:26:35.886045] Loaded Index: blocks.9.mlp.fc2.bias from Saved Weights
[18:26:35.886053] Loaded Index: blocks.10.norm1.weight from Saved Weights
[18:26:35.886061] Loaded Index: blocks.10.norm1.bias from Saved Weights
[18:26:35.886068] Loaded Index: blocks.10.attn.qkv.weight from Saved Weights
[18:26:35.886076] Loaded Index: blocks.10.attn.qkv.bias from Saved Weights
[18:26:35.886084] Loaded Index: blocks.10.attn.proj.weight from Saved Weights
[18:26:35.886091] Loaded Index: blocks.10.attn.proj.bias from Saved Weights
[18:26:35.886099] Loaded Index: blocks.10.norm2.weight from Saved Weights
[18:26:35.886107] Loaded Index: blocks.10.norm2.bias from Saved Weights
[18:26:35.886114] Loaded Index: blocks.10.mlp.fc1.weight from Saved Weights
[18:26:35.886122] Loaded Index: blocks.10.mlp.fc1.bias from Saved Weights
[18:26:35.886129] Loaded Index: blocks.10.mlp.fc2.weight from Saved Weights
[18:26:35.886137] Loaded Index: blocks.10.mlp.fc2.bias from Saved Weights
[18:26:35.886145] Loaded Index: blocks.11.norm1.weight from Saved Weights
[18:26:35.886152] Loaded Index: blocks.11.norm1.bias from Saved Weights
[18:26:35.886160] Loaded Index: blocks.11.attn.qkv.weight from Saved Weights
[18:26:35.886168] Loaded Index: blocks.11.attn.qkv.bias from Saved Weights
[18:26:35.886175] Loaded Index: blocks.11.attn.proj.weight from Saved Weights
[18:26:35.886183] Loaded Index: blocks.11.attn.proj.bias from Saved Weights
[18:26:35.886190] Loaded Index: blocks.11.norm2.weight from Saved Weights
[18:26:35.886198] Loaded Index: blocks.11.norm2.bias from Saved Weights
[18:26:35.886206] Loaded Index: blocks.11.mlp.fc1.weight from Saved Weights
[18:26:35.886213] Loaded Index: blocks.11.mlp.fc1.bias from Saved Weights
[18:26:35.886221] Loaded Index: blocks.11.mlp.fc2.weight from Saved Weights
[18:26:35.886229] Loaded Index: blocks.11.mlp.fc2.bias from Saved Weights
[18:26:35.886236] norm.weight not found in Init Model
[18:26:35.886242] norm.bias not found in Init Model
[18:26:35.886249] decoder_embed.weight not found in Init Model
[18:26:35.886255] decoder_embed.bias not found in Init Model
[18:26:35.886261] decoder_blocks.0.norm1.weight not found in Init Model
[18:26:35.886268] decoder_blocks.0.norm1.bias not found in Init Model
[18:26:35.886275] decoder_blocks.0.attn.qkv.weight not found in Init Model
[18:26:35.886282] decoder_blocks.0.attn.qkv.bias not found in Init Model
[18:26:35.886288] decoder_blocks.0.attn.proj.weight not found in Init Model
[18:26:35.886295] decoder_blocks.0.attn.proj.bias not found in Init Model
[18:26:35.886302] decoder_blocks.0.norm2.weight not found in Init Model
[18:26:35.886308] decoder_blocks.0.norm2.bias not found in Init Model
[18:26:35.886314] decoder_blocks.0.mlp.fc1.weight not found in Init Model
[18:26:35.886321] decoder_blocks.0.mlp.fc1.bias not found in Init Model
[18:26:35.886327] decoder_blocks.0.mlp.fc2.weight not found in Init Model
[18:26:35.886334] decoder_blocks.0.mlp.fc2.bias not found in Init Model
[18:26:35.886340] decoder_blocks.1.norm1.weight not found in Init Model
[18:26:35.886346] decoder_blocks.1.norm1.bias not found in Init Model
[18:26:35.886352] decoder_blocks.1.attn.qkv.weight not found in Init Model
[18:26:35.886359] decoder_blocks.1.attn.qkv.bias not found in Init Model
[18:26:35.886365] decoder_blocks.1.attn.proj.weight not found in Init Model
[18:26:35.886372] decoder_blocks.1.attn.proj.bias not found in Init Model
[18:26:35.886378] decoder_blocks.1.norm2.weight not found in Init Model
[18:26:35.886384] decoder_blocks.1.norm2.bias not found in Init Model
[18:26:35.886391] decoder_blocks.1.mlp.fc1.weight not found in Init Model
[18:26:35.886397] decoder_blocks.1.mlp.fc1.bias not found in Init Model
[18:26:35.886403] decoder_blocks.1.mlp.fc2.weight not found in Init Model
[18:26:35.886410] decoder_blocks.1.mlp.fc2.bias not found in Init Model
[18:26:35.886416] decoder_norm.weight not found in Init Model
[18:26:35.886422] decoder_norm.bias not found in Init Model
[18:26:35.886428] decoder_pred.weight not found in Init Model
[18:26:35.886435] decoder_pred.bias not found in Init Model
[18:26:35.889101] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[18:26:35.889400] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[18:26:35.904933] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[18:26:35.906442] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[18:26:35.908294] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)
[18:26:35.908322] number of params (M): 21.67
[18:26:35.908337] base lr: 2.50e-04
[18:26:35.908344] actual lr: 3.75e-04
[18:26:35.908351] accumulate grad iterations: 1
[18:26:35.908357] effective batch size: 384
[18:26:35.908859] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)
[18:26:35.908884] number of params (M): 21.67
[18:26:35.908901] base lr: 2.50e-04
[18:26:35.908908] actual lr: 3.75e-04
[18:26:35.908915] accumulate grad iterations: 1
[18:26:35.908921] effective batch size: 384
[18:26:35.913220] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)
[18:26:35.913247] number of params (M): 21.67
[18:26:35.913263] base lr: 2.50e-04
[18:26:35.913270] actual lr: 3.75e-04
[18:26:35.913277] accumulate grad iterations: 1
[18:26:35.913283] effective batch size: 384
[18:26:35.914796] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)
[18:26:35.914822] number of params (M): 21.67
[18:26:35.914838] base lr: 2.50e-04
[18:26:35.914845] actual lr: 3.75e-04
[18:26:35.914852] accumulate grad iterations: 1
[18:26:35.914857] effective batch size: 384
[18:26:35.915663] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)
[18:26:35.915689] number of params (M): 21.67
[18:26:35.915704] base lr: 2.50e-04
[18:26:35.915710] actual lr: 3.75e-04
[18:26:35.915717] accumulate grad iterations: 1
[18:26:35.915723] effective batch size: 384
[18:26:35.916525] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[18:26:35.923393] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)
[18:26:35.923419] number of params (M): 21.67
[18:26:35.923433] base lr: 2.50e-04
[18:26:35.923440] actual lr: 3.75e-04
[18:26:35.923447] accumulate grad iterations: 1
[18:26:35.923453] effective batch size: 384
[18:26:35.924827] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)
[18:26:35.924853] number of params (M): 21.67
[18:26:35.924867] base lr: 2.50e-04
[18:26:35.924874] actual lr: 3.75e-04
[18:26:35.924880] accumulate grad iterations: 1
[18:26:35.924886] effective batch size: 384
[18:26:35.935591] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.109)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.109)
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.127)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.127)
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.145)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.145)
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.164)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.164)
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.182)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.182)
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.200)
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.200)
    )
  )
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=384, out_features=14, bias=True)
)
[18:26:35.935616] number of params (M): 21.67
[18:26:35.935630] base lr: 2.50e-04
[18:26:35.935637] actual lr: 3.75e-04
[18:26:35.935643] accumulate grad iterations: 1
[18:26:35.935649] effective batch size: 384
[18:26:35.947622] criterion = BCEWithLogitsLoss()
[18:26:35.947650] Start training for 100 epochs
[18:26:35.947702] criterion = BCEWithLogitsLoss()
[18:26:35.947728] Start training for 100 epochs
[18:26:35.947798] criterion = BCEWithLogitsLoss()
[18:26:35.947826] Start training for 100 epochs[18:26:35.947819] 
criterion = BCEWithLogitsLoss()
[18:26:35.947845] Start training for 100 epochs
[18:26:35.947889] criterion = BCEWithLogitsLoss()
[18:26:35.947916] criterion = BCEWithLogitsLoss()
[18:26:35.947942] Start training for 100 epochs
[18:26:35.947918] Start training for 100 epochs
[18:26:35.948065] criterion = BCEWithLogitsLoss()
[18:26:35.948094] Start training for 100 epochs
[18:26:35.948037] criterion = BCEWithLogitsLoss()
[18:26:35.948144] criterion = BCEWithLogitsLoss()
[18:26:35.948066] Start training for 100 epochs
[18:26:35.948174] Start training for 100 epochs
[18:26:35.948192] criterion = BCEWithLogitsLoss()
[18:26:35.948223] Start training for 100 epochs
[18:26:35.948247] criterion = BCEWithLogitsLoss()
[18:26:35.948278] Start training for 100 epochs
[18:26:35.948729] log_dir: /mnt/home/mpaez/ceph/dani_cv2/finetune/finetuned_baseline_small_vit_e1/
[18:26:35.949828] criterion = BCEWithLogitsLoss()
[18:26:35.949857] Start training for 100 epochs
[rank6]: Traceback (most recent call last):
[rank6]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank6]:     main(args)
[rank6]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank6]:     train_stats = train_one_epoch(
[rank6]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank6]:     outputs = model(samples)
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank6]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank6]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank6]:     x = self.forward_head(x)
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank6]:     x = self.fc_norm(x)
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank6]:     return F.layer_norm(
[rank6]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank6]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank6]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
[rank2]: Traceback (most recent call last):
[rank2]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank2]:     main(args)
[rank2]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank2]:     train_stats = train_one_epoch(
[rank2]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank2]:     outputs = model(samples)
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank2]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank2]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank2]:     x = self.forward_head(x)
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank2]:     x = self.fc_norm(x)
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank2]:     return F.layer_norm(
[rank2]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank2]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank2]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
[rank7]: Traceback (most recent call last):
[rank7]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank7]:     main(args)
[rank7]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank7]:     train_stats = train_one_epoch(
[rank7]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank7]:     outputs = model(samples)
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank7]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank7]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank7]:     x = self.forward_head(x)
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank7]:     x = self.fc_norm(x)
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank7]:     return F.layer_norm(
[rank7]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank7]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank7]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
[rank4]: Traceback (most recent call last):
[rank4]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank4]:     main(args)
[rank4]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank4]:     train_stats = train_one_epoch(
[rank4]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank4]:     outputs = model(samples)
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank4]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank4]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank4]:     x = self.forward_head(x)
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank4]:     x = self.fc_norm(x)
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank4]:     return F.layer_norm(
[rank4]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank4]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank4]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
[rank5]: Traceback (most recent call last):
[rank5]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank5]:     main(args)
[rank5]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank5]:     train_stats = train_one_epoch(
[rank5]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank5]:     outputs = model(samples)
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank5]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank5]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank5]:     x = self.forward_head(x)
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank5]:     x = self.fc_norm(x)
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank5]:     return F.layer_norm(
[rank5]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank5]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank5]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
[rank8]: Traceback (most recent call last):
[rank8]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank8]:     main(args)
[rank8]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank8]:     train_stats = train_one_epoch(
[rank8]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank8]:     outputs = model(samples)
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank8]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank8]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank8]:     x = self.forward_head(x)
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank8]:     x = self.fc_norm(x)
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank8]:     return F.layer_norm(
[rank8]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank8]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank8]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
[rank11]: Traceback (most recent call last):
[rank11]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank11]:     main(args)
[rank11]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank11]:     train_stats = train_one_epoch(
[rank11]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank11]:     outputs = model(samples)
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank11]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank11]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank11]:     x = self.forward_head(x)
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank11]:     x = self.fc_norm(x)
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank11]:     return F.layer_norm(
[rank11]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank11]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank11]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
[rank10]: Traceback (most recent call last):
[rank10]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank10]:     main(args)
[rank10]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank10]:     train_stats = train_one_epoch(
[rank10]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank10]:     outputs = model(samples)
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank10]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank10]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank10]:     x = self.forward_head(x)
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank10]:     x = self.fc_norm(x)
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank10]:     return F.layer_norm(
[rank10]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank10]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank10]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
[rank3]: Traceback (most recent call last):
[rank3]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank3]:     main(args)
[rank3]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank3]:     train_stats = train_one_epoch(
[rank3]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank3]:     outputs = model(samples)
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank3]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank3]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank3]:     x = self.forward_head(x)
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank3]:     x = self.fc_norm(x)
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank3]:     return F.layer_norm(
[rank3]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank3]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank3]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
[rank1]: Traceback (most recent call last):
[rank1]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank1]:     main(args)
[rank1]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank1]:     train_stats = train_one_epoch(
[rank1]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank1]:     outputs = model(samples)
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank1]:     x = self.forward_head(x)
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank1]:     x = self.fc_norm(x)
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank1]:     return F.layer_norm(
[rank1]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank1]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank1]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
[rank9]: Traceback (most recent call last):
[rank9]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank9]:     main(args)
[rank9]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank9]:     train_stats = train_one_epoch(
[rank9]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank9]:     outputs = model(samples)
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank9]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank9]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank9]:     x = self.forward_head(x)
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank9]:     x = self.fc_norm(x)
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank9]:     return F.layer_norm(
[rank9]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank9]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank9]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 459, in <module>
[rank0]:     main(args)
[rank0]:   File "/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py", line 417, in main
[rank0]:     train_stats = train_one_epoch(
[rank0]:   File "/mnt/home/mpaez/cvii-final/med_mae/engine_med_finetune.py", line 61, in train_one_epoch
[rank0]:     outputs = model(samples)
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1593, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1411, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 705, in forward
[rank0]:     x = self.forward_head(x)
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 699, in forward_head
[rank0]:     x = self.fc_norm(x)
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
[rank0]:     return F.layer_norm(
[rank0]:   File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
[rank0]:     return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
[rank0]: RuntimeError: Given normalized_shape=[384], expected input with shape [*, 384], but got input of size[32]
E0427 18:26:42.147132 23456247911936 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 923587) of binary: /mnt/home/mpaez/env/cvii-final/bin/python
E0427 18:26:42.147179 23456247911936 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 875510) of binary: /mnt/home/mpaez/env/cvii-final/bin/python
E0427 18:26:42.148146 23456247911936 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 1384880) of binary: /mnt/home/mpaez/env/cvii-final/bin/python
Traceback (most recent call last):
  File "/mnt/home/mpaez/env/cvii-final/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-04-27_18:26:42
  host      : workergpu019.cm.cluster
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 923588)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-04-27_18:26:42
  host      : workergpu019.cm.cluster
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 923589)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-04-27_18:26:42
  host      : workergpu019.cm.cluster
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 923590)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-27_18:26:42
  host      : workergpu019.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 923587)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/mnt/home/mpaez/env/cvii-final/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-04-27_18:26:42
  host      : workergpu021.cm.cluster
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 1384881)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-04-27_18:26:42
  host      : workergpu021.cm.cluster
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 1384882)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-04-27_18:26:42
  host      : workergpu021.cm.cluster
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 1384883)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-27_18:26:42
  host      : workergpu021.cm.cluster
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 1384880)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/mnt/home/mpaez/env/cvii-final/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/home/mpaez/env/cvii-final/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/mnt/home/mpaez/cvii-final/med_mae/main_med_finetune.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-04-27_18:26:42
  host      : workergpu020.cm.cluster
  rank      : 5 (local_rank: 1)
  exitcode  : 1 (pid: 875511)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-04-27_18:26:42
  host      : workergpu020.cm.cluster
  rank      : 6 (local_rank: 2)
  exitcode  : 1 (pid: 875512)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-04-27_18:26:42
  host      : workergpu020.cm.cluster
  rank      : 7 (local_rank: 3)
  exitcode  : 1 (pid: 875513)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-27_18:26:42
  host      : workergpu020.cm.cluster
  rank      : 4 (local_rank: 0)
  exitcode  : 1 (pid: 875510)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: workergpu019: task 0: Exited with exit code 1
srun: error: workergpu020: task 1: Exited with exit code 1
srun: error: workergpu021: task 2: Exited with exit code 1
