W0429 19:24:41.950506 23456247911936 torch/distributed/run.py:757] 
W0429 19:24:41.950506 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 19:24:41.950506 23456247911936 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0429 19:24:41.950506 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 19:24:41.957634 23456247911936 torch/distributed/run.py:757] 
W0429 19:24:41.957634 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 19:24:41.957634 23456247911936 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0429 19:24:41.957634 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 19:24:41.963555 23456247911936 torch/distributed/run.py:757] 
W0429 19:24:41.963555 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 19:24:41.963555 23456247911936 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0429 19:24:41.963555 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 19:24:42.127601 23456247911936 torch/distributed/run.py:757] 
W0429 19:24:42.127601 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 19:24:42.127601 23456247911936 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0429 19:24:42.127601 23456247911936 torch/distributed/run.py:757] *****************************************
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 4): env://, gpu 0
| distributed init (rank 12): env://, gpu 0
| distributed init (rank 8): env://, gpu 0
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 15): env://, gpu 3
| distributed init (rank 14): env://, gpu 2
| distributed init (rank 13): env://, gpu 1
| distributed init (rank 7): env://, gpu 3
| distributed init (rank 6): env://, gpu 2
| distributed init (rank 5): env://, gpu 1
| distributed init (rank 10): env://, gpu 2
| distributed init (rank 11): env://, gpu 3
| distributed init (rank 9): env://, gpu 1
[19:24:50.818515] [19:24:50.818518] job dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_mae

[19:24:50.818536] job dir: /mnt/home/mpaez/cvii-final/med_mae
[19:24:50.818543] job dir: /mnt/home/mpaez/cvii-final/med_mae
[19:24:50.818592] [19:24:50.818596] job dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_mae

[19:24:50.818618] [19:24:50.818619] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=3,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=3,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=2,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=2,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)

[19:24:50.818592] job dir: /mnt/home/mpaez/cvii-final/med_mae
[19:24:50.818606] job dir: /mnt/home/mpaez/cvii-final/med_mae
[19:24:50.818628] [19:24:50.818635] job dir: /mnt/home/mpaez/cvii-final/med_mae
job dir: /mnt/home/mpaez/cvii-final/med_mae
[19:24:50.818639] [19:24:50.818642] job dir: /mnt/home/mpaez/cvii-final/med_mae
job dir: /mnt/home/mpaez/cvii-final/med_mae
[19:24:50.818614] [19:24:50.818619] job dir: /mnt/home/mpaez/cvii-final/med_mae
job dir: /mnt/home/mpaez/cvii-final/med_mae
[19:24:50.818627] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=1,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=1,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
[19:24:50.818624] job dir: /mnt/home/mpaez/cvii-final/med_mae
[19:24:50.818632] job dir: /mnt/home/mpaez/cvii-final/med_mae
[19:24:50.818730] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=3,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=11,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
[19:24:50.818694] [19:24:50.818695] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=3,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=15,
repeated_aug=False,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
[19:24:50.818638] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=0,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
[19:24:50.818699] [19:24:50.818700] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=2,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=6,
repeated_aug=False,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)[19:24:50.818733] 
Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=2,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=2,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
pin_mem=True,
rank=0,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=3,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
pin_mem=True,
rank=10,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=14,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)

[19:24:50.818719] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=0,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
[19:24:50.819702] Using Directly-Resize Mode. (no RandomResizedCrop)
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=7,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
[19:24:50.818739] [19:24:50.818740] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=0,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=8,
repeated_aug=False,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=12,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)[19:24:50.818722] 
Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=1,
input_size=224,
layer_decay=0.55,
local_rank=-1,
[19:24:50.819794] Using Directly-Resize Mode. (no RandomResizedCrop)

[19:24:50.818721] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=1,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=5,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=1,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
[19:24:50.819852] Using Directly-Resize Mode. (no RandomResizedCrop)
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=9,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=13,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
[19:24:50.820043] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:50.818730] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=0,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_baseline_small_chestxray14_vit_e1/',
pin_mem=True,
rank=4,
repeated_aug=False,
resume='finetuned_baseline_small_chestxray14_50epoch.pth',
seed=0,
[19:24:50.819857] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:50.819882] Using Directly-Resize Mode. (no RandomResizedCrop)
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
[19:24:50.819900] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:50.819887] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:50.819853] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:50.819927] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:50.819985] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:50.819989] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:50.819959] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:50.820047] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:50.820011] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:50.820067] Using Directly-Resize Mode. (no RandomResizedCrop)
[19:24:51.207001] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[19:24:51.207674] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[19:24:51.209070] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[19:24:51.213031] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862feee0>
[19:24:51.214236] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862feee0>
[19:24:51.215491] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef40>
[19:24:51.215731] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef70>
[19:24:51.219730] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef40>
[19:24:51.220534] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef70>
[19:24:51.221082] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[19:24:51.222134] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef70>
[19:24:51.222879] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[19:24:51.223636] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[19:24:51.226232] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef70>
[19:24:51.227332] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fee80>
[19:24:51.227608] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862feee0>
[19:24:51.479883] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.479916] number of params (M): 21.67
[19:24:51.479928] base lr: 2.50e-04
[19:24:51.479935] actual lr: 5.00e-04
[19:24:51.479941] accumulate grad iterations: 1
[19:24:51.479947] effective batch size: 512
[19:24:51.483716] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.483742] number of params (M): 21.67
[19:24:51.483756] base lr: 2.50e-04
[19:24:51.483762] actual lr: 5.00e-04
[19:24:51.483768] accumulate grad iterations: 1
[19:24:51.483774] effective batch size: 512
[19:24:51.486601] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.486626] number of params (M): 21.67
[19:24:51.486639] base lr: 2.50e-04
[19:24:51.486646] actual lr: 5.00e-04
[19:24:51.486652] accumulate grad iterations: 1
[19:24:51.486658] effective batch size: 512
[19:24:51.495464] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.495489] number of params (M): 21.67
[19:24:51.495502] base lr: 2.50e-04
[19:24:51.495508] actual lr: 5.00e-04
[19:24:51.495514] accumulate grad iterations: 1
[19:24:51.495520] effective batch size: 512
[19:24:51.497212] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.497246] number of params (M): 21.67
[19:24:51.497259] base lr: 2.50e-04
[19:24:51.497272] actual lr: 5.00e-04
[19:24:51.497279] accumulate grad iterations: 1
[19:24:51.497285] effective batch size: 512
[19:24:51.498082] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.498110] number of params (M): 21.67
[19:24:51.498123] base lr: 2.50e-04
[19:24:51.498131] actual lr: 5.00e-04
[19:24:51.498138] accumulate grad iterations: 1
[19:24:51.498144] effective batch size: 512
[19:24:51.502937] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.502973] number of params (M): 21.67
[19:24:51.502987] base lr: 2.50e-04
[19:24:51.503006] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
[19:24:51.502994] actual lr: 5.00e-04
[19:24:51.503001] accumulate grad iterations: 1
[19:24:51.503008] effective batch size: 512
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.503038] number of params (M): 21.67
[19:24:51.503052] base lr: 2.50e-04
[19:24:51.503058] actual lr: 5.00e-04
[19:24:51.503065] accumulate grad iterations: 1
[19:24:51.503071] effective batch size: 512
[19:24:51.503999] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.504026] number of params (M): 21.67
[19:24:51.504040] base lr: 2.50e-04
[19:24:51.504048] actual lr: 5.00e-04
[19:24:51.504054] accumulate grad iterations: 1
[19:24:51.504061] effective batch size: 512
[19:24:51.505543] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.505570] number of params (M): 21.67
[19:24:51.505584] base lr: 2.50e-04
[19:24:51.505591] actual lr: 5.00e-04
[19:24:51.505597] accumulate grad iterations: 1
[19:24:51.505603] effective batch size: 512
[19:24:51.512168] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.512196] number of params (M): 21.67
[19:24:51.512209] base lr: 2.50e-04
[19:24:51.512217] actual lr: 5.00e-04
[19:24:51.512224] accumulate grad iterations: 1
[19:24:51.512231] effective batch size: 512
[19:24:51.512509] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.512570] number of params (M): 21.67
[19:24:51.512587] base lr: 2.50e-04
[19:24:51.512594] actual lr: 5.00e-04
[19:24:51.512602] accumulate grad iterations: 1
[19:24:51.512608] effective batch size: 512
[19:24:51.512937] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.512966] number of params (M): 21.67
[19:24:51.512980] base lr: 2.50e-04
[19:24:51.512987] actual lr: 5.00e-04
[19:24:51.512993] accumulate grad iterations: 1
[19:24:51.512999] effective batch size: 512
[19:24:51.523547] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.523577] number of params (M): 21.67
[19:24:51.523590] base lr: 2.50e-04
[19:24:51.523597] actual lr: 5.00e-04
[19:24:51.523604] accumulate grad iterations: 1
[19:24:51.523611] effective batch size: 512
[19:24:51.524161] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.524188] number of params (M): 21.67
[19:24:51.524200] base lr: 2.50e-04
[19:24:51.524208] actual lr: 5.00e-04
[19:24:51.524215] accumulate grad iterations: 1
[19:24:51.524221] effective batch size: 512
[19:24:51.531911] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[19:24:51.531933] number of params (M): 21.67
[19:24:51.531946] base lr: 2.50e-04
[19:24:51.531953] actual lr: 5.00e-04
[19:24:51.531959] accumulate grad iterations: 1
[19:24:51.531965] effective batch size: 512
[19:24:51.542535] criterion = BCEWithLogitsLoss()
[19:24:51.542730] criterion = BCEWithLogitsLoss()
[19:24:51.542775] criterion = BCEWithLogitsLoss()
[19:24:51.542847] criterion = BCEWithLogitsLoss()
[19:24:51.543082] criterion = BCEWithLogitsLoss()
[19:24:51.543113] criterion = BCEWithLogitsLoss()
[19:24:51.543177] criterion = BCEWithLogitsLoss()
[19:24:51.543177] [19:24:51.543177] criterion = BCEWithLogitsLoss()criterion = BCEWithLogitsLoss()

[19:24:51.543224] criterion = BCEWithLogitsLoss()
[19:24:51.543227] criterion = BCEWithLogitsLoss()
[19:24:51.543241] criterion = BCEWithLogitsLoss()
[19:24:51.543273] criterion = BCEWithLogitsLoss()
[19:24:51.543338] criterion = BCEWithLogitsLoss()
[19:24:51.543480] criterion = BCEWithLogitsLoss()
[19:24:51.543871] criterion = BCEWithLogitsLoss()
[19:24:52.033400] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.033507] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.033572] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.034187] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.035996] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.036138] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.036275] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.037551] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.040671] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.042859] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.053214] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.054007] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.054459] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.054490] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.054671] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:24:52.069291] Resume checkpoint finetuned_baseline_small_chestxray14_50epoch.pth
[19:25:06.337515] Test:  [  0/800]  eta: 3:10:41  loss: 0.3750 (0.3750)  time: 14.3023  data: 13.7191  max mem: 337
[19:25:06.349352] Test:  [  0/800]  eta: 3:10:51  loss: 0.4047 (0.4047)  time: 14.3140  data: 13.7211  max mem: 337
[19:25:06.360953] Test:  [  0/800]  eta: 3:11:00  loss: 0.4093 (0.4093)  time: 14.3256  data: 13.7291  max mem: 337
[19:25:06.415233] Test:  [  0/800]  eta: 3:11:41  loss: 0.4034 (0.4034)  time: 14.3773  data: 13.7938  max mem: 337
[19:25:06.449877] Test:  [  0/800]  eta: 3:12:09  loss: 0.4139 (0.4139)  time: 14.4122  data: 13.8140  max mem: 337
[19:25:06.466665] Test:  [  0/800]  eta: 3:12:21  loss: 0.3968 (0.3968)  time: 14.4273  data: 13.8372  max mem: 337
[19:25:06.479665] Test:  [  0/800]  eta: 3:12:11  loss: 0.4356 (0.4356)  time: 14.4140  data: 13.8543  max mem: 337
[19:25:06.491181] Test:  [  0/800]  eta: 3:12:42  loss: 0.3867 (0.3867)  time: 14.4532  data: 13.8514  max mem: 337
[19:25:06.494584] Test:  [  0/800]  eta: 3:12:46  loss: 0.4056 (0.4056)  time: 14.4587  data: 13.8962  max mem: 337
[19:25:06.550828] Test:  [  0/800]  eta: 3:13:15  loss: 0.3891 (0.3891)  time: 14.4946  data: 13.8835  max mem: 337
[19:25:06.554339] Test:  [  0/800]  eta: 3:13:06  loss: 0.3895 (0.3895)  time: 14.4833  data: 13.8685  max mem: 337
[19:25:06.613115] Test:  [  0/800]  eta: 3:14:05  loss: 0.4318 (0.4318)  time: 14.5569  data: 13.9627  max mem: 337
[19:25:06.659966] Test:  [  0/800]  eta: 3:14:47  loss: 0.4108 (0.4108)  time: 14.6100  data: 14.0286  max mem: 337
[19:25:06.724360] Test:  [  0/800]  eta: 3:15:35  loss: 0.4055 (0.4055)  time: 14.6694  data: 14.0803  max mem: 337
[19:25:06.746007] Test:  [  0/800]  eta: 3:15:51  loss: 0.3925 (0.3925)  time: 14.6896  data: 14.0995  max mem: 337
[19:25:06.857012] Test:  [  0/800]  eta: 3:17:29  loss: 0.4052 (0.4052)  time: 14.8124  data: 14.2359  max mem: 337
[19:25:23.089014] Test:  [ 10/800]  eta: 0:37:10  loss: 0.2747 (0.3247)  time: 2.8230  data: 2.7576  max mem: 337
[19:25:23.098261] Test:  [ 10/800]  eta: 0:37:10  loss: 0.2917 (0.3241)  time: 2.8239  data: 2.7580  max mem: 337
[19:25:23.135899] Test:  [ 10/800]  eta: 0:37:13  loss: 0.2542 (0.3220)  time: 2.8273  data: 2.7627  max mem: 337
[19:25:23.542297] Test:  [ 10/800]  eta: 0:37:42  loss: 0.2916 (0.3313)  time: 2.8639  data: 2.7994  max mem: 337
[19:25:23.574710] Test:  [ 10/800]  eta: 0:37:44  loss: 0.2711 (0.3267)  time: 2.8670  data: 2.8018  max mem: 337
[19:25:23.580056] Test:  [ 10/800]  eta: 0:37:45  loss: 0.2675 (0.3259)  time: 2.8674  data: 2.8036  max mem: 337
[19:25:23.603108] Test:  [ 10/800]  eta: 0:37:45  loss: 0.2565 (0.3227)  time: 2.8679  data: 2.7999  max mem: 337
[19:25:23.629910] Test:  [ 10/800]  eta: 0:37:47  loss: 0.2958 (0.3278)  time: 2.8703  data: 2.8039  max mem: 337
[19:25:23.650826] Test:  [ 10/800]  eta: 0:37:47  loss: 0.2672 (0.3261)  time: 2.8708  data: 2.8026  max mem: 337
[19:25:23.656078] Test:  [ 10/800]  eta: 0:37:50  loss: 0.2747 (0.3331)  time: 2.8745  data: 2.8119  max mem: 337
[19:25:23.707224] Test:  [ 10/800]  eta: 0:37:53  loss: 0.2837 (0.3339)  time: 2.8779  data: 2.8143  max mem: 337
[19:25:23.769114] Test:  [ 10/800]  eta: 0:37:56  loss: 0.2482 (0.3218)  time: 2.8821  data: 2.8188  max mem: 337
[19:25:23.806323] Test:  [ 10/800]  eta: 0:38:01  loss: 0.2735 (0.3203)  time: 2.8880  data: 2.8225  max mem: 337
[19:25:23.888345] Test:  [ 10/800]  eta: 0:38:06  loss: 0.2667 (0.3195)  time: 2.8939  data: 2.8296  max mem: 337
[19:25:23.906521] Test:  [ 10/800]  eta: 0:38:07  loss: 0.2612 (0.3242)  time: 2.8954  data: 2.8310  max mem: 337
[19:25:23.970740] Test:  [ 10/800]  eta: 0:38:12  loss: 0.2789 (0.3282)  time: 2.9023  data: 2.8392  max mem: 337
[19:25:48.518757] Test:  [ 20/800]  eta: 0:34:57  loss: 0.2460 (0.2933)  time: 2.1078  data: 2.0946  max mem: 337
[19:25:48.583097] Test:  [ 20/800]  eta: 0:35:00  loss: 0.2521 (0.2968)  time: 2.1116  data: 2.0985  max mem: 337
[19:25:48.630578] Test:  [ 20/800]  eta: 0:35:02  loss: 0.2497 (0.2918)  time: 2.1146  data: 2.1014  max mem: 337
[19:25:48.860955] Test:  [ 20/800]  eta: 0:35:10  loss: 0.2719 (0.2989)  time: 2.1183  data: 2.1052  max mem: 337
[19:25:49.169665] Test:  [ 20/800]  eta: 0:35:21  loss: 0.2544 (0.2947)  time: 2.1309  data: 2.1177  max mem: 337
[19:25:49.185102] Test:  [ 20/800]  eta: 0:35:21  loss: 0.2536 (0.2966)  time: 2.1286  data: 2.1153  max mem: 337
[19:25:49.194066] Test:  [ 20/800]  eta: 0:35:21  loss: 0.2486 (0.2919)  time: 2.1319  data: 2.1183  max mem: 337
[19:25:49.202772] Test:  [ 20/800]  eta: 0:35:23  loss: 0.2571 (0.2978)  time: 2.1368  data: 2.1249  max mem: 337
[19:25:49.244008] Test:  [ 20/800]  eta: 0:35:24  loss: 0.2477 (0.2950)  time: 2.1414  data: 2.1296  max mem: 337
[19:25:49.248373] Test:  [ 20/800]  eta: 0:35:24  loss: 0.2572 (0.2927)  time: 2.1399  data: 2.1281  max mem: 337
[19:25:49.271098] Test:  [ 20/800]  eta: 0:35:25  loss: 0.2555 (0.2962)  time: 2.1262  data: 2.1145  max mem: 337
[19:25:49.309500] Test:  [ 20/800]  eta: 0:35:26  loss: 0.2611 (0.2958)  time: 2.1292  data: 2.1175  max mem: 337
[19:25:49.353815] Test:  [ 20/800]  eta: 0:35:27  loss: 0.2472 (0.2938)  time: 2.1437  data: 2.1300  max mem: 337
[19:25:49.374488] Test:  [ 20/800]  eta: 0:35:29  loss: 0.2633 (0.3026)  time: 2.1357  data: 2.1240  max mem: 337
[19:25:49.390346] Test:  [ 20/800]  eta: 0:35:30  loss: 0.2526 (0.2928)  time: 2.1449  data: 2.1331  max mem: 337
[19:25:49.528792] Test:  [ 20/800]  eta: 0:35:35  loss: 0.2509 (0.2940)  time: 2.1335  data: 2.1217  max mem: 337
[19:26:05.181705] Test:  [ 30/800]  eta: 0:30:16  loss: 0.2816 (0.2938)  time: 2.1046  data: 2.0909  max mem: 337
[19:26:05.206048] Test:  [ 30/800]  eta: 0:30:17  loss: 0.2574 (0.2919)  time: 2.1053  data: 2.0916  max mem: 337
[19:26:05.215598] Test:  [ 30/800]  eta: 0:30:17  loss: 0.2605 (0.2878)  time: 2.1039  data: 2.0903  max mem: 337
[19:26:05.859396] Test:  [ 30/800]  eta: 0:30:33  loss: 0.2765 (0.2947)  time: 2.1101  data: 2.0965  max mem: 337
[19:26:06.352345] Test:  [ 30/800]  eta: 0:30:45  loss: 0.2824 (0.2967)  time: 2.1361  data: 2.1228  max mem: 337
[19:26:06.352319] Test:  [ 30/800]  eta: 0:30:45  loss: 0.2661 (0.2946)  time: 2.1405  data: 2.1288  max mem: 337
[19:26:06.352690] Test:  [ 30/800]  eta: 0:30:45  loss: 0.2660 (0.2912)  time: 2.1374  data: 2.1243  max mem: 337
[19:26:06.391531] Test:  [ 30/800]  eta: 0:30:45  loss: 0.2682 (0.2899)  time: 2.1370  data: 2.1234  max mem: 337
[19:26:06.433362] Test:  [ 30/800]  eta: 0:30:47  loss: 0.2602 (0.2897)  time: 2.1429  data: 2.1311  max mem: 337
[19:26:06.464847] Test:  [ 30/800]  eta: 0:30:48  loss: 0.2477 (0.2904)  time: 2.1442  data: 2.1325  max mem: 337
[19:26:06.501741] Test:  [ 30/800]  eta: 0:30:49  loss: 0.2654 (0.2929)  time: 2.1297  data: 2.1180  max mem: 337
[19:26:06.600521] Test:  [ 30/800]  eta: 0:30:52  loss: 0.2715 (0.2910)  time: 2.1397  data: 2.1278  max mem: 337
[19:26:06.670866] Test:  [ 30/800]  eta: 0:30:53  loss: 0.2636 (0.2927)  time: 2.1450  data: 2.1314  max mem: 337
[19:26:06.692116] Test:  [ 30/800]  eta: 0:30:54  loss: 0.2663 (0.2914)  time: 2.1360  data: 2.1242  max mem: 337
[19:26:06.704758] Test:  [ 30/800]  eta: 0:30:54  loss: 0.2788 (0.2995)  time: 2.1498  data: 2.1382  max mem: 337
[19:26:08.652396] Test:  [ 30/800]  eta: 0:31:42  loss: 0.2679 (0.2938)  time: 2.2382  data: 2.2265  max mem: 337
[19:26:31.330977] Test:  [ 40/800]  eta: 0:30:40  loss: 0.2852 (0.2942)  time: 2.1373  data: 2.1242  max mem: 337
[19:26:31.377509] Test:  [ 40/800]  eta: 0:30:41  loss: 0.2860 (0.2906)  time: 2.1373  data: 2.1240  max mem: 337
[19:26:31.606856] Test:  [ 40/800]  eta: 0:30:45  loss: 0.2863 (0.2921)  time: 2.1544  data: 2.1407  max mem: 337
[19:26:33.053697] Test:  [ 40/800]  eta: 0:31:12  loss: 0.2868 (0.2943)  time: 2.2096  data: 2.1965  max mem: 337
[19:26:33.115246] Test:  [ 40/800]  eta: 0:31:13  loss: 0.2876 (0.2939)  time: 2.1972  data: 2.1836  max mem: 337
[19:26:33.222897] Test:  [ 40/800]  eta: 0:31:15  loss: 0.2888 (0.2920)  time: 2.1989  data: 2.1872  max mem: 337
[19:26:33.226098] Test:  [ 40/800]  eta: 0:31:15  loss: 0.2802 (0.2930)  time: 2.2011  data: 2.1894  max mem: 337
[19:26:33.257750] Test:  [ 40/800]  eta: 0:31:16  loss: 0.2873 (0.2918)  time: 2.2004  data: 2.1886  max mem: 337
[19:26:33.260302] Test:  [ 40/800]  eta: 0:31:15  loss: 0.2842 (0.2927)  time: 2.1994  data: 2.1876  max mem: 337
[19:26:33.266643] Test:  [ 40/800]  eta: 0:31:16  loss: 0.2870 (0.2963)  time: 2.2040  data: 2.1904  max mem: 337
[19:26:33.288987] Test:  [ 40/800]  eta: 0:31:16  loss: 0.2907 (0.2915)  time: 2.2047  data: 2.1911  max mem: 337
[19:26:33.306890] Test:  [ 40/800]  eta: 0:31:16  loss: 0.2891 (0.2934)  time: 2.1998  data: 2.1880  max mem: 337
[19:26:33.586069] Test:  [ 40/800]  eta: 0:31:22  loss: 0.2918 (0.2924)  time: 2.2028  data: 2.1910  max mem: 337
[19:26:33.594686] Test:  [ 40/800]  eta: 0:31:21  loss: 0.3021 (0.2944)  time: 2.2120  data: 2.1983  max mem: 337
[19:26:33.707957] Test:  [ 40/800]  eta: 0:31:24  loss: 0.2914 (0.2911)  time: 2.2158  data: 2.2040  max mem: 337
[19:26:33.855372] Test:  [ 40/800]  eta: 0:31:27  loss: 0.2974 (0.2999)  time: 2.2240  data: 2.2122  max mem: 337
[19:26:48.384116] Test:  [ 50/800]  eta: 0:28:30  loss: 0.3017 (0.3057)  time: 2.1601  data: 2.1469  max mem: 337
[19:26:48.388034] Test:  [ 50/800]  eta: 0:28:31  loss: 0.3041 (0.3030)  time: 2.1586  data: 2.1453  max mem: 337
[19:26:48.434931] Test:  [ 50/800]  eta: 0:28:31  loss: 0.2967 (0.3032)  time: 2.1614  data: 2.1477  max mem: 337
[19:26:49.738491] Test:  [ 50/800]  eta: 0:28:50  loss: 0.2900 (0.3030)  time: 2.1939  data: 2.1808  max mem: 337
[19:26:50.692864] Test:  [ 50/800]  eta: 0:29:04  loss: 0.3137 (0.3060)  time: 2.2170  data: 2.2035  max mem: 337
[19:26:50.775951] Test:  [ 50/800]  eta: 0:29:05  loss: 0.3113 (0.3040)  time: 2.2192  data: 2.2060  max mem: 337
[19:26:50.776407] Test:  [ 50/800]  eta: 0:29:05  loss: 0.3035 (0.3037)  time: 2.2211  data: 2.2076  max mem: 337
[19:26:50.799627] Test:  [ 50/800]  eta: 0:29:06  loss: 0.3189 (0.3045)  time: 2.2223  data: 2.2104  max mem: 337
[19:26:50.846438] Test:  [ 50/800]  eta: 0:29:07  loss: 0.3137 (0.3028)  time: 2.2206  data: 2.2089  max mem: 337
[19:26:50.890842] Test:  [ 50/800]  eta: 0:29:07  loss: 0.2952 (0.3021)  time: 2.2213  data: 2.2095  max mem: 337
[19:26:50.968841] Test:  [ 50/800]  eta: 0:29:08  loss: 0.3078 (0.3087)  time: 2.2131  data: 2.2015  max mem: 337
[19:26:51.007159] Test:  [ 50/800]  eta: 0:29:09  loss: 0.3064 (0.3019)  time: 2.2203  data: 2.2086  max mem: 337
[19:26:51.031126] Test:  [ 50/800]  eta: 0:29:09  loss: 0.3009 (0.3044)  time: 2.2264  data: 2.2147  max mem: 337
[19:26:51.040093] Test:  [ 50/800]  eta: 0:29:09  loss: 0.3025 (0.3057)  time: 2.2184  data: 2.2052  max mem: 337
[19:26:51.066990] Test:  [ 50/800]  eta: 0:29:10  loss: 0.3053 (0.3027)  time: 2.2187  data: 2.2069  max mem: 337
[19:26:54.067698] Test:  [ 50/800]  eta: 0:29:54  loss: 0.3079 (0.3036)  time: 2.2707  data: 2.2590  max mem: 337
[19:27:12.678088] Test:  [ 60/800]  eta: 0:28:26  loss: 0.2822 (0.2942)  time: 2.0673  data: 2.0537  max mem: 337
[19:27:12.689962] Test:  [ 60/800]  eta: 0:28:26  loss: 0.2765 (0.2907)  time: 2.0656  data: 2.0519  max mem: 337
[19:27:12.927718] Test:  [ 60/800]  eta: 0:28:29  loss: 0.2852 (0.2923)  time: 2.0660  data: 2.0523  max mem: 337
[19:27:15.457489] Test:  [ 60/800]  eta: 0:28:59  loss: 0.2748 (0.2920)  time: 2.1201  data: 2.1065  max mem: 337
[19:27:15.840562] Test:  [ 60/800]  eta: 0:29:04  loss: 0.2918 (0.2917)  time: 2.1266  data: 2.1149  max mem: 337
[19:27:15.868324] Test:  [ 60/800]  eta: 0:29:04  loss: 0.2751 (0.2912)  time: 2.1304  data: 2.1186  max mem: 337
[19:27:15.893721] Test:  [ 60/800]  eta: 0:29:04  loss: 0.2772 (0.2917)  time: 2.1302  data: 2.1170  max mem: 337
[19:27:15.912392] Test:  [ 60/800]  eta: 0:29:05  loss: 0.2900 (0.2916)  time: 2.1327  data: 2.1211  max mem: 337
[19:27:15.934804] Test:  [ 60/800]  eta: 0:29:05  loss: 0.2787 (0.2926)  time: 2.1409  data: 2.1274  max mem: 337
[19:27:15.937725] Test:  [ 60/800]  eta: 0:29:05  loss: 0.2800 (0.2909)  time: 2.1357  data: 2.1239  max mem: 337
[19:27:15.937939] Test:  [ 60/800]  eta: 0:29:05  loss: 0.2736 (0.2938)  time: 2.1335  data: 2.1201  max mem: 337
[19:27:15.950536] Test:  [ 60/800]  eta: 0:29:05  loss: 0.2826 (0.2932)  time: 2.1362  data: 2.1243  max mem: 337
[19:27:16.164072] Test:  [ 60/800]  eta: 0:29:08  loss: 0.2803 (0.2924)  time: 2.1289  data: 2.1171  max mem: 337
[19:27:16.438542] Test:  [ 60/800]  eta: 0:29:11  loss: 0.2826 (0.2909)  time: 2.1365  data: 2.1249  max mem: 337
[19:27:16.471220] Test:  [ 60/800]  eta: 0:29:11  loss: 0.2827 (0.2932)  time: 2.1438  data: 2.1306  max mem: 337
[19:27:16.696730] Test:  [ 60/800]  eta: 0:29:14  loss: 0.2622 (0.2955)  time: 2.1420  data: 2.1303  max mem: 337
[19:27:30.215816] Test:  [ 70/800]  eta: 0:27:06  loss: 0.2500 (0.2924)  time: 2.0890  data: 2.0753  max mem: 337
[19:27:30.266421] Test:  [ 70/800]  eta: 0:27:06  loss: 0.2458 (0.2912)  time: 2.0939  data: 2.0802  max mem: 337
[19:27:30.297578] Test:  [ 70/800]  eta: 0:27:07  loss: 0.2548 (0.2941)  time: 2.0956  data: 2.0823  max mem: 337
[19:27:32.877794] Test:  [ 70/800]  eta: 0:27:33  loss: 0.2455 (0.2927)  time: 2.1569  data: 2.1433  max mem: 337
[19:27:33.815330] Test:  [ 70/800]  eta: 0:27:43  loss: 0.2560 (0.2930)  time: 2.1519  data: 2.1382  max mem: 337
[19:27:33.849407] Test:  [ 70/800]  eta: 0:27:43  loss: 0.2372 (0.2932)  time: 2.1578  data: 2.1441  max mem: 337
[19:27:33.955173] Test:  [ 70/800]  eta: 0:27:44  loss: 0.2389 (0.2927)  time: 2.1589  data: 2.1453  max mem: 337
[19:27:34.002006] Test:  [ 70/800]  eta: 0:27:45  loss: 0.2488 (0.2916)  time: 2.1555  data: 2.1437  max mem: 337
[19:27:34.037599] Test:  [ 70/800]  eta: 0:27:45  loss: 0.2571 (0.2939)  time: 2.1619  data: 2.1500  max mem: 337
[19:27:34.041375] Test:  [ 70/800]  eta: 0:27:45  loss: 0.2463 (0.2915)  time: 2.1517  data: 2.1399  max mem: 337
[19:27:34.077253] Test:  [ 70/800]  eta: 0:27:45  loss: 0.2441 (0.2926)  time: 2.1518  data: 2.1381  max mem: 337
[19:27:34.089543] Test:  [ 70/800]  eta: 0:27:46  loss: 0.2649 (0.2932)  time: 2.1621  data: 2.1503  max mem: 337
[19:27:34.295576] Test:  [ 70/800]  eta: 0:27:48  loss: 0.2615 (0.2922)  time: 2.1614  data: 2.1496  max mem: 337
[19:27:34.300903] Test:  [ 70/800]  eta: 0:27:48  loss: 0.2466 (0.2968)  time: 2.1665  data: 2.1547  max mem: 337
[19:27:34.309490] Test:  [ 70/800]  eta: 0:27:48  loss: 0.2479 (0.2917)  time: 2.1639  data: 2.1521  max mem: 337
[19:27:37.144240] Test:  [ 70/800]  eta: 0:28:17  loss: 0.2440 (0.2933)  time: 2.1538  data: 2.1421  max mem: 337
[19:27:55.227270] Test:  [ 80/800]  eta: 0:27:08  loss: 0.2584 (0.2909)  time: 2.1274  data: 2.1140  max mem: 337
[19:27:55.282074] Test:  [ 80/800]  eta: 0:27:08  loss: 0.2679 (0.2883)  time: 2.1296  data: 2.1159  max mem: 337
[19:27:55.536653] Test:  [ 80/800]  eta: 0:27:11  loss: 0.2593 (0.2893)  time: 2.1304  data: 2.1167  max mem: 337
[19:27:59.389694] Test:  [ 80/800]  eta: 0:27:45  loss: 0.2527 (0.2889)  time: 2.1966  data: 2.1830  max mem: 337
[19:27:59.618751] Test:  [ 80/800]  eta: 0:27:47  loss: 0.2552 (0.2900)  time: 2.1840  data: 2.1705  max mem: 337
[19:27:59.662584] Test:  [ 80/800]  eta: 0:27:47  loss: 0.2650 (0.2888)  time: 2.1863  data: 2.1727  max mem: 337
[19:27:59.715674] Test:  [ 80/800]  eta: 0:27:48  loss: 0.2642 (0.2882)  time: 2.1923  data: 2.1806  max mem: 337
[19:27:59.736420] Test:  [ 80/800]  eta: 0:27:48  loss: 0.2689 (0.2892)  time: 2.1912  data: 2.1794  max mem: 337
[19:27:59.736505] Test:  [ 80/800]  eta: 0:27:48  loss: 0.2542 (0.2889)  time: 2.1899  data: 2.1781  max mem: 337
[19:27:59.774748] Test:  [ 80/800]  eta: 0:27:48  loss: 0.2549 (0.2902)  time: 2.1967  data: 2.1849  max mem: 337
[19:27:59.843964] Test:  [ 80/800]  eta: 0:27:49  loss: 0.2615 (0.2888)  time: 2.1839  data: 2.1721  max mem: 337
[19:27:59.880889] Test:  [ 80/800]  eta: 0:27:49  loss: 0.2692 (0.2900)  time: 2.1993  data: 2.1859  max mem: 337
[19:28:00.003684] Test:  [ 80/800]  eta: 0:27:50  loss: 0.2577 (0.2900)  time: 2.2026  data: 2.1907  max mem: 337
[19:28:00.229854] Test:  [ 80/800]  eta: 0:27:52  loss: 0.2684 (0.2888)  time: 2.1879  data: 2.1747  max mem: 337
[19:28:00.241159] Test:  [ 80/800]  eta: 0:27:52  loss: 0.2692 (0.2883)  time: 2.1901  data: 2.1784  max mem: 337
[19:28:00.499560] Test:  [ 80/800]  eta: 0:27:55  loss: 0.2577 (0.2930)  time: 2.1901  data: 2.1783  max mem: 337
[19:28:12.223052] Test:  [ 90/800]  eta: 0:26:01  loss: 0.2570 (0.2865)  time: 2.1003  data: 2.0866  max mem: 337
[19:28:12.234263] Test:  [ 90/800]  eta: 0:26:01  loss: 0.2470 (0.2884)  time: 2.0968  data: 2.0832  max mem: 337
[19:28:12.255488] Test:  [ 90/800]  eta: 0:26:02  loss: 0.2463 (0.2856)  time: 2.0994  data: 2.0857  max mem: 337
[19:28:16.274996] Test:  [ 90/800]  eta: 0:26:33  loss: 0.2527 (0.2866)  time: 2.1698  data: 2.1562  max mem: 337
[19:28:16.897468] Test:  [ 90/800]  eta: 0:26:38  loss: 0.2634 (0.2861)  time: 2.1541  data: 2.1404  max mem: 337
[19:28:16.989229] Test:  [ 90/800]  eta: 0:26:39  loss: 0.2575 (0.2866)  time: 2.1493  data: 2.1376  max mem: 337
[19:28:17.011554] Test:  [ 90/800]  eta: 0:26:39  loss: 0.2552 (0.2874)  time: 2.1581  data: 2.1446  max mem: 337
[19:28:17.023806] Test:  [ 90/800]  eta: 0:26:39  loss: 0.2574 (0.2861)  time: 2.1467  data: 2.1349  max mem: 337
[19:28:17.061680] Test:  [ 90/800]  eta: 0:26:39  loss: 0.2557 (0.2884)  time: 2.1512  data: 2.1393  max mem: 337
[19:28:17.081471] Test:  [ 90/800]  eta: 0:26:39  loss: 0.2568 (0.2873)  time: 2.1563  data: 2.1428  max mem: 337
[19:28:17.266936] Test:  [ 90/800]  eta: 0:26:41  loss: 0.2436 (0.2856)  time: 2.1612  data: 2.1495  max mem: 337
[19:28:17.316428] Test:  [ 90/800]  eta: 0:26:41  loss: 0.2520 (0.2859)  time: 2.1503  data: 2.1384  max mem: 337
[19:28:17.359145] Test:  [ 90/800]  eta: 0:26:41  loss: 0.2427 (0.2860)  time: 2.1640  data: 2.1509  max mem: 337
[19:28:17.376295] Test:  [ 90/800]  eta: 0:26:42  loss: 0.2460 (0.2867)  time: 2.1540  data: 2.1422  max mem: 337
[19:28:17.757907] Test:  [ 90/800]  eta: 0:26:44  loss: 0.2493 (0.2900)  time: 2.1728  data: 2.1610  max mem: 337
[19:28:20.323395] Test:  [ 90/800]  eta: 0:27:04  loss: 0.2549 (0.2871)  time: 2.1589  data: 2.1470  max mem: 337
[19:28:36.006566] Test:  [100/800]  eta: 0:25:52  loss: 0.2504 (0.2870)  time: 2.0362  data: 2.0225  max mem: 337
[19:28:36.119494] Test:  [100/800]  eta: 0:25:53  loss: 0.2445 (0.2885)  time: 2.0446  data: 2.0310  max mem: 337
[19:28:36.233899] Test:  [100/800]  eta: 0:25:53  loss: 0.2492 (0.2874)  time: 2.0348  data: 2.0211  max mem: 337
[19:28:40.725637] Test:  [100/800]  eta: 0:26:24  loss: 0.2541 (0.2870)  time: 2.0668  data: 2.0531  max mem: 337
[19:28:41.685956] Test:  [100/800]  eta: 0:26:31  loss: 0.2461 (0.2868)  time: 2.0985  data: 2.0866  max mem: 337
[19:28:41.698333] Test:  [100/800]  eta: 0:26:31  loss: 0.2443 (0.2867)  time: 2.1017  data: 2.0881  max mem: 337
[19:28:41.720259] Test:  [100/800]  eta: 0:26:31  loss: 0.2572 (0.2879)  time: 2.0972  data: 2.0855  max mem: 337
[19:28:41.819067] Test:  [100/800]  eta: 0:26:32  loss: 0.2356 (0.2865)  time: 2.1041  data: 2.0923  max mem: 337
[19:28:41.819097] Test:  [100/800]  eta: 0:26:32  loss: 0.2438 (0.2870)  time: 2.1041  data: 2.0922  max mem: 337
[19:28:41.850444] Test:  [100/800]  eta: 0:26:32  loss: 0.2531 (0.2884)  time: 2.1115  data: 2.0980  max mem: 337
[19:28:41.878491] Test:  [100/800]  eta: 0:26:32  loss: 0.2541 (0.2870)  time: 2.1017  data: 2.0899  max mem: 337
[19:28:41.884939] Test:  [100/800]  eta: 0:26:32  loss: 0.2568 (0.2877)  time: 2.1002  data: 2.0866  max mem: 337
[19:28:42.035804] Test:  [100/800]  eta: 0:26:34  loss: 0.2501 (0.2884)  time: 2.1016  data: 2.0897  max mem: 337
[19:28:42.372598] Test:  [100/800]  eta: 0:26:36  loss: 0.2436 (0.2860)  time: 2.1065  data: 2.0947  max mem: 337
[19:28:42.396111] Test:  [100/800]  eta: 0:26:36  loss: 0.2454 (0.2869)  time: 2.1083  data: 2.0947  max mem: 337
[19:28:42.576837] Test:  [100/800]  eta: 0:26:37  loss: 0.2462 (0.2903)  time: 2.1038  data: 2.0920  max mem: 337
[19:28:54.064678] Test:  [110/800]  eta: 0:25:04  loss: 0.2883 (0.2864)  time: 2.0904  data: 2.0770  max mem: 337
[19:28:54.113074] Test:  [110/800]  eta: 0:25:04  loss: 0.2692 (0.2863)  time: 2.0945  data: 2.0813  max mem: 337
[19:28:54.415892] Test:  [110/800]  eta: 0:25:06  loss: 0.2684 (0.2877)  time: 2.1090  data: 2.0957  max mem: 337
[19:28:56.610245] Test:  [110/800]  eta: 0:25:20  loss: 0.2750 (0.2878)  time: 2.0167  data: 2.0031  max mem: 337
[19:29:00.258981] Test:  [110/800]  eta: 0:25:42  loss: 0.2589 (0.2859)  time: 2.1496  data: 2.1379  max mem: 337
[19:29:00.285086] Test:  [110/800]  eta: 0:25:42  loss: 0.2765 (0.2876)  time: 2.1601  data: 2.1470  max mem: 337
[19:29:00.289650] Test:  [110/800]  eta: 0:25:43  loss: 0.2541 (0.2875)  time: 2.1639  data: 2.1505  max mem: 337
[19:29:00.306362] Test:  [110/800]  eta: 0:25:43  loss: 0.2557 (0.2857)  time: 2.1641  data: 2.1525  max mem: 337
[19:29:00.317787] Test:  [110/800]  eta: 0:25:43  loss: 0.2662 (0.2867)  time: 2.1710  data: 2.1573  max mem: 337
[19:29:00.386897] Test:  [110/800]  eta: 0:25:43  loss: 0.2652 (0.2864)  time: 2.1513  data: 2.1380  max mem: 337
[19:29:00.401320] Test:  [110/800]  eta: 0:25:43  loss: 0.2611 (0.2886)  time: 2.1669  data: 2.1551  max mem: 337
[19:29:00.477891] Test:  [110/800]  eta: 0:25:44  loss: 0.2845 (0.2864)  time: 2.1744  data: 2.1626  max mem: 337
[19:29:00.523912] Test:  [110/800]  eta: 0:25:44  loss: 0.2566 (0.2859)  time: 2.1603  data: 2.1485  max mem: 337
[19:29:00.544962] Test:  [110/800]  eta: 0:25:44  loss: 0.2682 (0.2900)  time: 2.1393  data: 2.1275  max mem: 337
[19:29:00.657099] Test:  [110/800]  eta: 0:25:45  loss: 0.2590 (0.2862)  time: 2.1640  data: 2.1522  max mem: 337
[19:29:03.495751] Test:  [110/800]  eta: 0:26:02  loss: 0.2610 (0.2874)  time: 2.1586  data: 2.1469  max mem: 337
[19:29:19.042923] Test:  [120/800]  eta: 0:25:00  loss: 0.2566 (0.2806)  time: 2.1518  data: 2.1384  max mem: 337
[19:29:19.047648] Test:  [120/800]  eta: 0:25:00  loss: 0.2585 (0.2806)  time: 2.1406  data: 2.1274  max mem: 337
[19:29:19.067198] Test:  [120/800]  eta: 0:25:00  loss: 0.2577 (0.2824)  time: 2.1473  data: 2.1340  max mem: 337
[19:29:23.336355] Test:  [120/800]  eta: 0:25:24  loss: 0.2676 (0.2828)  time: 2.1305  data: 2.1174  max mem: 337
[19:29:25.577460] Test:  [120/800]  eta: 0:25:37  loss: 0.2500 (0.2818)  time: 2.1928  data: 2.1810  max mem: 337
[19:29:25.614163] Test:  [120/800]  eta: 0:25:37  loss: 0.2466 (0.2802)  time: 2.1964  data: 2.1845  max mem: 337
[19:29:25.774277] Test:  [120/800]  eta: 0:25:38  loss: 0.2648 (0.2806)  time: 2.1977  data: 2.1861  max mem: 337
[19:29:25.816749] Test:  [120/800]  eta: 0:25:38  loss: 0.2654 (0.2810)  time: 2.1998  data: 2.1881  max mem: 337
[19:29:25.836343] Test:  [120/800]  eta: 0:25:38  loss: 0.2477 (0.2809)  time: 2.1978  data: 2.1860  max mem: 337
[19:29:25.879241] Test:  [120/800]  eta: 0:25:38  loss: 0.2580 (0.2821)  time: 2.1997  data: 2.1864  max mem: 337
[19:29:25.897806] Test:  [120/800]  eta: 0:25:38  loss: 0.2592 (0.2815)  time: 2.2099  data: 2.1963  max mem: 337
[19:29:25.949011] Test:  [120/800]  eta: 0:25:39  loss: 0.2518 (0.2823)  time: 2.2049  data: 2.1914  max mem: 337
[19:29:26.243379] Test:  [120/800]  eta: 0:25:40  loss: 0.2611 (0.2829)  time: 2.2103  data: 2.1986  max mem: 337
[19:29:26.401204] Test:  [120/800]  eta: 0:25:41  loss: 0.2589 (0.2808)  time: 2.2014  data: 2.1897  max mem: 337
[19:29:26.487540] Test:  [120/800]  eta: 0:25:42  loss: 0.2652 (0.2812)  time: 2.2045  data: 2.1911  max mem: 337
[19:29:26.622470] Test:  [120/800]  eta: 0:25:43  loss: 0.2627 (0.2842)  time: 2.2022  data: 2.1904  max mem: 337
[19:29:36.818806] Test:  [130/800]  eta: 0:24:16  loss: 0.2431 (0.2797)  time: 2.1377  data: 2.1240  max mem: 337
[19:29:36.831263] Test:  [130/800]  eta: 0:24:16  loss: 0.2512 (0.2805)  time: 2.1359  data: 2.1222  max mem: 337
[19:29:36.871752] Test:  [130/800]  eta: 0:24:16  loss: 0.2565 (0.2815)  time: 2.1227  data: 2.1091  max mem: 337
[19:29:41.073799] Test:  [130/800]  eta: 0:24:38  loss: 0.2538 (0.2817)  time: 2.2231  data: 2.2100  max mem: 337
[19:29:43.874336] Test:  [130/800]  eta: 0:24:52  loss: 0.2544 (0.2798)  time: 2.1675  data: 2.1556  max mem: 337
[19:29:43.996004] Test:  [130/800]  eta: 0:24:53  loss: 0.2603 (0.2800)  time: 2.1844  data: 2.1726  max mem: 337
[19:29:44.054127] Test:  [130/800]  eta: 0:24:53  loss: 0.2589 (0.2821)  time: 2.1884  data: 2.1748  max mem: 337
[19:29:44.059707] Test:  [130/800]  eta: 0:24:53  loss: 0.2586 (0.2812)  time: 2.1790  data: 2.1672  max mem: 337
[19:29:44.064216] Test:  [130/800]  eta: 0:24:53  loss: 0.2618 (0.2807)  time: 2.1703  data: 2.1585  max mem: 337
[19:29:44.092987] Test:  [130/800]  eta: 0:24:53  loss: 0.2499 (0.2808)  time: 2.1887  data: 2.1751  max mem: 337
[19:29:44.095258] Test:  [130/800]  eta: 0:24:53  loss: 0.2526 (0.2818)  time: 2.1902  data: 2.1766  max mem: 337
[19:29:44.453944] Test:  [130/800]  eta: 0:24:55  loss: 0.2513 (0.2824)  time: 2.2026  data: 2.1908  max mem: 337
[19:29:44.616743] Test:  [130/800]  eta: 0:24:56  loss: 0.2463 (0.2803)  time: 2.2178  data: 2.2060  max mem: 337
[19:29:44.693312] Test:  [130/800]  eta: 0:24:56  loss: 0.2695 (0.2805)  time: 2.2153  data: 2.2016  max mem: 337
[19:29:44.864206] Test:  [130/800]  eta: 0:24:57  loss: 0.2489 (0.2836)  time: 2.2159  data: 2.2041  max mem: 337
[19:29:46.850265] Test:  [130/800]  eta: 0:25:07  loss: 0.2500 (0.2816)  time: 2.1677  data: 2.1559  max mem: 337
[19:30:01.567282] Test:  [140/800]  eta: 0:24:08  loss: 0.3234 (0.2831)  time: 2.1259  data: 2.1127  max mem: 337
[19:30:01.633899] Test:  [140/800]  eta: 0:24:09  loss: 0.3013 (0.2846)  time: 2.1283  data: 2.1146  max mem: 337
[19:30:01.635215] Test:  [140/800]  eta: 0:24:09  loss: 0.3090 (0.2830)  time: 2.1296  data: 2.1159  max mem: 337
[19:30:05.882194] Test:  [140/800]  eta: 0:24:29  loss: 0.3061 (0.2845)  time: 2.1272  data: 2.1136  max mem: 337
[19:30:09.307566] Test:  [140/800]  eta: 0:24:44  loss: 0.3180 (0.2825)  time: 2.1846  data: 2.1729  max mem: 337
[19:30:09.313645] Test:  [140/800]  eta: 0:24:45  loss: 0.3067 (0.2846)  time: 2.1868  data: 2.1749  max mem: 337
[19:30:09.525468] Test:  [140/800]  eta: 0:24:46  loss: 0.3094 (0.2834)  time: 2.1844  data: 2.1728  max mem: 337
[19:30:09.532021] Test:  [140/800]  eta: 0:24:46  loss: 0.3185 (0.2836)  time: 2.1857  data: 2.1739  max mem: 337
[19:30:09.537956] Test:  [140/800]  eta: 0:24:45  loss: 0.3183 (0.2852)  time: 2.1829  data: 2.1692  max mem: 337
[19:30:09.539010] Test:  [140/800]  eta: 0:24:46  loss: 0.3108 (0.2829)  time: 2.1882  data: 2.1764  max mem: 337
[19:30:09.541060] Test:  [140/800]  eta: 0:24:46  loss: 0.3142 (0.2848)  time: 2.1796  data: 2.1659  max mem: 337
[19:30:09.543598] Test:  [140/800]  eta: 0:24:46  loss: 0.3070 (0.2835)  time: 2.1822  data: 2.1686  max mem: 337
[19:30:09.985192] Test:  [140/800]  eta: 0:24:48  loss: 0.3034 (0.2848)  time: 2.1870  data: 2.1752  max mem: 337
[19:30:10.001563] Test:  [140/800]  eta: 0:24:48  loss: 0.3058 (0.2830)  time: 2.1800  data: 2.1681  max mem: 337
[19:30:10.269338] Test:  [140/800]  eta: 0:24:49  loss: 0.3124 (0.2835)  time: 2.1890  data: 2.1754  max mem: 337
[19:30:10.364953] Test:  [140/800]  eta: 0:24:49  loss: 0.3225 (0.2863)  time: 2.1871  data: 2.1752  max mem: 337
[19:30:20.043805] Test:  [150/800]  eta: 0:23:31  loss: 0.3203 (0.2829)  time: 2.1586  data: 2.1449  max mem: 337
[19:30:20.048529] Test:  [150/800]  eta: 0:23:31  loss: 0.3211 (0.2825)  time: 2.1614  data: 2.1478  max mem: 337
[19:30:20.080868] Test:  [150/800]  eta: 0:23:32  loss: 0.3225 (0.2817)  time: 2.1624  data: 2.1492  max mem: 337
[19:30:24.334952] Test:  [150/800]  eta: 0:23:50  loss: 0.3246 (0.2832)  time: 2.1630  data: 2.1499  max mem: 337
[19:30:28.253831] Test:  [150/800]  eta: 0:24:07  loss: 0.3297 (0.2811)  time: 2.2189  data: 2.2072  max mem: 337
[19:30:28.435365] Test:  [150/800]  eta: 0:24:07  loss: 0.3299 (0.2839)  time: 2.2190  data: 2.2054  max mem: 337
[19:30:28.445087] Test:  [150/800]  eta: 0:24:08  loss: 0.2964 (0.2822)  time: 2.2190  data: 2.2074  max mem: 337
[19:30:28.454914] Test:  [150/800]  eta: 0:24:08  loss: 0.3169 (0.2822)  time: 2.2180  data: 2.2046  max mem: 337
[19:30:28.467846] Test:  [150/800]  eta: 0:24:08  loss: 0.3163 (0.2827)  time: 2.2204  data: 2.2086  max mem: 337
[19:30:28.481455] Test:  [150/800]  eta: 0:24:08  loss: 0.3151 (0.2831)  time: 2.2193  data: 2.2059  max mem: 337
[19:30:28.485857] Test:  [150/800]  eta: 0:24:08  loss: 0.3180 (0.2822)  time: 2.2244  data: 2.2126  max mem: 337
[19:30:28.999656] Test:  [150/800]  eta: 0:24:10  loss: 0.3034 (0.2835)  time: 2.2272  data: 2.2154  max mem: 337
[19:30:29.039497] Test:  [150/800]  eta: 0:24:10  loss: 0.3043 (0.2816)  time: 2.2211  data: 2.2093  max mem: 337
[19:30:29.215738] Test:  [150/800]  eta: 0:24:11  loss: 0.3148 (0.2820)  time: 2.2261  data: 2.2124  max mem: 337
[19:30:29.368349] Test:  [150/800]  eta: 0:24:11  loss: 0.3250 (0.2850)  time: 2.2251  data: 2.2133  max mem: 337
[19:30:30.948466] Test:  [150/800]  eta: 0:24:18  loss: 0.3199 (0.2834)  time: 2.2049  data: 2.1930  max mem: 337
[19:30:46.089418] Test:  [160/800]  eta: 0:23:27  loss: 0.2757 (0.2855)  time: 2.2227  data: 2.2091  max mem: 337
[19:30:46.089859] Test:  [160/800]  eta: 0:23:27  loss: 0.2753 (0.2853)  time: 2.2227  data: 2.2090  max mem: 337
[19:30:46.103120] Test:  [160/800]  eta: 0:23:27  loss: 0.2657 (0.2847)  time: 2.2267  data: 2.2131  max mem: 337
[19:30:50.546067] Test:  [160/800]  eta: 0:23:45  loss: 0.2750 (0.2857)  time: 2.2331  data: 2.2200  max mem: 337
[19:30:55.226268] Test:  [160/800]  eta: 0:24:03  loss: 0.2863 (0.2862)  time: 2.2956  data: 2.2837  max mem: 337
[19:30:55.273246] Test:  [160/800]  eta: 0:24:03  loss: 0.2797 (0.2843)  time: 2.2982  data: 2.2864  max mem: 337
[19:30:55.355465] Test:  [160/800]  eta: 0:24:04  loss: 0.2705 (0.2851)  time: 2.2905  data: 2.2771  max mem: 337
[19:30:55.381254] Test:  [160/800]  eta: 0:24:04  loss: 0.2709 (0.2854)  time: 2.2920  data: 2.2786  max mem: 337
[19:30:55.471165] Test:  [160/800]  eta: 0:24:04  loss: 0.2888 (0.2850)  time: 2.2972  data: 2.2854  max mem: 337
[19:30:55.480442] Test:  [160/800]  eta: 0:24:04  loss: 0.2658 (0.2850)  time: 2.2974  data: 2.2856  max mem: 337
[19:30:55.509476] Test:  [160/800]  eta: 0:24:04  loss: 0.2712 (0.2850)  time: 2.2985  data: 2.2867  max mem: 337
[19:30:55.510609] Test:  [160/800]  eta: 0:24:04  loss: 0.2666 (0.2864)  time: 2.2986  data: 2.2850  max mem: 337
[19:30:56.111981] Test:  [160/800]  eta: 0:24:07  loss: 0.2712 (0.2843)  time: 2.3055  data: 2.2936  max mem: 337
[19:30:56.115757] Test:  [160/800]  eta: 0:24:07  loss: 0.2825 (0.2860)  time: 2.3065  data: 2.2947  max mem: 337
[19:30:56.278374] Test:  [160/800]  eta: 0:24:07  loss: 0.2637 (0.2847)  time: 2.3004  data: 2.2867  max mem: 337
[19:30:56.388468] Test:  [160/800]  eta: 0:24:08  loss: 0.2593 (0.2876)  time: 2.3011  data: 2.2893  max mem: 337
[19:31:03.525018] Test:  [170/800]  eta: 0:22:48  loss: 0.3076 (0.2865)  time: 2.1740  data: 2.1604  max mem: 337
[19:31:03.577375] Test:  [170/800]  eta: 0:22:48  loss: 0.3052 (0.2869)  time: 2.1764  data: 2.1627  max mem: 337
[19:31:03.577802] Test:  [170/800]  eta: 0:22:48  loss: 0.3015 (0.2862)  time: 2.1748  data: 2.1611  max mem: 337
[19:31:08.073664] Test:  [170/800]  eta: 0:23:05  loss: 0.2933 (0.2868)  time: 2.1869  data: 2.1733  max mem: 337
[19:31:13.317722] Test:  [170/800]  eta: 0:23:24  loss: 0.3010 (0.2865)  time: 2.2418  data: 2.2281  max mem: 337
[19:31:13.342679] Test:  [170/800]  eta: 0:23:24  loss: 0.3140 (0.2859)  time: 2.2544  data: 2.2425  max mem: 337
[19:31:13.351424] Test:  [170/800]  eta: 0:23:24  loss: 0.2935 (0.2858)  time: 2.2448  data: 2.2311  max mem: 337
[19:31:13.376460] Test:  [170/800]  eta: 0:23:24  loss: 0.2963 (0.2863)  time: 2.2465  data: 2.2347  max mem: 337
[19:31:13.455383] Test:  [170/800]  eta: 0:23:25  loss: 0.2819 (0.2858)  time: 2.2493  data: 2.2375  max mem: 337
[19:31:13.497682] Test:  [170/800]  eta: 0:23:25  loss: 0.3045 (0.2877)  time: 2.2531  data: 2.2394  max mem: 337
[19:31:13.504611] Test:  [170/800]  eta: 0:23:25  loss: 0.3062 (0.2866)  time: 2.2509  data: 2.2391  max mem: 337
[19:31:14.065373] Test:  [170/800]  eta: 0:23:27  loss: 0.2889 (0.2854)  time: 2.2512  data: 2.2394  max mem: 337
[19:31:14.099046] Test:  [170/800]  eta: 0:23:27  loss: 0.2825 (0.2876)  time: 2.2549  data: 2.2430  max mem: 337
[19:31:14.301695] Test:  [170/800]  eta: 0:23:28  loss: 0.2937 (0.2860)  time: 2.2543  data: 2.2406  max mem: 337
[19:31:14.329207] Test:  [170/800]  eta: 0:23:28  loss: 0.3011 (0.2888)  time: 2.2480  data: 2.2362  max mem: 337
[19:31:15.297247] Test:  [170/800]  eta: 0:23:31  loss: 0.2865 (0.2875)  time: 2.2174  data: 2.2056  max mem: 337
[19:31:30.162907] Test:  [180/800]  eta: 0:22:43  loss: 0.2796 (0.2903)  time: 2.2036  data: 2.1899  max mem: 337
[19:31:30.175111] Test:  [180/800]  eta: 0:22:43  loss: 0.3015 (0.2896)  time: 2.2035  data: 2.1898  max mem: 337
[19:31:30.185662] Test:  [180/800]  eta: 0:22:43  loss: 0.3036 (0.2898)  time: 2.2048  data: 2.1911  max mem: 337
[19:31:35.044690] Test:  [180/800]  eta: 0:23:00  loss: 0.2962 (0.2897)  time: 2.2249  data: 2.2113  max mem: 337
[19:31:40.385285] Test:  [180/800]  eta: 0:23:18  loss: 0.2796 (0.2903)  time: 2.2579  data: 2.2462  max mem: 337
[19:31:40.399147] Test:  [180/800]  eta: 0:23:18  loss: 0.2806 (0.2890)  time: 2.2459  data: 2.2340  max mem: 337
[19:31:40.417442] Test:  [180/800]  eta: 0:23:18  loss: 0.3152 (0.2898)  time: 2.2454  data: 2.2335  max mem: 337
[19:31:40.422152] Test:  [180/800]  eta: 0:23:18  loss: 0.3098 (0.2895)  time: 2.2520  data: 2.2385  max mem: 337
[19:31:40.426262] Test:  [180/800]  eta: 0:23:18  loss: 0.3207 (0.2892)  time: 2.2576  data: 2.2458  max mem: 337
[19:31:40.452504] Test:  [180/800]  eta: 0:23:18  loss: 0.3033 (0.2892)  time: 2.2548  data: 2.2413  max mem: 337
[19:31:40.485246] Test:  [180/800]  eta: 0:23:18  loss: 0.3014 (0.2909)  time: 2.2487  data: 2.2351  max mem: 337
[19:31:40.490340] Test:  [180/800]  eta: 0:23:19  loss: 0.2763 (0.2893)  time: 2.2509  data: 2.2393  max mem: 337
[19:31:40.535461] Test:  [180/800]  eta: 0:23:19  loss: 0.3140 (0.2910)  time: 2.2209  data: 2.2091  max mem: 337
[19:31:40.537083] Test:  [180/800]  eta: 0:23:19  loss: 0.2960 (0.2882)  time: 2.2212  data: 2.2094  max mem: 337
[19:31:40.866903] Test:  [180/800]  eta: 0:23:20  loss: 0.2732 (0.2889)  time: 2.2294  data: 2.2157  max mem: 337
[19:31:41.135879] Test:  [180/800]  eta: 0:23:21  loss: 0.2778 (0.2914)  time: 2.2373  data: 2.2257  max mem: 337
[19:31:47.433550] Test:  [190/800]  eta: 0:22:06  loss: 0.2758 (0.2894)  time: 2.1954  data: 2.1817  max mem: 337
[19:31:47.436719] Test:  [190/800]  eta: 0:22:06  loss: 0.2997 (0.2890)  time: 2.1929  data: 2.1792  max mem: 337
[19:31:47.471460] Test:  [190/800]  eta: 0:22:06  loss: 0.2874 (0.2899)  time: 2.1947  data: 2.1810  max mem: 337
[19:31:52.079013] Test:  [190/800]  eta: 0:22:21  loss: 0.2908 (0.2893)  time: 2.2002  data: 2.1871  max mem: 337
[19:31:58.457768] Test:  [190/800]  eta: 0:22:41  loss: 0.2970 (0.2904)  time: 2.2480  data: 2.2343  max mem: 337
[19:31:58.460599] Test:  [190/800]  eta: 0:22:41  loss: 0.2919 (0.2888)  time: 2.2554  data: 2.2422  max mem: 337
[19:31:58.460761] Test:  [190/800]  eta: 0:22:41  loss: 0.2862 (0.2893)  time: 2.2571  data: 2.2440  max mem: 337
[19:31:58.462137] Test:  [190/800]  eta: 0:22:41  loss: 0.2778 (0.2913)  time: 2.2066  data: 2.1949  max mem: 337
[19:31:58.463267] Test:  [190/800]  eta: 0:22:41  loss: 0.3042 (0.2891)  time: 2.2560  data: 2.2441  max mem: 337
[19:31:58.463366] Test:  [190/800]  eta: 0:22:41  loss: 0.2954 (0.2901)  time: 2.1583  data: 2.1465  max mem: 337
[19:31:58.464556] Test:  [190/800]  eta: 0:22:41  loss: 0.2910 (0.2890)  time: 2.2544  data: 2.2427  max mem: 337
[19:31:58.475997] Test:  [190/800]  eta: 0:22:41  loss: 0.3019 (0.2889)  time: 2.2087  data: 2.1950  max mem: 337
[19:31:58.584941] Test:  [190/800]  eta: 0:22:42  loss: 0.2935 (0.2908)  time: 2.2242  data: 2.2124  max mem: 337
[19:31:58.625553] Test:  [190/800]  eta: 0:22:42  loss: 0.2849 (0.2887)  time: 2.2585  data: 2.2468  max mem: 337
[19:31:58.681713] Test:  [190/800]  eta: 0:22:42  loss: 0.2960 (0.2881)  time: 2.2308  data: 2.2191  max mem: 337
[19:31:58.785389] Test:  [190/800]  eta: 0:22:42  loss: 0.3085 (0.2895)  time: 2.2640  data: 2.2524  max mem: 337
[19:32:12.693575] Test:  [200/800]  eta: 0:21:55  loss: 0.2987 (0.2901)  time: 2.1253  data: 2.1117  max mem: 337
[19:32:12.727128] Test:  [200/800]  eta: 0:21:55  loss: 0.2986 (0.2910)  time: 2.1282  data: 2.1144  max mem: 337
[19:32:12.752239] Test:  [200/800]  eta: 0:21:55  loss: 0.2997 (0.2899)  time: 2.1288  data: 2.1151  max mem: 337
[19:32:18.056532] Test:  [200/800]  eta: 0:22:11  loss: 0.2979 (0.2903)  time: 2.1505  data: 2.1374  max mem: 337
[19:32:24.595127] Test:  [200/800]  eta: 0:22:30  loss: 0.2971 (0.2912)  time: 2.2054  data: 2.1918  max mem: 337
[19:32:24.609220] Test:  [200/800]  eta: 0:22:30  loss: 0.2994 (0.2895)  time: 2.2078  data: 2.1945  max mem: 337
[19:32:24.624193] Test:  [200/800]  eta: 0:22:30  loss: 0.2862 (0.2904)  time: 2.2101  data: 2.1968  max mem: 337
[19:32:24.866221] Test:  [200/800]  eta: 0:22:31  loss: 0.3019 (0.2896)  time: 2.1999  data: 2.1862  max mem: 337
[19:32:24.962194] Test:  [200/800]  eta: 0:22:31  loss: 0.3002 (0.2899)  time: 2.2235  data: 2.2117  max mem: 337
[19:32:25.013402] Test:  [200/800]  eta: 0:22:32  loss: 0.2986 (0.2909)  time: 2.2314  data: 2.2195  max mem: 337
[19:32:25.015051] Test:  [200/800]  eta: 0:22:32  loss: 0.2993 (0.2902)  time: 2.2294  data: 2.2175  max mem: 337
[19:32:25.041063] Test:  [200/800]  eta: 0:22:32  loss: 0.2968 (0.2922)  time: 2.1952  data: 2.1834  max mem: 337
[19:32:25.165490] Test:  [200/800]  eta: 0:22:32  loss: 0.3071 (0.2897)  time: 2.2383  data: 2.2266  max mem: 337
[19:32:25.168017] Test:  [200/800]  eta: 0:22:32  loss: 0.3029 (0.2918)  time: 2.2316  data: 2.2197  max mem: 337
[19:32:25.200877] Test:  [200/800]  eta: 0:22:32  loss: 0.2896 (0.2902)  time: 2.2391  data: 2.2275  max mem: 337
[19:32:25.202232] Test:  [200/800]  eta: 0:22:32  loss: 0.3077 (0.2893)  time: 2.2332  data: 2.2215  max mem: 337
[19:32:29.306272] Test:  [210/800]  eta: 0:21:18  loss: 0.2729 (0.2887)  time: 2.0936  data: 2.0800  max mem: 337
[19:32:29.315827] Test:  [210/800]  eta: 0:21:18  loss: 0.2986 (0.2898)  time: 2.0922  data: 2.0784  max mem: 337
[19:32:29.347889] Test:  [210/800]  eta: 0:21:18  loss: 0.2757 (0.2885)  time: 2.0955  data: 2.0818  max mem: 337
[19:32:35.013571] Test:  [210/800]  eta: 0:21:34  loss: 0.2888 (0.2889)  time: 2.1467  data: 2.1331  max mem: 337
[19:32:42.009389] Test:  [210/800]  eta: 0:21:54  loss: 0.2811 (0.2880)  time: 2.1774  data: 2.1637  max mem: 337
[19:32:42.025214] Test:  [210/800]  eta: 0:21:54  loss: 0.2968 (0.2900)  time: 2.1783  data: 2.1650  max mem: 337
[19:32:42.142987] Test:  [210/800]  eta: 0:21:54  loss: 0.2751 (0.2893)  time: 2.1841  data: 2.1708  max mem: 337
[19:32:42.290564] Test:  [210/800]  eta: 0:21:54  loss: 0.2916 (0.2885)  time: 2.1907  data: 2.1774  max mem: 337
[19:32:42.402706] Test:  [210/800]  eta: 0:21:55  loss: 0.2757 (0.2907)  time: 2.1970  data: 2.1851  max mem: 337
[19:32:42.407461] Test:  [210/800]  eta: 0:21:55  loss: 0.2885 (0.2893)  time: 2.1972  data: 2.1855  max mem: 337
[19:32:42.412737] Test:  [210/800]  eta: 0:21:55  loss: 0.2691 (0.2886)  time: 2.1974  data: 2.1855  max mem: 337
[19:32:42.413783] Test:  [210/800]  eta: 0:21:55  loss: 0.2748 (0.2894)  time: 2.1975  data: 2.1858  max mem: 337
[19:32:42.483129] Test:  [210/800]  eta: 0:21:55  loss: 0.2882 (0.2880)  time: 2.1900  data: 2.1782  max mem: 337
[19:32:42.490721] Test:  [210/800]  eta: 0:21:55  loss: 0.2663 (0.2901)  time: 2.1952  data: 2.1834  max mem: 337
[19:32:42.495937] Test:  [210/800]  eta: 0:21:55  loss: 0.2687 (0.2884)  time: 2.1935  data: 2.1816  max mem: 337
[19:32:42.499652] Test:  [210/800]  eta: 0:21:55  loss: 0.2840 (0.2888)  time: 2.1857  data: 2.1739  max mem: 337
[19:32:54.161988] Test:  [220/800]  eta: 0:21:05  loss: 0.2864 (0.2901)  time: 2.0734  data: 2.0598  max mem: 337
[19:32:54.183124] Test:  [220/800]  eta: 0:21:05  loss: 0.2929 (0.2902)  time: 2.0715  data: 2.0582  max mem: 337
[19:32:54.187491] Test:  [220/800]  eta: 0:21:05  loss: 0.2987 (0.2915)  time: 2.0730  data: 2.0593  max mem: 337
[19:33:00.566383] Test:  [220/800]  eta: 0:21:22  loss: 0.2965 (0.2905)  time: 2.1254  data: 2.1118  max mem: 337
[19:33:07.828260] Test:  [220/800]  eta: 0:21:41  loss: 0.3042 (0.2915)  time: 2.1616  data: 2.1485  max mem: 337
[19:33:07.863901] Test:  [220/800]  eta: 0:21:41  loss: 0.2811 (0.2894)  time: 2.1627  data: 2.1490  max mem: 337
[19:33:08.008557] Test:  [220/800]  eta: 0:21:41  loss: 0.3123 (0.2908)  time: 2.1692  data: 2.1560  max mem: 337
[19:33:08.022590] Test:  [220/800]  eta: 0:21:41  loss: 0.3007 (0.2902)  time: 2.1578  data: 2.1446  max mem: 337
[19:33:08.227729] Test:  [220/800]  eta: 0:21:42  loss: 0.2848 (0.2909)  time: 2.1607  data: 2.1490  max mem: 337
[19:33:08.274345] Test:  [220/800]  eta: 0:21:42  loss: 0.3097 (0.2922)  time: 2.1616  data: 2.1498  max mem: 337
[19:33:08.274913] Test:  [220/800]  eta: 0:21:42  loss: 0.2943 (0.2899)  time: 2.1656  data: 2.1538  max mem: 337
[19:33:08.294761] Test:  [220/800]  eta: 0:21:42  loss: 0.3154 (0.2911)  time: 2.1639  data: 2.1523  max mem: 337
[19:33:08.438225] Test:  [220/800]  eta: 0:21:42  loss: 0.2944 (0.2902)  time: 2.1618  data: 2.1500  max mem: 337
[19:33:08.467088] Test:  [220/800]  eta: 0:21:42  loss: 0.2749 (0.2915)  time: 2.1649  data: 2.1530  max mem: 337
[19:33:08.469314] Test:  [220/800]  eta: 0:21:42  loss: 0.2960 (0.2898)  time: 2.1633  data: 2.1516  max mem: 337
[19:33:08.469976] Test:  [220/800]  eta: 0:21:42  loss: 0.2876 (0.2896)  time: 2.1652  data: 2.1533  max mem: 337
[19:33:10.188205] Test:  [230/800]  eta: 0:20:29  loss: 0.3222 (0.2907)  time: 2.0441  data: 2.0304  max mem: 337
[19:33:10.193924] Test:  [230/800]  eta: 0:20:29  loss: 0.3141 (0.2921)  time: 2.0439  data: 2.0302  max mem: 337
[19:33:10.200320] Test:  [230/800]  eta: 0:20:29  loss: 0.3144 (0.2910)  time: 2.0426  data: 2.0292  max mem: 337
[19:33:16.953040] Test:  [230/800]  eta: 0:20:45  loss: 0.3163 (0.2908)  time: 2.0969  data: 2.0833  max mem: 337
[19:33:24.449160] Test:  [230/800]  eta: 0:21:04  loss: 0.3205 (0.2923)  time: 2.1212  data: 2.1076  max mem: 337
[19:33:24.532804] Test:  [230/800]  eta: 0:21:04  loss: 0.3243 (0.2903)  time: 2.1261  data: 2.1125  max mem: 337
[19:33:24.705863] Test:  [230/800]  eta: 0:21:04  loss: 0.3128 (0.2909)  time: 2.1207  data: 2.1072  max mem: 337
[19:33:24.746819] Test:  [230/800]  eta: 0:21:05  loss: 0.3133 (0.2915)  time: 2.1301  data: 2.1166  max mem: 337
[19:33:24.769330] Test:  [230/800]  eta: 0:21:05  loss: 0.2984 (0.2904)  time: 2.1178  data: 2.1060  max mem: 337
[19:33:24.790194] Test:  [230/800]  eta: 0:21:05  loss: 0.3208 (0.2929)  time: 2.1193  data: 2.1075  max mem: 337
[19:33:24.810998] Test:  [230/800]  eta: 0:21:05  loss: 0.3262 (0.2918)  time: 2.1198  data: 2.1080  max mem: 337
[19:33:24.812160] Test:  [230/800]  eta: 0:21:05  loss: 0.3154 (0.2920)  time: 2.1202  data: 2.1083  max mem: 337
[19:33:25.027440] Test:  [230/800]  eta: 0:21:05  loss: 0.3161 (0.2902)  time: 2.1272  data: 2.1155  max mem: 337
[19:33:25.099045] Test:  [230/800]  eta: 0:21:05  loss: 0.3086 (0.2921)  time: 2.1304  data: 2.1185  max mem: 337
[19:33:25.102765] Test:  [230/800]  eta: 0:21:05  loss: 0.3004 (0.2904)  time: 2.1303  data: 2.1185  max mem: 337
[19:33:25.102910] Test:  [230/800]  eta: 0:21:05  loss: 0.3126 (0.2911)  time: 2.1301  data: 2.1183  max mem: 337
[19:33:34.167658] Test:  [240/800]  eta: 0:20:13  loss: 0.2889 (0.2909)  time: 1.9992  data: 1.9855  max mem: 337
[19:33:34.196559] Test:  [240/800]  eta: 0:20:13  loss: 0.2891 (0.2922)  time: 2.0004  data: 1.9867  max mem: 337
[19:33:34.203754] Test:  [240/800]  eta: 0:20:13  loss: 0.2858 (0.2909)  time: 2.0020  data: 1.9884  max mem: 337
[19:33:41.493378] Test:  [240/800]  eta: 0:20:30  loss: 0.2822 (0.2910)  time: 2.0463  data: 2.0327  max mem: 337
[19:33:49.387203] Test:  [240/800]  eta: 0:20:48  loss: 0.2957 (0.2930)  time: 2.0556  data: 2.0439  max mem: 337
[19:33:49.398286] Test:  [240/800]  eta: 0:20:48  loss: 0.2983 (0.2927)  time: 2.0785  data: 2.0648  max mem: 337
[19:33:49.431684] Test:  [240/800]  eta: 0:20:48  loss: 0.2854 (0.2921)  time: 2.0568  data: 2.0451  max mem: 337
[19:33:49.435213] Test:  [240/800]  eta: 0:20:48  loss: 0.2860 (0.2903)  time: 2.0785  data: 2.0652  max mem: 337
[19:33:49.442455] Test:  [240/800]  eta: 0:20:48  loss: 0.2865 (0.2901)  time: 2.0583  data: 2.0465  max mem: 337
[19:33:49.444895] Test:  [240/800]  eta: 0:20:48  loss: 0.2960 (0.2917)  time: 2.0608  data: 2.0490  max mem: 337
[19:33:49.477513] Test:  [240/800]  eta: 0:20:48  loss: 0.2818 (0.2912)  time: 2.0727  data: 2.0590  max mem: 337
[19:33:49.485333] Test:  [240/800]  eta: 0:20:48  loss: 0.2934 (0.2917)  time: 2.0738  data: 2.0606  max mem: 337
[19:33:49.849100] Test:  [240/800]  eta: 0:20:49  loss: 0.3032 (0.2912)  time: 2.0705  data: 2.0587  max mem: 337
[19:33:49.853490] Test:  [240/800]  eta: 0:20:49  loss: 0.2849 (0.2904)  time: 2.0692  data: 2.0574  max mem: 337
[19:33:49.874509] Test:  [240/800]  eta: 0:20:49  loss: 0.2794 (0.2903)  time: 2.0702  data: 2.0584  max mem: 337
[19:33:50.050668] Test:  [240/800]  eta: 0:20:50  loss: 0.2884 (0.2922)  time: 2.0791  data: 2.0673  max mem: 337
[19:33:50.396097] Test:  [250/800]  eta: 0:19:39  loss: 0.2891 (0.2924)  time: 2.0101  data: 1.9964  max mem: 337
[19:33:50.403536] Test:  [250/800]  eta: 0:19:39  loss: 0.2838 (0.2914)  time: 2.0107  data: 1.9971  max mem: 337
[19:33:50.447330] Test:  [250/800]  eta: 0:19:39  loss: 0.2799 (0.2913)  time: 2.0123  data: 1.9986  max mem: 337
[19:33:58.055244] Test:  [250/800]  eta: 0:19:56  loss: 0.2573 (0.2917)  time: 2.0551  data: 2.0415  max mem: 337
[19:34:06.000341] Test:  [250/800]  eta: 0:20:13  loss: 0.2939 (0.2935)  time: 2.0604  data: 2.0488  max mem: 337
[19:34:06.035624] Test:  [250/800]  eta: 0:20:13  loss: 0.2760 (0.2920)  time: 2.0612  data: 2.0493  max mem: 337
[19:34:06.039375] Test:  [250/800]  eta: 0:20:13  loss: 0.2804 (0.2925)  time: 2.0613  data: 2.0496  max mem: 337
[19:34:06.041855] Test:  [250/800]  eta: 0:20:13  loss: 0.2790 (0.2906)  time: 2.0636  data: 2.0518  max mem: 337
[19:34:06.086179] Test:  [250/800]  eta: 0:20:13  loss: 0.2962 (0.2931)  time: 2.0818  data: 2.0682  max mem: 337
[19:34:06.099754] Test:  [250/800]  eta: 0:20:14  loss: 0.2841 (0.2906)  time: 2.0783  data: 2.0649  max mem: 337
[19:34:06.140366] Test:  [250/800]  eta: 0:20:14  loss: 0.2801 (0.2924)  time: 2.0696  data: 2.0565  max mem: 337
[19:34:06.163660] Test:  [250/800]  eta: 0:20:14  loss: 0.2795 (0.2918)  time: 2.0728  data: 2.0592  max mem: 337
[19:34:06.512006] Test:  [250/800]  eta: 0:20:14  loss: 0.2704 (0.2908)  time: 2.0704  data: 2.0588  max mem: 337
[19:34:06.537127] Test:  [250/800]  eta: 0:20:15  loss: 0.2825 (0.2910)  time: 2.0754  data: 2.0638  max mem: 337
[19:34:06.543293] Test:  [250/800]  eta: 0:20:15  loss: 0.2608 (0.2919)  time: 2.0720  data: 2.0603  max mem: 337
[19:34:06.789455] Test:  [250/800]  eta: 0:20:15  loss: 0.2843 (0.2928)  time: 2.0845  data: 2.0726  max mem: 337
[19:34:14.918399] Test:  [260/800]  eta: 0:19:24  loss: 0.2981 (0.2922)  time: 2.0360  data: 2.0225  max mem: 337
[19:34:14.953966] Test:  [260/800]  eta: 0:19:24  loss: 0.2761 (0.2913)  time: 2.0393  data: 2.0257  max mem: 337
[19:34:14.956627] Test:  [260/800]  eta: 0:19:24  loss: 0.2725 (0.2913)  time: 2.0376  data: 2.0240  max mem: 337
[19:34:23.020150] Test:  [260/800]  eta: 0:19:41  loss: 0.2958 (0.2921)  time: 2.0763  data: 2.0630  max mem: 337
[19:34:31.111823] Test:  [270/800]  eta: 0:18:52  loss: 0.2877 (0.2923)  time: 2.0354  data: 2.0217  max mem: 337
[19:34:31.122633] Test:  [270/800]  eta: 0:18:52  loss: 0.2794 (0.2929)  time: 2.0363  data: 2.0231  max mem: 337
[19:34:31.128073] Test:  [270/800]  eta: 0:18:52  loss: 0.2767 (0.2919)  time: 2.0340  data: 2.0207  max mem: 337
[19:34:31.149632] Test:  [260/800]  eta: 0:19:58  loss: 0.2927 (0.2925)  time: 2.0859  data: 2.0740  max mem: 337
[19:34:31.153678] Test:  [260/800]  eta: 0:19:58  loss: 0.2760 (0.2922)  time: 2.0854  data: 2.0735  max mem: 337
[19:34:31.175302] Test:  [260/800]  eta: 0:19:58  loss: 0.2859 (0.2909)  time: 2.0866  data: 2.0748  max mem: 337
[19:34:31.325098] Test:  [260/800]  eta: 0:19:58  loss: 0.2792 (0.2934)  time: 2.0968  data: 2.0850  max mem: 337
[19:34:31.404147] Test:  [260/800]  eta: 0:19:58  loss: 0.2839 (0.2919)  time: 2.0963  data: 2.0826  max mem: 337
[19:34:31.413227] Test:  [260/800]  eta: 0:19:58  loss: 0.2767 (0.2906)  time: 2.0989  data: 2.0852  max mem: 337
[19:34:31.427766] Test:  [260/800]  eta: 0:19:58  loss: 0.2701 (0.2931)  time: 2.1014  data: 2.0878  max mem: 337
[19:34:31.472144] Test:  [260/800]  eta: 0:19:58  loss: 0.2782 (0.2922)  time: 2.0993  data: 2.0856  max mem: 337
[19:34:31.785289] Test:  [260/800]  eta: 0:19:59  loss: 0.3008 (0.2923)  time: 2.0968  data: 2.0851  max mem: 337
[19:34:31.856116] Test:  [260/800]  eta: 0:19:59  loss: 0.2825 (0.2911)  time: 2.1001  data: 2.0884  max mem: 337
[19:34:31.861431] Test:  [260/800]  eta: 0:19:59  loss: 0.2773 (0.2908)  time: 2.0993  data: 2.0876  max mem: 337
[19:34:32.225733] Test:  [260/800]  eta: 0:20:00  loss: 0.2694 (0.2930)  time: 2.1087  data: 2.0968  max mem: 337
[19:34:39.263548] Test:  [270/800]  eta: 0:19:08  loss: 0.2958 (0.2931)  time: 2.0604  data: 2.0470  max mem: 337
[19:34:47.870813] Test:  [270/800]  eta: 0:19:25  loss: 0.2927 (0.2934)  time: 2.0915  data: 2.0797  max mem: 337
[19:34:47.875415] Test:  [270/800]  eta: 0:19:25  loss: 0.2935 (0.2918)  time: 2.0916  data: 2.0800  max mem: 337
[19:34:47.899107] Test:  [270/800]  eta: 0:19:25  loss: 0.2709 (0.2928)  time: 2.0931  data: 2.0813  max mem: 337
[19:34:48.052246] Test:  [270/800]  eta: 0:19:25  loss: 0.2721 (0.2938)  time: 2.1025  data: 2.0907  max mem: 337
[19:34:48.107993] Test:  [270/800]  eta: 0:19:25  loss: 0.2857 (0.2929)  time: 2.0983  data: 2.0846  max mem: 337
[19:34:48.112283] Test:  [270/800]  eta: 0:19:25  loss: 0.2856 (0.2915)  time: 2.1006  data: 2.0874  max mem: 337
[19:34:48.120321] Test:  [270/800]  eta: 0:19:25  loss: 0.2881 (0.2939)  time: 2.1017  data: 2.0880  max mem: 337
[19:34:48.135045] Test:  [270/800]  eta: 0:19:25  loss: 0.2839 (0.2925)  time: 2.0985  data: 2.0849  max mem: 337
[19:34:48.517997] Test:  [270/800]  eta: 0:19:26  loss: 0.2910 (0.2919)  time: 2.0990  data: 2.0872  max mem: 337
[19:34:48.564753] Test:  [270/800]  eta: 0:19:26  loss: 0.2899 (0.2915)  time: 2.1026  data: 2.0907  max mem: 337
[19:34:48.569582] Test:  [270/800]  eta: 0:19:26  loss: 0.2968 (0.2929)  time: 2.1013  data: 2.0895  max mem: 337
[19:34:48.974229] Test:  [270/800]  eta: 0:19:27  loss: 0.2885 (0.2938)  time: 2.1092  data: 2.0975  max mem: 337
[19:34:55.551693] Test:  [280/800]  eta: 0:18:36  loss: 0.2950 (0.2937)  time: 2.0316  data: 2.0183  max mem: 337
[19:34:55.565653] Test:  [280/800]  eta: 0:18:36  loss: 0.2912 (0.2930)  time: 2.0305  data: 2.0172  max mem: 337
[19:34:55.600292] Test:  [280/800]  eta: 0:18:36  loss: 0.2929 (0.2933)  time: 2.0321  data: 2.0184  max mem: 337
[19:35:03.793971] Test:  [280/800]  eta: 0:18:52  loss: 0.3081 (0.2941)  time: 2.0386  data: 2.0250  max mem: 337
[19:35:11.890634] Test:  [290/800]  eta: 0:18:06  loss: 0.3441 (0.2954)  time: 2.0389  data: 2.0252  max mem: 337
[19:35:11.920140] Test:  [290/800]  eta: 0:18:06  loss: 0.3535 (0.2961)  time: 2.0398  data: 2.0261  max mem: 337
[19:35:11.933078] Test:  [290/800]  eta: 0:18:06  loss: 0.3557 (0.2954)  time: 2.0402  data: 2.0265  max mem: 337
[19:35:13.049536] Test:  [280/800]  eta: 0:19:09  loss: 0.2923 (0.2936)  time: 2.0947  data: 2.0829  max mem: 337
[19:35:13.052530] Test:  [280/800]  eta: 0:19:09  loss: 0.2935 (0.2925)  time: 2.0938  data: 2.0821  max mem: 337
[19:35:13.055726] Test:  [280/800]  eta: 0:19:09  loss: 0.2939 (0.2942)  time: 2.0953  data: 2.0834  max mem: 337
[19:35:13.129534] Test:  [280/800]  eta: 0:19:09  loss: 0.2945 (0.2944)  time: 2.0902  data: 2.0783  max mem: 337
[19:35:13.226051] Test:  [280/800]  eta: 0:19:09  loss: 0.2921 (0.2949)  time: 2.0899  data: 2.0762  max mem: 337
[19:35:13.229902] Test:  [280/800]  eta: 0:19:09  loss: 0.2847 (0.2934)  time: 2.0912  data: 2.0776  max mem: 337
[19:35:13.237915] Test:  [280/800]  eta: 0:19:09  loss: 0.2988 (0.2925)  time: 2.0912  data: 2.0780  max mem: 337
[19:35:13.293559] Test:  [280/800]  eta: 0:19:09  loss: 0.2997 (0.2934)  time: 2.0910  data: 2.0774  max mem: 337
[19:35:13.738855] Test:  [280/800]  eta: 0:19:10  loss: 0.2968 (0.2936)  time: 2.0976  data: 2.0858  max mem: 337
[19:35:13.745148] Test:  [280/800]  eta: 0:19:10  loss: 0.3027 (0.2925)  time: 2.0941  data: 2.0823  max mem: 337
[19:35:13.760041] Test:  [280/800]  eta: 0:19:10  loss: 0.2973 (0.2929)  time: 2.0952  data: 2.0833  max mem: 337
[19:35:14.285025] Test:  [280/800]  eta: 0:19:11  loss: 0.3103 (0.2949)  time: 2.1029  data: 2.0912  max mem: 337
[19:35:20.198275] Test:  [290/800]  eta: 0:18:20  loss: 0.3534 (0.2964)  time: 2.0467  data: 2.0331  max mem: 337
[19:35:29.881985] Test:  [290/800]  eta: 0:18:37  loss: 0.3540 (0.2965)  time: 2.1005  data: 2.0887  max mem: 337
[19:35:29.918132] Test:  [290/800]  eta: 0:18:37  loss: 0.3512 (0.2950)  time: 2.1021  data: 2.0903  max mem: 337
[19:35:29.936970] Test:  [290/800]  eta: 0:18:37  loss: 0.3494 (0.2958)  time: 2.1018  data: 2.0900  max mem: 337
[19:35:29.943950] Test:  [290/800]  eta: 0:18:37  loss: 0.3498 (0.2968)  time: 2.0945  data: 2.0827  max mem: 337
[19:35:30.177789] Test:  [290/800]  eta: 0:18:38  loss: 0.3610 (0.2950)  time: 2.1032  data: 2.0896  max mem: 337
[19:35:30.184534] Test:  [290/800]  eta: 0:18:38  loss: 0.3344 (0.2956)  time: 2.1038  data: 2.0901  max mem: 337
[19:35:30.197425] Test:  [290/800]  eta: 0:18:38  loss: 0.3470 (0.2972)  time: 2.1038  data: 2.0902  max mem: 337
[19:35:30.207638] Test:  [290/800]  eta: 0:18:38  loss: 0.3539 (0.2956)  time: 2.1036  data: 2.0902  max mem: 337
[19:35:30.657057] Test:  [290/800]  eta: 0:18:39  loss: 0.3508 (0.2949)  time: 2.1046  data: 2.0927  max mem: 337
[19:35:30.679835] Test:  [290/800]  eta: 0:18:39  loss: 0.3479 (0.2952)  time: 2.1080  data: 2.0962  max mem: 337
[19:35:30.698856] Test:  [290/800]  eta: 0:18:39  loss: 0.3443 (0.2960)  time: 2.1064  data: 2.0946  max mem: 337
[19:35:31.253326] Test:  [290/800]  eta: 0:18:40  loss: 0.3375 (0.2971)  time: 2.1139  data: 2.1021  max mem: 337
[19:35:36.109142] Test:  [300/800]  eta: 0:17:49  loss: 0.3400 (0.2950)  time: 2.0254  data: 2.0117  max mem: 337
[19:35:36.126079] Test:  [300/800]  eta: 0:17:49  loss: 0.3415 (0.2952)  time: 2.0280  data: 2.0143  max mem: 337
[19:35:36.126123] Test:  [300/800]  eta: 0:17:49  loss: 0.3535 (0.2958)  time: 2.0287  data: 2.0150  max mem: 337
[19:35:44.392395] Test:  [300/800]  eta: 0:18:03  loss: 0.3391 (0.2961)  time: 2.0299  data: 2.0164  max mem: 337
[19:35:52.489834] Test:  [310/800]  eta: 0:17:20  loss: 0.2523 (0.2947)  time: 2.0278  data: 2.0146  max mem: 337
[19:35:52.497188] Test:  [310/800]  eta: 0:17:20  loss: 0.2512 (0.2950)  time: 2.0288  data: 2.0156  max mem: 337
[19:35:52.511594] Test:  [310/800]  eta: 0:17:20  loss: 0.2571 (0.2946)  time: 2.0310  data: 2.0178  max mem: 337
[19:35:54.618713] Test:  [300/800]  eta: 0:18:20  loss: 0.3387 (0.2956)  time: 2.0784  data: 2.0666  max mem: 337
[19:35:54.620793] Test:  [300/800]  eta: 0:18:20  loss: 0.3498 (0.2965)  time: 2.0745  data: 2.0627  max mem: 337
[19:35:54.630216] Test:  [300/800]  eta: 0:18:20  loss: 0.3457 (0.2945)  time: 2.0788  data: 2.0670  max mem: 337
[19:35:54.631337] Test:  [300/800]  eta: 0:18:20  loss: 0.3434 (0.2961)  time: 2.0787  data: 2.0669  max mem: 337
[19:35:55.002806] Test:  [300/800]  eta: 0:18:21  loss: 0.3388 (0.2949)  time: 2.0882  data: 2.0745  max mem: 337
[19:35:55.052712] Test:  [300/800]  eta: 0:18:21  loss: 0.3364 (0.2950)  time: 2.0911  data: 2.0776  max mem: 337
[19:35:55.064150] Test:  [300/800]  eta: 0:18:21  loss: 0.3470 (0.2969)  time: 2.0919  data: 2.0782  max mem: 337
[19:35:55.065737] Test:  [300/800]  eta: 0:18:21  loss: 0.3344 (0.2951)  time: 2.0886  data: 2.0749  max mem: 337
[19:35:55.537082] Test:  [300/800]  eta: 0:18:22  loss: 0.3451 (0.2945)  time: 2.0896  data: 2.0777  max mem: 337
[19:35:55.540360] Test:  [300/800]  eta: 0:18:22  loss: 0.3420 (0.2949)  time: 2.0890  data: 2.0771  max mem: 337
[19:35:55.540620] Test:  [300/800]  eta: 0:18:22  loss: 0.3326 (0.2956)  time: 2.0900  data: 2.0782  max mem: 337
[19:35:56.174701] Test:  [300/800]  eta: 0:18:23  loss: 0.3375 (0.2968)  time: 2.0944  data: 2.0826  max mem: 337
[19:36:00.755514] Test:  [310/800]  eta: 0:17:33  loss: 0.2590 (0.2957)  time: 2.0278  data: 2.0146  max mem: 337
[19:36:11.423371] Test:  [310/800]  eta: 0:17:50  loss: 0.2631 (0.2959)  time: 2.0739  data: 2.0623  max mem: 337
[19:36:11.511746] Test:  [310/800]  eta: 0:17:50  loss: 0.2414 (0.2940)  time: 2.0796  data: 2.0678  max mem: 337
[19:36:11.511778] Test:  [310/800]  eta: 0:17:50  loss: 0.2597 (0.2954)  time: 2.0814  data: 2.0698  max mem: 337
[19:36:11.822181] Test:  [310/800]  eta: 0:17:50  loss: 0.2505 (0.2945)  time: 2.0818  data: 2.0686  max mem: 337
[19:36:11.836372] Test:  [310/800]  eta: 0:17:50  loss: 0.2427 (0.2964)  time: 2.0819  data: 2.0686  max mem: 337
[19:36:11.863346] Test:  [310/800]  eta: 0:17:51  loss: 0.2541 (0.2944)  time: 2.0842  data: 2.0706  max mem: 337
[19:36:11.880038] Test:  [310/800]  eta: 0:17:51  loss: 0.2369 (0.2943)  time: 2.0836  data: 2.0702  max mem: 337
[19:36:12.370455] Test:  [310/800]  eta: 0:17:51  loss: 0.2579 (0.2943)  time: 2.0845  data: 2.0727  max mem: 337
[19:36:12.398695] Test:  [310/800]  eta: 0:17:51  loss: 0.2538 (0.2948)  time: 2.0849  data: 2.0731  max mem: 337
[19:36:12.412279] Test:  [310/800]  eta: 0:17:51  loss: 0.2518 (0.2942)  time: 2.0877  data: 2.0759  max mem: 337
[19:36:12.595013] Test:  [310/800]  eta: 0:17:52  loss: 0.2523 (0.2950)  time: 2.1329  data: 2.1212  max mem: 337
[19:36:13.079597] Test:  [310/800]  eta: 0:17:52  loss: 0.2535 (0.2962)  time: 2.0913  data: 2.0794  max mem: 337
[19:36:16.422230] Test:  [320/800]  eta: 0:17:03  loss: 0.3005 (0.2948)  time: 2.0148  data: 2.0015  max mem: 337
[19:36:16.464954] Test:  [320/800]  eta: 0:17:03  loss: 0.2969 (0.2952)  time: 2.0169  data: 2.0037  max mem: 337
[19:36:16.466913] Test:  [320/800]  eta: 0:17:03  loss: 0.2899 (0.2949)  time: 2.0178  data: 2.0046  max mem: 337
[19:36:24.837206] Test:  [320/800]  eta: 0:17:15  loss: 0.3046 (0.2960)  time: 2.0222  data: 2.0089  max mem: 337
[19:36:32.502309] Test:  [330/800]  eta: 0:16:34  loss: 0.3111 (0.2945)  time: 2.0006  data: 1.9869  max mem: 337
[19:36:32.532222] Test:  [330/800]  eta: 0:16:34  loss: 0.3027 (0.2949)  time: 2.0017  data: 1.9880  max mem: 337
[19:36:32.535102] Test:  [330/800]  eta: 0:16:34  loss: 0.3005 (0.2946)  time: 2.0011  data: 1.9875  max mem: 337
[19:36:35.961253] Test:  [320/800]  eta: 0:17:32  loss: 0.2830 (0.2944)  time: 2.0665  data: 2.0547  max mem: 337
[19:36:35.983817] Test:  [320/800]  eta: 0:17:32  loss: 0.2978 (0.2957)  time: 2.0676  data: 2.0559  max mem: 337
[19:36:36.018735] Test:  [320/800]  eta: 0:17:32  loss: 0.2711 (0.2953)  time: 2.0700  data: 2.0583  max mem: 337
[19:36:36.081252] Test:  [320/800]  eta: 0:17:32  loss: 0.2990 (0.2963)  time: 2.0730  data: 2.0613  max mem: 337
[19:36:36.452555] Test:  [320/800]  eta: 0:17:33  loss: 0.3090 (0.2969)  time: 2.0694  data: 2.0562  max mem: 337
[19:36:36.460821] Test:  [320/800]  eta: 0:17:33  loss: 0.3041 (0.2948)  time: 2.0704  data: 2.0570  max mem: 337
[19:36:36.475361] Test:  [320/800]  eta: 0:17:33  loss: 0.2779 (0.2947)  time: 2.0704  data: 2.0573  max mem: 337
[19:36:36.495064] Test:  [320/800]  eta: 0:17:33  loss: 0.2999 (0.2944)  time: 2.0746  data: 2.0609  max mem: 337
[19:36:36.993460] Test:  [320/800]  eta: 0:17:34  loss: 0.2999 (0.2950)  time: 2.0726  data: 2.0610  max mem: 337
[19:36:37.002243] Test:  [320/800]  eta: 0:17:34  loss: 0.3020 (0.2948)  time: 2.0732  data: 2.0616  max mem: 337
[19:36:37.021289] Test:  [320/800]  eta: 0:17:34  loss: 0.2935 (0.2946)  time: 2.0740  data: 2.0623  max mem: 337
[19:36:37.830919] Test:  [320/800]  eta: 0:17:35  loss: 0.2934 (0.2963)  time: 2.0828  data: 2.0709  max mem: 337
[19:36:40.920079] Test:  [330/800]  eta: 0:16:46  loss: 0.2983 (0.2955)  time: 2.0082  data: 1.9946  max mem: 337
[19:36:52.522321] Test:  [330/800]  eta: 0:17:03  loss: 0.3084 (0.2943)  time: 2.0505  data: 2.0387  max mem: 337
[19:36:52.600420] Test:  [330/800]  eta: 0:17:03  loss: 0.2978 (0.2954)  time: 2.0544  data: 2.0425  max mem: 337
[19:36:52.697215] Test:  [330/800]  eta: 0:17:03  loss: 0.2990 (0.2959)  time: 2.0636  data: 2.0518  max mem: 337
[19:36:53.305409] Test:  [330/800]  eta: 0:17:04  loss: 0.2999 (0.2941)  time: 2.0721  data: 2.0584  max mem: 337
[19:36:53.323322] Test:  [330/800]  eta: 0:17:04  loss: 0.2984 (0.2946)  time: 2.0750  data: 2.0615  max mem: 337
[19:36:53.334773] Test:  [330/800]  eta: 0:17:04  loss: 0.3041 (0.2945)  time: 2.0727  data: 2.0591  max mem: 337
[19:36:53.509906] Test:  [330/800]  eta: 0:17:04  loss: 0.2963 (0.2964)  time: 2.0836  data: 2.0701  max mem: 337
[19:36:53.575164] Test:  [330/800]  eta: 0:17:04  loss: 0.3082 (0.2946)  time: 2.0581  data: 2.0464  max mem: 337
[19:36:53.586860] Test:  [330/800]  eta: 0:17:04  loss: 0.2890 (0.2945)  time: 2.0594  data: 2.0477  max mem: 337
[19:36:53.592394] Test:  [330/800]  eta: 0:17:04  loss: 0.2935 (0.2941)  time: 2.0611  data: 2.0493  max mem: 337
[19:36:54.557933] Test:  [330/800]  eta: 0:17:05  loss: 0.2994 (0.2961)  time: 2.0739  data: 2.0620  max mem: 337
[19:36:54.737707] Test:  [330/800]  eta: 0:17:06  loss: 0.3100 (0.2949)  time: 2.1071  data: 2.0952  max mem: 337
[19:36:57.389578] Test:  [340/800]  eta: 0:16:18  loss: 0.2978 (0.2955)  time: 2.0462  data: 2.0325  max mem: 337
[19:36:57.389966] Test:  [340/800]  eta: 0:16:18  loss: 0.2945 (0.2952)  time: 2.0483  data: 2.0347  max mem: 337
[19:36:57.508223] Test:  [340/800]  eta: 0:16:18  loss: 0.2985 (0.2954)  time: 2.0520  data: 2.0384  max mem: 337
[19:37:05.900527] Test:  [340/800]  eta: 0:16:29  loss: 0.2947 (0.2961)  time: 2.0531  data: 2.0396  max mem: 337
[19:37:14.085673] Test:  [350/800]  eta: 0:15:51  loss: 0.2776 (0.2930)  time: 2.0791  data: 2.0656  max mem: 337
[19:37:14.090737] Test:  [350/800]  eta: 0:15:51  loss: 0.2773 (0.2934)  time: 2.0779  data: 2.0645  max mem: 337
[19:37:14.608507] Test:  [350/800]  eta: 0:15:51  loss: 0.2760 (0.2932)  time: 2.1036  data: 2.0904  max mem: 337
[19:37:18.050560] Test:  [340/800]  eta: 0:16:46  loss: 0.2884 (0.2955)  time: 2.1015  data: 2.0898  max mem: 337
[19:37:18.082220] Test:  [340/800]  eta: 0:16:46  loss: 0.2923 (0.2960)  time: 2.1049  data: 2.0930  max mem: 337
[19:37:18.271843] Test:  [340/800]  eta: 0:16:46  loss: 0.2921 (0.2965)  time: 2.1095  data: 2.0976  max mem: 337
[19:37:18.273305] Test:  [340/800]  eta: 0:16:46  loss: 0.3037 (0.2949)  time: 2.1156  data: 2.1039  max mem: 337
[19:37:18.538243] Test:  [340/800]  eta: 0:16:46  loss: 0.2957 (0.2971)  time: 2.1042  data: 2.0908  max mem: 337
[19:37:18.582665] Test:  [340/800]  eta: 0:16:47  loss: 0.2996 (0.2952)  time: 2.1060  data: 2.0924  max mem: 337
[19:37:18.585850] Test:  [340/800]  eta: 0:16:47  loss: 0.2942 (0.2948)  time: 2.1045  data: 2.0913  max mem: 337
[19:37:18.586053] Test:  [340/800]  eta: 0:16:47  loss: 0.2972 (0.2952)  time: 2.1055  data: 2.0921  max mem: 337
[19:37:19.191648] Test:  [340/800]  eta: 0:16:47  loss: 0.2986 (0.2947)  time: 2.1085  data: 2.0966  max mem: 337
[19:37:19.202176] Test:  [340/800]  eta: 0:16:47  loss: 0.2890 (0.2950)  time: 2.1104  data: 2.0986  max mem: 337
[19:37:19.240049] Test:  [340/800]  eta: 0:16:47  loss: 0.2980 (0.2953)  time: 2.1118  data: 2.1000  max mem: 337
[19:37:20.223506] Test:  [340/800]  eta: 0:16:49  loss: 0.3061 (0.2969)  time: 2.1196  data: 2.1077  max mem: 337
[19:37:22.292238] Test:  [350/800]  eta: 0:16:01  loss: 0.2718 (0.2939)  time: 2.0686  data: 2.0555  max mem: 337
[19:37:36.555534] Test:  [350/800]  eta: 0:16:20  loss: 0.2782 (0.2943)  time: 2.1929  data: 2.1810  max mem: 337
[19:37:36.580464] Test:  [350/800]  eta: 0:16:20  loss: 0.2716 (0.2928)  time: 2.2029  data: 2.1912  max mem: 337
[19:37:36.626997] Test:  [350/800]  eta: 0:16:20  loss: 0.2753 (0.2938)  time: 2.2013  data: 2.1894  max mem: 337
[19:37:37.123822] Test:  [350/800]  eta: 0:16:20  loss: 0.2723 (0.2927)  time: 2.1768  data: 2.1650  max mem: 337
[19:37:37.127129] Test:  [350/800]  eta: 0:16:20  loss: 0.2779 (0.2931)  time: 2.1776  data: 2.1659  max mem: 337
[19:37:37.127354] Test:  [350/800]  eta: 0:16:20  loss: 0.2847 (0.2926)  time: 2.1767  data: 2.1649  max mem: 337
[19:37:37.216079] Test:  [350/800]  eta: 0:16:20  loss: 0.2858 (0.2930)  time: 2.1946  data: 2.1812  max mem: 337
[19:37:37.235593] Test:  [350/800]  eta: 0:16:20  loss: 0.2845 (0.2926)  time: 2.1965  data: 2.1833  max mem: 337
[19:37:37.236393] Test:  [350/800]  eta: 0:16:20  loss: 0.2877 (0.2932)  time: 2.1950  data: 2.1814  max mem: 337
[19:37:37.570128] Test:  [350/800]  eta: 0:16:21  loss: 0.2670 (0.2949)  time: 2.2030  data: 2.1895  max mem: 337
[19:37:37.828367] Test:  [350/800]  eta: 0:16:21  loss: 0.2766 (0.2948)  time: 2.1635  data: 2.1516  max mem: 337
[19:37:38.070881] Test:  [360/800]  eta: 0:15:33  loss: 0.2838 (0.2941)  time: 2.0340  data: 2.0206  max mem: 337
[19:37:38.110284] Test:  [360/800]  eta: 0:15:33  loss: 0.2891 (0.2947)  time: 2.0360  data: 2.0227  max mem: 337
[19:37:38.353165] Test:  [360/800]  eta: 0:15:33  loss: 0.2760 (0.2942)  time: 2.0422  data: 2.0290  max mem: 337
[19:37:38.791245] Test:  [350/800]  eta: 0:16:22  loss: 0.2754 (0.2933)  time: 2.2026  data: 2.1909  max mem: 337
[19:37:47.144449] Test:  [360/800]  eta: 0:15:44  loss: 0.2883 (0.2951)  time: 2.0622  data: 2.0490  max mem: 337
[19:37:55.686551] Test:  [370/800]  eta: 0:15:08  loss: 0.3256 (0.2960)  time: 2.0797  data: 2.0662  max mem: 337
[19:37:55.774083] Test:  [370/800]  eta: 0:15:08  loss: 0.3285 (0.2952)  time: 2.0844  data: 2.0708  max mem: 337
[19:37:56.331117] Test:  [370/800]  eta: 0:15:09  loss: 0.3180 (0.2956)  time: 2.0861  data: 2.0725  max mem: 337
[19:38:00.144775] Test:  [360/800]  eta: 0:16:00  loss: 0.2824 (0.2955)  time: 2.0936  data: 2.0818  max mem: 337
[19:38:00.148552] Test:  [360/800]  eta: 0:16:00  loss: 0.2916 (0.2947)  time: 2.1033  data: 2.0914  max mem: 337
[19:38:00.154739] Test:  [360/800]  eta: 0:16:00  loss: 0.2806 (0.2939)  time: 2.0940  data: 2.0822  max mem: 337
[19:38:00.274777] Test:  [360/800]  eta: 0:16:00  loss: 0.2778 (0.2943)  time: 2.1112  data: 2.0993  max mem: 337
[19:38:00.692679] Test:  [360/800]  eta: 0:16:01  loss: 0.2895 (0.2944)  time: 2.1055  data: 2.0918  max mem: 337
[19:38:00.694589] Test:  [360/800]  eta: 0:16:01  loss: 0.2903 (0.2940)  time: 2.1054  data: 2.0917  max mem: 337
[19:38:00.698280] Test:  [360/800]  eta: 0:16:01  loss: 0.2780 (0.2958)  time: 2.1080  data: 2.0943  max mem: 337
[19:38:00.700200] Test:  [360/800]  eta: 0:16:01  loss: 0.2924 (0.2937)  time: 2.1057  data: 2.0920  max mem: 337
[19:38:01.087510] Test:  [360/800]  eta: 0:16:01  loss: 0.2891 (0.2943)  time: 2.0923  data: 2.0806  max mem: 337
[19:38:01.090239] Test:  [360/800]  eta: 0:16:01  loss: 0.2854 (0.2937)  time: 2.0949  data: 2.0831  max mem: 337
[19:38:01.096869] Test:  [360/800]  eta: 0:16:01  loss: 0.2866 (0.2937)  time: 2.0947  data: 2.0829  max mem: 337
[19:38:02.142498] Test:  [360/800]  eta: 0:16:02  loss: 0.2962 (0.2960)  time: 2.0959  data: 2.0840  max mem: 337
[19:38:04.252478] Test:  [370/800]  eta: 0:15:18  loss: 0.3239 (0.2964)  time: 2.0980  data: 2.0844  max mem: 337
[19:38:19.291263] Test:  [380/800]  eta: 0:14:49  loss: 0.3342 (0.2969)  time: 2.0590  data: 2.0454  max mem: 337
[19:38:19.353811] Test:  [380/800]  eta: 0:14:49  loss: 0.3391 (0.2961)  time: 2.0641  data: 2.0504  max mem: 337
[19:38:19.356909] Test:  [370/800]  eta: 0:15:35  loss: 0.3193 (0.2952)  time: 2.1388  data: 2.1271  max mem: 337
[19:38:19.378549] Test:  [370/800]  eta: 0:15:35  loss: 0.3229 (0.2959)  time: 2.1375  data: 2.1258  max mem: 337
[19:38:19.432313] Test:  [370/800]  eta: 0:15:35  loss: 0.3194 (0.2968)  time: 2.1438  data: 2.1320  max mem: 337
[19:38:19.585541] Test:  [380/800]  eta: 0:14:50  loss: 0.3406 (0.2964)  time: 2.0616  data: 2.0480  max mem: 337
[19:38:19.843294] Test:  [370/800]  eta: 0:15:36  loss: 0.3258 (0.2952)  time: 2.1359  data: 2.1242  max mem: 337
[19:38:19.845987] Test:  [370/800]  eta: 0:15:36  loss: 0.3242 (0.2957)  time: 2.1359  data: 2.1241  max mem: 337
[19:38:19.849359] Test:  [370/800]  eta: 0:15:36  loss: 0.3240 (0.2951)  time: 2.1361  data: 2.1243  max mem: 337
[19:38:20.187446] Test:  [370/800]  eta: 0:15:36  loss: 0.3281 (0.2950)  time: 2.1475  data: 2.1341  max mem: 337
[19:38:20.226245] Test:  [370/800]  eta: 0:15:36  loss: 0.3308 (0.2957)  time: 2.1494  data: 2.1359  max mem: 337
[19:38:20.279143] Test:  [370/800]  eta: 0:15:36  loss: 0.3164 (0.2970)  time: 2.1354  data: 2.1220  max mem: 337
[19:38:20.279502] Test:  [370/800]  eta: 0:15:36  loss: 0.3305 (0.2954)  time: 2.1531  data: 2.1397  max mem: 337
[19:38:20.463870] Test:  [370/800]  eta: 0:15:36  loss: 0.3255 (0.2956)  time: 2.0836  data: 2.0718  max mem: 337
[19:38:20.590549] Test:  [370/800]  eta: 0:15:37  loss: 0.3229 (0.2975)  time: 2.1381  data: 2.1263  max mem: 337
[19:38:29.278140] Test:  [380/800]  eta: 0:15:00  loss: 0.3419 (0.2974)  time: 2.1066  data: 2.0931  max mem: 337
[19:38:36.191038] Test:  [390/800]  eta: 0:14:24  loss: 0.3237 (0.2961)  time: 2.0208  data: 2.0071  max mem: 337
[19:38:36.232932] Test:  [390/800]  eta: 0:14:24  loss: 0.3370 (0.2968)  time: 2.0273  data: 2.0136  max mem: 337
[19:38:36.673991] Test:  [390/800]  eta: 0:14:24  loss: 0.3406 (0.2963)  time: 2.0171  data: 2.0035  max mem: 337
[19:38:42.237455] Test:  [380/800]  eta: 0:15:15  loss: 0.3425 (0.2968)  time: 2.1044  data: 2.0926  max mem: 337
[19:38:42.242172] Test:  [380/800]  eta: 0:15:15  loss: 0.3368 (0.2961)  time: 2.1043  data: 2.0926  max mem: 337
[19:38:42.313107] Test:  [380/800]  eta: 0:15:15  loss: 0.3339 (0.2975)  time: 2.1084  data: 2.0966  max mem: 337
[19:38:42.475651] Test:  [380/800]  eta: 0:15:15  loss: 0.3234 (0.2963)  time: 2.1100  data: 2.0982  max mem: 337
[19:38:42.852738] Test:  [380/800]  eta: 0:15:15  loss: 0.3392 (0.2959)  time: 2.1076  data: 2.0941  max mem: 337
[19:38:42.906743] Test:  [380/800]  eta: 0:15:15  loss: 0.3445 (0.2978)  time: 2.1104  data: 2.0971  max mem: 337
[19:38:42.914046] Test:  [380/800]  eta: 0:15:15  loss: 0.3352 (0.2964)  time: 2.1110  data: 2.0976  max mem: 337
[19:38:42.916807] Test:  [380/800]  eta: 0:15:15  loss: 0.3575 (0.2964)  time: 2.1111  data: 2.0977  max mem: 337
[19:38:43.515741] Test:  [380/800]  eta: 0:15:16  loss: 0.3633 (0.2962)  time: 2.1212  data: 2.1095  max mem: 337
[19:38:43.518827] Test:  [380/800]  eta: 0:15:16  loss: 0.3466 (0.2968)  time: 2.1215  data: 2.1098  max mem: 337
[19:38:43.532684] Test:  [380/800]  eta: 0:15:16  loss: 0.3430 (0.2960)  time: 2.1217  data: 2.1100  max mem: 337
[19:38:44.449034] Test:  [380/800]  eta: 0:15:17  loss: 0.3352 (0.2983)  time: 2.1153  data: 2.1035  max mem: 337
[19:38:46.488941] Test:  [390/800]  eta: 0:14:34  loss: 0.3366 (0.2972)  time: 2.1118  data: 2.0982  max mem: 337
[19:39:00.478338] Test:  [390/800]  eta: 0:14:49  loss: 0.3271 (0.2975)  time: 2.0523  data: 2.0405  max mem: 337
[19:39:00.488176] Test:  [390/800]  eta: 0:14:49  loss: 0.3368 (0.2961)  time: 2.0565  data: 2.0447  max mem: 337
[19:39:00.491426] Test:  [390/800]  eta: 0:14:49  loss: 0.3355 (0.2968)  time: 2.0556  data: 2.0437  max mem: 337
[19:39:01.025123] Test:  [400/800]  eta: 0:14:06  loss: 0.3078 (0.2971)  time: 2.0866  data: 2.0735  max mem: 337
[19:39:01.025973] Test:  [400/800]  eta: 0:14:06  loss: 0.2830 (0.2964)  time: 2.0836  data: 2.0704  max mem: 337
[19:39:01.112255] Test:  [390/800]  eta: 0:14:50  loss: 0.3472 (0.2968)  time: 2.0633  data: 2.0515  max mem: 337
[19:39:01.115672] Test:  [390/800]  eta: 0:14:50  loss: 0.3320 (0.2958)  time: 2.0636  data: 2.0518  max mem: 337
[19:39:01.165945] Test:  [390/800]  eta: 0:14:50  loss: 0.3535 (0.2959)  time: 2.0658  data: 2.0540  max mem: 337
[19:39:01.186611] Test:  [390/800]  eta: 0:14:50  loss: 0.3418 (0.2963)  time: 2.0453  data: 2.0317  max mem: 337
[19:39:01.257621] Test:  [390/800]  eta: 0:14:50  loss: 0.3445 (0.2957)  time: 2.0535  data: 2.0398  max mem: 337
[19:39:01.258329] Test:  [390/800]  eta: 0:14:50  loss: 0.3391 (0.2976)  time: 2.0489  data: 2.0355  max mem: 337
[19:39:01.259429] Test:  [390/800]  eta: 0:14:50  loss: 0.3207 (0.2964)  time: 2.0516  data: 2.0382  max mem: 337
[19:39:01.263704] Test:  [400/800]  eta: 0:14:07  loss: 0.2783 (0.2965)  time: 2.0839  data: 2.0702  max mem: 337
[19:39:01.621530] Test:  [390/800]  eta: 0:14:50  loss: 0.3208 (0.2961)  time: 2.0578  data: 2.0460  max mem: 337
[19:39:01.878609] Test:  [390/800]  eta: 0:14:51  loss: 0.3271 (0.2980)  time: 2.0644  data: 2.0525  max mem: 337
[19:39:11.862182] Test:  [400/800]  eta: 0:14:17  loss: 0.2951 (0.2973)  time: 2.1292  data: 2.1155  max mem: 337
[19:39:18.046455] Test:  [410/800]  eta: 0:13:41  loss: 0.3041 (0.2966)  time: 2.0927  data: 2.0795  max mem: 337
[19:39:18.083532] Test:  [410/800]  eta: 0:13:41  loss: 0.3078 (0.2974)  time: 2.0925  data: 2.0793  max mem: 337
[19:39:19.248599] Test:  [410/800]  eta: 0:13:42  loss: 0.2968 (0.2968)  time: 2.1287  data: 2.1150  max mem: 337
[19:39:24.849134] Test:  [400/800]  eta: 0:14:30  loss: 0.2877 (0.2975)  time: 2.1268  data: 2.1149  max mem: 337
[19:39:24.868369] Test:  [400/800]  eta: 0:14:30  loss: 0.2759 (0.2968)  time: 2.1315  data: 2.1196  max mem: 337
[19:39:24.878710] Test:  [400/800]  eta: 0:14:30  loss: 0.2824 (0.2963)  time: 2.1318  data: 2.1200  max mem: 337
[19:39:25.172232] Test:  [400/800]  eta: 0:14:30  loss: 0.2956 (0.2964)  time: 2.1348  data: 2.1229  max mem: 337
[19:39:25.472024] Test:  [400/800]  eta: 0:14:31  loss: 0.2890 (0.2967)  time: 2.1279  data: 2.1143  max mem: 337
[19:39:25.475561] Test:  [400/800]  eta: 0:14:31  loss: 0.2965 (0.2957)  time: 2.1311  data: 2.1177  max mem: 337
[19:39:25.478125] Test:  [400/800]  eta: 0:14:31  loss: 0.2869 (0.2977)  time: 2.1285  data: 2.1150  max mem: 337
[19:39:25.485811] Test:  [400/800]  eta: 0:14:31  loss: 0.3001 (0.2964)  time: 2.1284  data: 2.1148  max mem: 337
[19:39:26.168278] Test:  [400/800]  eta: 0:14:31  loss: 0.2740 (0.2961)  time: 2.1326  data: 2.1208  max mem: 337
[19:39:26.172409] Test:  [400/800]  eta: 0:14:31  loss: 0.2634 (0.2968)  time: 2.1326  data: 2.1208  max mem: 337
[19:39:26.186253] Test:  [400/800]  eta: 0:14:31  loss: 0.2813 (0.2959)  time: 2.1326  data: 2.1208  max mem: 337
[19:39:27.231227] Test:  [400/800]  eta: 0:14:32  loss: 0.2988 (0.2983)  time: 2.1391  data: 2.1274  max mem: 337
[19:39:29.114736] Test:  [410/800]  eta: 0:13:52  loss: 0.3062 (0.2976)  time: 2.1312  data: 2.1177  max mem: 337
[19:39:41.995142] Test:  [420/800]  eta: 0:13:23  loss: 0.3127 (0.2975)  time: 2.0484  data: 2.0347  max mem: 337
[19:39:42.086067] Test:  [420/800]  eta: 0:13:23  loss: 0.3132 (0.2983)  time: 2.0530  data: 2.0393  max mem: 337
[19:39:42.252594] Test:  [420/800]  eta: 0:13:23  loss: 0.3064 (0.2974)  time: 2.0494  data: 2.0358  max mem: 337
[19:39:43.364262] Test:  [410/800]  eta: 0:14:05  loss: 0.2880 (0.2965)  time: 2.1438  data: 2.1319  max mem: 337
[19:39:43.385430] Test:  [410/800]  eta: 0:14:05  loss: 0.2973 (0.2978)  time: 2.1453  data: 2.1336  max mem: 337
[19:39:43.391165] Test:  [410/800]  eta: 0:14:05  loss: 0.2929 (0.2971)  time: 2.1449  data: 2.1331  max mem: 337
[19:39:44.068347] Test:  [410/800]  eta: 0:14:06  loss: 0.3060 (0.2968)  time: 2.1440  data: 2.1304  max mem: 337
[19:39:44.104055] Test:  [410/800]  eta: 0:14:06  loss: 0.3021 (0.2970)  time: 2.1422  data: 2.1288  max mem: 337
[19:39:44.125292] Test:  [410/800]  eta: 0:14:06  loss: 0.2965 (0.2959)  time: 2.1433  data: 2.1299  max mem: 337
[19:39:44.127741] Test:  [410/800]  eta: 0:14:06  loss: 0.3009 (0.2980)  time: 2.1434  data: 2.1300  max mem: 337
[19:39:44.279711] Test:  [410/800]  eta: 0:14:06  loss: 0.2970 (0.2961)  time: 2.1582  data: 2.1463  max mem: 337
[19:39:44.281232] Test:  [410/800]  eta: 0:14:06  loss: 0.2943 (0.2964)  time: 2.1557  data: 2.1439  max mem: 337
[19:39:44.284459] Test:  [410/800]  eta: 0:14:06  loss: 0.3043 (0.2970)  time: 2.1586  data: 2.1467  max mem: 337
[19:39:44.686824] Test:  [410/800]  eta: 0:14:07  loss: 0.3034 (0.2967)  time: 2.1532  data: 2.1415  max mem: 337
[19:39:45.210910] Test:  [410/800]  eta: 0:14:07  loss: 0.3033 (0.2985)  time: 2.1666  data: 2.1549  max mem: 337
[19:39:53.628411] Test:  [420/800]  eta: 0:13:33  loss: 0.3101 (0.2983)  time: 2.0883  data: 2.0746  max mem: 337
[19:39:59.876495] Test:  [430/800]  eta: 0:12:59  loss: 0.3050 (0.2981)  time: 2.0914  data: 2.0778  max mem: 337
[19:39:59.974839] Test:  [430/800]  eta: 0:12:59  loss: 0.3067 (0.2990)  time: 2.0945  data: 2.0808  max mem: 337
[19:40:00.086969] Test:  [430/800]  eta: 0:12:59  loss: 0.3062 (0.2981)  time: 2.0419  data: 2.0286  max mem: 337
[19:40:06.803211] Test:  [420/800]  eta: 0:13:45  loss: 0.3010 (0.2986)  time: 2.0976  data: 2.0859  max mem: 337
[19:40:06.839396] Test:  [420/800]  eta: 0:13:45  loss: 0.2983 (0.2974)  time: 2.0980  data: 2.0862  max mem: 337
[19:40:06.846186] Test:  [420/800]  eta: 0:13:45  loss: 0.3084 (0.2979)  time: 2.0988  data: 2.0870  max mem: 337
[19:40:07.325594] Test:  [420/800]  eta: 0:13:46  loss: 0.3073 (0.2975)  time: 2.1076  data: 2.0959  max mem: 337
[19:40:07.373211] Test:  [420/800]  eta: 0:13:46  loss: 0.3117 (0.2978)  time: 2.0950  data: 2.0816  max mem: 337
[19:40:07.377459] Test:  [420/800]  eta: 0:13:46  loss: 0.3158 (0.2976)  time: 2.0945  data: 2.0809  max mem: 337
[19:40:07.397140] Test:  [420/800]  eta: 0:13:46  loss: 0.3138 (0.2987)  time: 2.0959  data: 2.0825  max mem: 337
[19:40:07.398447] Test:  [420/800]  eta: 0:13:46  loss: 0.2967 (0.2967)  time: 2.0961  data: 2.0824  max mem: 337
[19:40:08.333360] Test:  [420/800]  eta: 0:13:47  loss: 0.3038 (0.2969)  time: 2.1073  data: 2.0955  max mem: 337
[19:40:08.346134] Test:  [420/800]  eta: 0:13:47  loss: 0.3190 (0.2972)  time: 2.1088  data: 2.0970  max mem: 337
[19:40:08.349397] Test:  [420/800]  eta: 0:13:47  loss: 0.3084 (0.2979)  time: 2.1088  data: 2.0969  max mem: 337
[19:40:09.364128] Test:  [420/800]  eta: 0:13:47  loss: 0.3044 (0.2992)  time: 2.1066  data: 2.0947  max mem: 337
[19:40:11.900764] Test:  [430/800]  eta: 0:13:09  loss: 0.2980 (0.2989)  time: 2.1393  data: 2.1256  max mem: 337
[19:40:25.143431] Test:  [440/800]  eta: 0:12:41  loss: 0.3164 (0.2996)  time: 2.1528  data: 2.1391  max mem: 337
[19:40:25.179983] Test:  [440/800]  eta: 0:12:41  loss: 0.3240 (0.2990)  time: 2.1592  data: 2.1455  max mem: 337
[19:40:25.260885] Test:  [440/800]  eta: 0:12:41  loss: 0.3083 (0.2989)  time: 2.1503  data: 2.1371  max mem: 337
[19:40:25.500003] Test:  [430/800]  eta: 0:13:21  loss: 0.3245 (0.2979)  time: 2.1067  data: 2.0949  max mem: 337
[19:40:25.509973] Test:  [430/800]  eta: 0:13:21  loss: 0.3146 (0.2984)  time: 2.1059  data: 2.0940  max mem: 337
[19:40:25.510772] Test:  [430/800]  eta: 0:13:21  loss: 0.3253 (0.2994)  time: 2.1062  data: 2.0944  max mem: 337
[19:40:26.358566] Test:  [430/800]  eta: 0:13:22  loss: 0.3181 (0.2974)  time: 2.1116  data: 2.0979  max mem: 337
[19:40:26.398619] Test:  [430/800]  eta: 0:13:22  loss: 0.2931 (0.2984)  time: 2.1147  data: 2.1010  max mem: 337
[19:40:26.399161] Test:  [430/800]  eta: 0:13:22  loss: 0.3138 (0.2994)  time: 2.1135  data: 2.0999  max mem: 337
[19:40:26.467429] Test:  [430/800]  eta: 0:13:22  loss: 0.3120 (0.2983)  time: 2.1199  data: 2.1062  max mem: 337
[19:40:26.664056] Test:  [430/800]  eta: 0:13:22  loss: 0.3166 (0.2979)  time: 2.1191  data: 2.1072  max mem: 337
[19:40:26.681280] Test:  [430/800]  eta: 0:13:22  loss: 0.3015 (0.2974)  time: 2.1200  data: 2.1082  max mem: 337
[19:40:26.696119] Test:  [430/800]  eta: 0:13:22  loss: 0.3118 (0.2987)  time: 2.1205  data: 2.1087  max mem: 337
[19:40:27.062921] Test:  [430/800]  eta: 0:13:22  loss: 0.3117 (0.2982)  time: 2.1187  data: 2.1069  max mem: 337
[19:40:27.733260] Test:  [430/800]  eta: 0:13:23  loss: 0.3264 (0.2998)  time: 2.1261  data: 2.1143  max mem: 337
[19:40:37.548987] Test:  [440/800]  eta: 0:12:51  loss: 0.3134 (0.2997)  time: 2.1960  data: 2.1824  max mem: 337
[19:40:41.736109] Test:  [450/800]  eta: 0:12:17  loss: 0.3210 (0.2994)  time: 2.0880  data: 2.0743  max mem: 337
[19:40:41.746420] Test:  [450/800]  eta: 0:12:17  loss: 0.3240 (0.2989)  time: 2.0935  data: 2.0797  max mem: 337
[19:40:41.921782] Test:  [450/800]  eta: 0:12:17  loss: 0.3442 (0.2990)  time: 2.0917  data: 2.0780  max mem: 337
[19:40:51.066791] Test:  [440/800]  eta: 0:13:02  loss: 0.3271 (0.3002)  time: 2.2131  data: 2.2014  max mem: 337
[19:40:51.079604] Test:  [440/800]  eta: 0:13:02  loss: 0.3301 (0.2988)  time: 2.2119  data: 2.2001  max mem: 337
[19:40:51.100858] Test:  [440/800]  eta: 0:13:02  loss: 0.3235 (0.2994)  time: 2.2127  data: 2.2008  max mem: 337
[19:40:51.698002] Test:  [440/800]  eta: 0:13:03  loss: 0.3268 (0.2991)  time: 2.2186  data: 2.2068  max mem: 337
[19:40:51.772726] Test:  [440/800]  eta: 0:13:03  loss: 0.3287 (0.3001)  time: 2.2187  data: 2.2056  max mem: 337
[19:40:51.777469] Test:  [440/800]  eta: 0:13:03  loss: 0.3183 (0.2981)  time: 2.2189  data: 2.2052  max mem: 337
[19:40:51.785914] Test:  [440/800]  eta: 0:13:03  loss: 0.3234 (0.2994)  time: 2.2206  data: 2.2074  max mem: 337
[19:40:52.014257] Test:  [440/800]  eta: 0:13:03  loss: 0.3268 (0.2992)  time: 2.2318  data: 2.2181  max mem: 337
[19:40:52.650727] Test:  [440/800]  eta: 0:13:04  loss: 0.3281 (0.2996)  time: 2.2150  data: 2.2031  max mem: 337
[19:40:52.652624] Test:  [440/800]  eta: 0:13:04  loss: 0.3341 (0.2988)  time: 2.2153  data: 2.2034  max mem: 337
[19:40:52.653738] Test:  [440/800]  eta: 0:13:04  loss: 0.3066 (0.2984)  time: 2.2160  data: 2.2041  max mem: 337
[19:40:53.739080] Test:  [440/800]  eta: 0:13:05  loss: 0.3264 (0.3007)  time: 2.2187  data: 2.2070  max mem: 337
[19:40:54.818195] Test:  [450/800]  eta: 0:12:27  loss: 0.3243 (0.2995)  time: 2.1458  data: 2.1322  max mem: 337
[19:41:07.086895] Test:  [460/800]  eta: 0:11:59  loss: 0.2675 (0.2986)  time: 2.0953  data: 2.0816  max mem: 337
[19:41:07.089185] Test:  [460/800]  eta: 0:11:59  loss: 0.2666 (0.2992)  time: 2.0972  data: 2.0836  max mem: 337
[19:41:07.089321] Test:  [460/800]  eta: 0:11:59  loss: 0.2698 (0.2987)  time: 2.0914  data: 2.0777  max mem: 337
[19:41:08.248475] Test:  [450/800]  eta: 0:12:37  loss: 0.3271 (0.3000)  time: 2.1368  data: 2.1252  max mem: 337
[19:41:08.256760] Test:  [450/800]  eta: 0:12:37  loss: 0.3301 (0.2990)  time: 2.1378  data: 2.1260  max mem: 337
[19:41:08.331676] Test:  [450/800]  eta: 0:12:37  loss: 0.3243 (0.2993)  time: 2.1410  data: 2.1292  max mem: 337
[19:41:08.893971] Test:  [450/800]  eta: 0:12:38  loss: 0.3296 (0.2991)  time: 2.0915  data: 2.0797  max mem: 337
[19:41:08.990899] Test:  [450/800]  eta: 0:12:38  loss: 0.3277 (0.2993)  time: 2.1296  data: 2.1164  max mem: 337
[19:41:09.011546] Test:  [450/800]  eta: 0:12:38  loss: 0.3318 (0.2981)  time: 2.1326  data: 2.1189  max mem: 337
[19:41:09.017685] Test:  [450/800]  eta: 0:12:38  loss: 0.3312 (0.3001)  time: 2.1309  data: 2.1177  max mem: 337
[19:41:09.312512] Test:  [450/800]  eta: 0:12:38  loss: 0.3268 (0.2990)  time: 2.1422  data: 2.1286  max mem: 337
[19:41:09.850493] Test:  [450/800]  eta: 0:12:38  loss: 0.3249 (0.2982)  time: 2.1584  data: 2.1466  max mem: 337
[19:41:09.875825] Test:  [450/800]  eta: 0:12:38  loss: 0.3281 (0.2996)  time: 2.1589  data: 2.1471  max mem: 337
[19:41:09.887830] Test:  [450/800]  eta: 0:12:38  loss: 0.3341 (0.2988)  time: 2.1611  data: 2.1493  max mem: 337
[19:41:11.228795] Test:  [450/800]  eta: 0:12:39  loss: 0.3278 (0.3005)  time: 2.1747  data: 2.1629  max mem: 337
[19:41:20.543117] Test:  [460/800]  eta: 0:12:09  loss: 0.2537 (0.2992)  time: 2.1497  data: 2.1361  max mem: 337
[19:41:22.627316] Test:  [470/800]  eta: 0:11:34  loss: 0.2675 (0.2996)  time: 2.0440  data: 2.0307  max mem: 337
[19:41:22.637933] Test:  [470/800]  eta: 0:11:34  loss: 0.2525 (0.2995)  time: 2.0358  data: 2.0221  max mem: 337
[19:41:22.670329] Test:  [470/800]  eta: 0:11:34  loss: 0.2666 (0.3000)  time: 2.0467  data: 2.0335  max mem: 337
[19:41:34.300073] Test:  [460/800]  eta: 0:12:19  loss: 0.2786 (0.2988)  time: 2.1610  data: 2.1492  max mem: 337
[19:41:34.381699] Test:  [460/800]  eta: 0:12:19  loss: 0.2706 (0.2990)  time: 2.1640  data: 2.1521  max mem: 337
[19:41:34.400908] Test:  [460/800]  eta: 0:12:19  loss: 0.2661 (0.2998)  time: 2.1667  data: 2.1548  max mem: 337
[19:41:35.039878] Test:  [460/800]  eta: 0:12:19  loss: 0.2610 (0.2988)  time: 2.1670  data: 2.1552  max mem: 337
[19:41:35.148660] Test:  [460/800]  eta: 0:12:19  loss: 0.2524 (0.2997)  time: 2.1688  data: 2.1551  max mem: 337
[19:41:35.148706] Test:  [460/800]  eta: 0:12:19  loss: 0.2699 (0.2988)  time: 2.1681  data: 2.1544  max mem: 337
[19:41:35.206157] Test:  [460/800]  eta: 0:12:19  loss: 0.2545 (0.2978)  time: 2.1714  data: 2.1577  max mem: 337
[19:41:35.377732] Test:  [460/800]  eta: 0:12:19  loss: 0.2563 (0.2986)  time: 2.1681  data: 2.1545  max mem: 337
[19:41:35.935662] Test:  [460/800]  eta: 0:12:20  loss: 0.2619 (0.2984)  time: 2.1641  data: 2.1523  max mem: 337
[19:41:35.958282] Test:  [460/800]  eta: 0:12:20  loss: 0.2564 (0.2978)  time: 2.1652  data: 2.1534  max mem: 337
[19:41:35.968688] Test:  [460/800]  eta: 0:12:20  loss: 0.2563 (0.2992)  time: 2.1659  data: 2.1540  max mem: 337
[19:41:36.715823] Test:  [470/800]  eta: 0:11:43  loss: 0.2525 (0.3000)  time: 2.0948  data: 2.0812  max mem: 337
[19:41:37.434426] Test:  [460/800]  eta: 0:12:21  loss: 0.2694 (0.3001)  time: 2.1847  data: 2.1728  max mem: 337
[19:41:48.032760] Test:  [480/800]  eta: 0:11:15  loss: 0.2781 (0.2993)  time: 2.0472  data: 2.0340  max mem: 337
[19:41:48.046854] Test:  [480/800]  eta: 0:11:15  loss: 0.2644 (0.2998)  time: 2.0478  data: 2.0347  max mem: 337
[19:41:48.084924] Test:  [480/800]  eta: 0:11:15  loss: 0.2733 (0.2990)  time: 2.0497  data: 2.0361  max mem: 337
[19:41:50.577249] Test:  [470/800]  eta: 0:11:53  loss: 0.2706 (0.2999)  time: 2.1122  data: 2.1004  max mem: 337
[19:41:50.638839] Test:  [470/800]  eta: 0:11:53  loss: 0.2661 (0.3006)  time: 2.1195  data: 2.1076  max mem: 337
[19:41:50.645165] Test:  [470/800]  eta: 0:11:53  loss: 0.2786 (0.2995)  time: 2.1194  data: 2.1076  max mem: 337
[19:41:51.487462] Test:  [470/800]  eta: 0:11:54  loss: 0.2487 (0.2996)  time: 2.1248  data: 2.1114  max mem: 337
[19:41:51.489239] Test:  [470/800]  eta: 0:11:54  loss: 0.2484 (0.2987)  time: 2.1238  data: 2.1102  max mem: 337
[19:41:51.496703] Test:  [470/800]  eta: 0:11:54  loss: 0.2524 (0.3005)  time: 2.1239  data: 2.1103  max mem: 337
[19:41:51.731072] Test:  [470/800]  eta: 0:11:54  loss: 0.2563 (0.2993)  time: 2.1209  data: 2.1072  max mem: 337
[19:41:52.300274] Test:  [470/800]  eta: 0:11:54  loss: 0.2563 (0.2999)  time: 2.1212  data: 2.1093  max mem: 337
[19:41:52.303873] Test:  [470/800]  eta: 0:11:54  loss: 0.2564 (0.2987)  time: 2.1226  data: 2.1108  max mem: 337
[19:41:52.308241] Test:  [470/800]  eta: 0:11:54  loss: 0.2619 (0.2992)  time: 2.1210  data: 2.1091  max mem: 337
[19:41:52.912106] Test:  [470/800]  eta: 0:11:55  loss: 0.2610 (0.2997)  time: 2.2009  data: 2.1890  max mem: 337
[19:41:53.782008] Test:  [470/800]  eta: 0:11:55  loss: 0.2694 (0.3011)  time: 2.1276  data: 2.1157  max mem: 337
[19:42:02.741769] Test:  [480/800]  eta: 0:11:25  loss: 0.2772 (0.2998)  time: 2.1099  data: 2.0962  max mem: 337
[19:42:04.500881] Test:  [490/800]  eta: 0:10:51  loss: 0.2781 (0.2994)  time: 2.0936  data: 2.0799  max mem: 337
[19:42:04.520056] Test:  [490/800]  eta: 0:10:51  loss: 0.2842 (0.2999)  time: 2.0924  data: 2.0788  max mem: 337
[19:42:04.930049] Test:  [490/800]  eta: 0:10:52  loss: 0.2789 (0.2991)  time: 2.1146  data: 2.1009  max mem: 337
[19:42:16.623484] Test:  [480/800]  eta: 0:11:34  loss: 0.3067 (0.3004)  time: 2.1111  data: 2.0993  max mem: 337
[19:42:16.649756] Test:  [480/800]  eta: 0:11:34  loss: 0.2734 (0.2992)  time: 2.1174  data: 2.1056  max mem: 337
[19:42:16.656855] Test:  [480/800]  eta: 0:11:34  loss: 0.2768 (0.2995)  time: 2.1137  data: 2.1019  max mem: 337
[19:42:17.404955] Test:  [480/800]  eta: 0:11:35  loss: 0.2901 (0.2995)  time: 2.1182  data: 2.1063  max mem: 337
[19:42:17.551076] Test:  [480/800]  eta: 0:11:35  loss: 0.2768 (0.2991)  time: 2.1201  data: 2.1067  max mem: 337
[19:42:17.577196] Test:  [480/800]  eta: 0:11:35  loss: 0.3016 (0.3002)  time: 2.1214  data: 2.1077  max mem: 337
[19:42:17.592242] Test:  [480/800]  eta: 0:11:35  loss: 0.3129 (0.2984)  time: 2.1193  data: 2.1056  max mem: 337
[19:42:17.916352] Test:  [480/800]  eta: 0:11:35  loss: 0.2904 (0.2989)  time: 2.1269  data: 2.1132  max mem: 337
[19:42:18.364867] Test:  [480/800]  eta: 0:11:36  loss: 0.2806 (0.2998)  time: 2.1198  data: 2.1079  max mem: 337
[19:42:18.400913] Test:  [480/800]  eta: 0:11:36  loss: 0.2830 (0.2990)  time: 2.1232  data: 2.1114  max mem: 337
[19:42:18.408800] Test:  [480/800]  eta: 0:11:36  loss: 0.2937 (0.2985)  time: 2.1225  data: 2.1107  max mem: 337
[19:42:19.631182] Test:  [490/800]  eta: 0:11:01  loss: 0.2691 (0.2999)  time: 2.1457  data: 2.1321  max mem: 337
[19:42:19.925766] Test:  [480/800]  eta: 0:11:37  loss: 0.2928 (0.3008)  time: 2.1245  data: 2.1126  max mem: 337
[19:42:29.300652] Test:  [500/800]  eta: 0:10:33  loss: 0.2802 (0.2990)  time: 2.0633  data: 2.0496  max mem: 337
[19:42:29.358840] Test:  [500/800]  eta: 0:10:33  loss: 0.2842 (0.2996)  time: 2.0656  data: 2.0519  max mem: 337
[19:42:29.366124] Test:  [500/800]  eta: 0:10:33  loss: 0.2789 (0.2986)  time: 2.0640  data: 2.0507  max mem: 337
[19:42:34.316703] Test:  [490/800]  eta: 0:11:10  loss: 0.2700 (0.2991)  time: 2.1835  data: 2.1717  max mem: 337
[19:42:34.346547] Test:  [490/800]  eta: 0:11:10  loss: 0.2747 (0.2997)  time: 2.1884  data: 2.1767  max mem: 337
[19:42:34.359430] Test:  [490/800]  eta: 0:11:10  loss: 0.2706 (0.3003)  time: 2.1860  data: 2.1742  max mem: 337
[19:42:35.257700] Test:  [490/800]  eta: 0:11:11  loss: 0.2678 (0.3001)  time: 2.1880  data: 2.1744  max mem: 337
[19:42:35.289060] Test:  [490/800]  eta: 0:11:11  loss: 0.2768 (0.2993)  time: 2.1900  data: 2.1764  max mem: 337
[19:42:35.379340] Test:  [490/800]  eta: 0:11:11  loss: 0.2803 (0.2986)  time: 2.1537  data: 2.1419  max mem: 337
[19:42:35.399541] Test:  [490/800]  eta: 0:11:11  loss: 0.2743 (0.2997)  time: 2.1549  data: 2.1430  max mem: 337
[19:42:35.437373] Test:  [490/800]  eta: 0:11:11  loss: 0.2747 (0.2990)  time: 2.1564  data: 2.1446  max mem: 337
[19:42:35.465300] Test:  [490/800]  eta: 0:11:11  loss: 0.2842 (0.2985)  time: 2.1988  data: 2.1851  max mem: 337
[19:42:35.478312] Test:  [490/800]  eta: 0:11:11  loss: 0.2787 (0.2989)  time: 2.1873  data: 2.1737  max mem: 337
[19:42:36.996747] Test:  [490/800]  eta: 0:11:12  loss: 0.2748 (0.3007)  time: 2.1607  data: 2.1488  max mem: 337
[19:42:38.376242] Test:  [490/800]  eta: 0:11:13  loss: 0.2870 (0.2995)  time: 2.2732  data: 2.2614  max mem: 337
[19:42:45.227725] Test:  [500/800]  eta: 0:10:42  loss: 0.2639 (0.2994)  time: 2.1243  data: 2.1107  max mem: 337
[19:42:45.589472] Test:  [510/800]  eta: 0:10:09  loss: 0.3018 (0.3008)  time: 2.0534  data: 2.0398  max mem: 337
[19:42:45.626948] Test:  [510/800]  eta: 0:10:09  loss: 0.2884 (0.3001)  time: 2.0563  data: 2.0429  max mem: 337
[19:42:45.812417] Test:  [510/800]  eta: 0:10:09  loss: 0.2978 (0.2996)  time: 2.0441  data: 2.0307  max mem: 337
[19:42:59.153032] Test:  [500/800]  eta: 0:10:50  loss: 0.2672 (0.2992)  time: 2.1248  data: 2.1130  max mem: 337
[19:42:59.170955] Test:  [500/800]  eta: 0:10:50  loss: 0.2679 (0.2988)  time: 2.1260  data: 2.1142  max mem: 337
[19:42:59.171390] Test:  [500/800]  eta: 0:10:50  loss: 0.2705 (0.2998)  time: 2.1273  data: 2.1156  max mem: 337
[19:43:00.135051] Test:  [500/800]  eta: 0:10:51  loss: 0.2741 (0.2992)  time: 2.1365  data: 2.1247  max mem: 337
[19:43:00.195749] Test:  [500/800]  eta: 0:10:51  loss: 0.2730 (0.2980)  time: 2.1301  data: 2.1165  max mem: 337
[19:43:00.207422] Test:  [500/800]  eta: 0:10:51  loss: 0.2677 (0.2997)  time: 2.1315  data: 2.1181  max mem: 337
[19:43:00.210746] Test:  [500/800]  eta: 0:10:51  loss: 0.2753 (0.2989)  time: 2.1329  data: 2.1195  max mem: 337
[19:43:00.724687] Test:  [500/800]  eta: 0:10:51  loss: 0.2709 (0.2983)  time: 2.1404  data: 2.1267  max mem: 337
[19:43:01.113693] Test:  [500/800]  eta: 0:10:52  loss: 0.2803 (0.2983)  time: 2.1352  data: 2.1234  max mem: 337
[19:43:01.118934] Test:  [500/800]  eta: 0:10:52  loss: 0.2557 (0.2991)  time: 2.1377  data: 2.1258  max mem: 337
[19:43:01.120165] Test:  [500/800]  eta: 0:10:52  loss: 0.2706 (0.2985)  time: 2.1359  data: 2.1241  max mem: 337
[19:43:01.951900] Test:  [510/800]  eta: 0:10:18  loss: 0.2887 (0.3005)  time: 2.1160  data: 2.1027  max mem: 337
[19:43:02.704519] Test:  [500/800]  eta: 0:10:53  loss: 0.2674 (0.3004)  time: 2.1389  data: 2.1270  max mem: 337
[19:43:09.488954] Test:  [520/800]  eta: 0:09:49  loss: 0.3442 (0.3000)  time: 2.0061  data: 1.9925  max mem: 337
[19:43:09.493336] Test:  [520/800]  eta: 0:09:49  loss: 0.3438 (0.3004)  time: 2.0096  data: 1.9964  max mem: 337
[19:43:09.536520] Test:  [520/800]  eta: 0:09:49  loss: 0.3485 (0.3010)  time: 2.0088  data: 1.9952  max mem: 337
[19:43:15.976803] Test:  [510/800]  eta: 0:10:26  loss: 0.3007 (0.3003)  time: 2.0815  data: 2.0696  max mem: 337
[19:43:16.011109] Test:  [510/800]  eta: 0:10:26  loss: 0.3084 (0.3000)  time: 2.0847  data: 2.0729  max mem: 337
[19:43:16.016322] Test:  [510/800]  eta: 0:10:26  loss: 0.2871 (0.3010)  time: 2.0828  data: 2.0710  max mem: 337
[19:43:17.220952] Test:  [510/800]  eta: 0:10:27  loss: 0.2770 (0.3008)  time: 2.0981  data: 2.0847  max mem: 337
[19:43:17.230804] Test:  [510/800]  eta: 0:10:27  loss: 0.2897 (0.3000)  time: 2.0970  data: 2.0836  max mem: 337
[19:43:17.270711] Test:  [510/800]  eta: 0:10:27  loss: 0.2932 (0.2990)  time: 2.0902  data: 2.0766  max mem: 337
[19:43:17.678787] Test:  [510/800]  eta: 0:10:27  loss: 0.2794 (0.2993)  time: 2.1100  data: 2.0963  max mem: 337
[19:43:18.001012] Test:  [510/800]  eta: 0:10:27  loss: 0.3092 (0.2998)  time: 2.1281  data: 2.1163  max mem: 337
[19:43:18.010083] Test:  [510/800]  eta: 0:10:27  loss: 0.2912 (0.3002)  time: 2.1305  data: 2.1186  max mem: 337
[19:43:18.010673] Test:  [510/800]  eta: 0:10:27  loss: 0.2850 (0.2995)  time: 2.1315  data: 2.1197  max mem: 337
[19:43:19.709442] Test:  [510/800]  eta: 0:10:28  loss: 0.3023 (0.3014)  time: 2.1356  data: 2.1239  max mem: 337
[19:43:21.186021] Test:  [510/800]  eta: 0:10:29  loss: 0.2868 (0.3002)  time: 2.1404  data: 2.1286  max mem: 337
[19:43:25.373576] Test:  [530/800]  eta: 0:09:26  loss: 0.3074 (0.3009)  time: 1.9892  data: 1.9755  max mem: 337
[19:43:25.430712] Test:  [530/800]  eta: 0:09:26  loss: 0.2887 (0.3000)  time: 1.9901  data: 1.9766  max mem: 337
[19:43:26.262989] Test:  [530/800]  eta: 0:09:26  loss: 0.2811 (0.2998)  time: 2.0225  data: 2.0089  max mem: 337
[19:43:26.326780] Test:  [520/800]  eta: 0:09:58  loss: 0.3343 (0.3006)  time: 2.0549  data: 2.0415  max mem: 337
[19:43:40.658102] Test:  [520/800]  eta: 0:10:06  loss: 0.3459 (0.3012)  time: 2.0743  data: 2.0626  max mem: 337
[19:43:40.662087] Test:  [520/800]  eta: 0:10:06  loss: 0.3439 (0.3005)  time: 2.0754  data: 2.0637  max mem: 337
[19:43:40.780722] Test:  [520/800]  eta: 0:10:06  loss: 0.3522 (0.3001)  time: 2.0804  data: 2.0687  max mem: 337
[19:43:41.797317] Test:  [520/800]  eta: 0:10:07  loss: 0.3401 (0.3010)  time: 2.0794  data: 2.0659  max mem: 337
[19:43:41.797807] Test:  [520/800]  eta: 0:10:07  loss: 0.3407 (0.3002)  time: 2.0793  data: 2.0656  max mem: 337
[19:43:41.821169] Test:  [520/800]  eta: 0:10:07  loss: 0.3143 (0.2991)  time: 2.0812  data: 2.0676  max mem: 337
[19:43:41.854433] Test:  [520/800]  eta: 0:10:07  loss: 0.3221 (0.3002)  time: 2.0859  data: 2.0741  max mem: 337
[19:43:42.487644] Test:  [530/800]  eta: 0:09:34  loss: 0.2799 (0.3003)  time: 2.0267  data: 2.0131  max mem: 337
[19:43:42.534273] Test:  [520/800]  eta: 0:10:07  loss: 0.3394 (0.2995)  time: 2.0904  data: 2.0768  max mem: 337
[19:43:42.727185] Test:  [520/800]  eta: 0:10:07  loss: 0.3554 (0.3004)  time: 2.0804  data: 2.0685  max mem: 337
[19:43:42.732272] Test:  [520/800]  eta: 0:10:07  loss: 0.3286 (0.2997)  time: 2.0809  data: 2.0691  max mem: 337
[19:43:42.767317] Test:  [520/800]  eta: 0:10:07  loss: 0.3401 (0.2998)  time: 2.0823  data: 2.0705  max mem: 337
[19:43:44.574623] Test:  [520/800]  eta: 0:10:08  loss: 0.3363 (0.3016)  time: 2.0935  data: 2.0818  max mem: 337
[19:43:50.479301] Test:  [540/800]  eta: 0:09:07  loss: 0.2948 (0.3007)  time: 2.0493  data: 2.0355  max mem: 337
[19:43:50.497833] Test:  [540/800]  eta: 0:09:07  loss: 0.3074 (0.3016)  time: 2.0480  data: 2.0343  max mem: 337
[19:43:50.709194] Test:  [540/800]  eta: 0:09:07  loss: 0.3026 (0.3005)  time: 2.0610  data: 2.0478  max mem: 337
[19:43:57.784927] Test:  [530/800]  eta: 0:09:42  loss: 0.2995 (0.3009)  time: 2.0884  data: 2.0767  max mem: 337
[19:43:57.803490] Test:  [530/800]  eta: 0:09:42  loss: 0.2807 (0.3002)  time: 2.0913  data: 2.0796  max mem: 337
[19:43:57.881529] Test:  [530/800]  eta: 0:09:42  loss: 0.2846 (0.3000)  time: 2.0935  data: 2.0818  max mem: 337
[19:43:58.840882] Test:  [530/800]  eta: 0:09:43  loss: 0.3008 (0.3007)  time: 2.0809  data: 2.0678  max mem: 337
[19:43:58.876200] Test:  [530/800]  eta: 0:09:43  loss: 0.2974 (0.2989)  time: 2.0802  data: 2.0666  max mem: 337
[19:43:58.878006] Test:  [530/800]  eta: 0:09:43  loss: 0.2921 (0.2999)  time: 2.0823  data: 2.0689  max mem: 337
[19:43:59.009428] Test:  [530/800]  eta: 0:09:43  loss: 0.2864 (0.2992)  time: 2.0665  data: 2.0528  max mem: 337
[19:43:59.063168] Test:  [530/800]  eta: 0:09:43  loss: 0.3079 (0.3001)  time: 2.0526  data: 2.0408  max mem: 337
[19:43:59.111906] Test:  [530/800]  eta: 0:09:43  loss: 0.2874 (0.2994)  time: 2.0550  data: 2.0434  max mem: 337
[19:43:59.114415] Test:  [530/800]  eta: 0:09:43  loss: 0.2955 (0.2996)  time: 2.0556  data: 2.0440  max mem: 337
[19:44:01.000584] Test:  [530/800]  eta: 0:09:44  loss: 0.2980 (0.3014)  time: 2.0645  data: 2.0527  max mem: 337
[19:44:03.906060] Test:  [530/800]  eta: 0:09:45  loss: 0.2860 (0.3000)  time: 2.1360  data: 2.1242  max mem: 337
[19:44:08.192706] Test:  [540/800]  eta: 0:09:15  loss: 0.2896 (0.3009)  time: 2.0933  data: 2.0797  max mem: 337
[19:44:08.215882] Test:  [550/800]  eta: 0:08:44  loss: 0.3176 (0.3008)  time: 2.1392  data: 2.1255  max mem: 337
[19:44:08.246865] Test:  [550/800]  eta: 0:08:44  loss: 0.3238 (0.3017)  time: 2.1436  data: 2.1299  max mem: 337
[19:44:08.595943] Test:  [550/800]  eta: 0:08:44  loss: 0.3142 (0.3007)  time: 2.1166  data: 2.1035  max mem: 337
[19:44:22.860185] Test:  [540/800]  eta: 0:09:22  loss: 0.3008 (0.3010)  time: 2.1099  data: 2.0980  max mem: 337
[19:44:22.925273] Test:  [540/800]  eta: 0:09:22  loss: 0.2995 (0.3016)  time: 2.1133  data: 2.1015  max mem: 337
[19:44:23.060548] Test:  [540/800]  eta: 0:09:22  loss: 0.2937 (0.3007)  time: 2.1139  data: 2.1021  max mem: 337
[19:44:23.939714] Test:  [540/800]  eta: 0:09:23  loss: 0.3085 (0.3006)  time: 2.1070  data: 2.0936  max mem: 337
[19:44:23.944914] Test:  [540/800]  eta: 0:09:23  loss: 0.3008 (0.3014)  time: 2.1073  data: 2.0941  max mem: 337
[19:44:23.980225] Test:  [540/800]  eta: 0:09:23  loss: 0.3089 (0.2996)  time: 2.1079  data: 2.0942  max mem: 337
[19:44:24.170573] Test:  [540/800]  eta: 0:09:23  loss: 0.2887 (0.3005)  time: 2.1158  data: 2.1040  max mem: 337
[19:44:24.952937] Test:  [540/800]  eta: 0:09:23  loss: 0.3089 (0.3000)  time: 2.1209  data: 2.1072  max mem: 337
[19:44:24.965170] Test:  [540/800]  eta: 0:09:23  loss: 0.3052 (0.3009)  time: 2.1119  data: 2.1000  max mem: 337
[19:44:24.990433] Test:  [540/800]  eta: 0:09:23  loss: 0.3069 (0.3004)  time: 2.1111  data: 2.0995  max mem: 337
[19:44:24.993588] Test:  [540/800]  eta: 0:09:23  loss: 0.3028 (0.3001)  time: 2.1130  data: 2.1014  max mem: 337
[19:44:26.435391] Test:  [550/800]  eta: 0:08:52  loss: 0.3215 (0.3011)  time: 2.1973  data: 2.1837  max mem: 337
[19:44:27.000010] Test:  [540/800]  eta: 0:09:24  loss: 0.3028 (0.3021)  time: 2.1212  data: 2.1095  max mem: 337
[19:44:32.175703] Test:  [560/800]  eta: 0:08:24  loss: 0.2995 (0.3004)  time: 2.0848  data: 2.0711  max mem: 337
[19:44:32.190407] Test:  [560/800]  eta: 0:08:24  loss: 0.2783 (0.3014)  time: 2.0846  data: 2.0709  max mem: 337
[19:44:32.643840] Test:  [560/800]  eta: 0:08:25  loss: 0.3087 (0.3004)  time: 2.0967  data: 2.0831  max mem: 337
[19:44:41.232313] Test:  [550/800]  eta: 0:08:59  loss: 0.3201 (0.3012)  time: 2.1714  data: 2.1597  max mem: 337
[19:44:41.248311] Test:  [550/800]  eta: 0:08:59  loss: 0.3178 (0.3018)  time: 2.1731  data: 2.1615  max mem: 337
[19:44:41.544808] Test:  [550/800]  eta: 0:08:59  loss: 0.3250 (0.3009)  time: 2.1831  data: 2.1715  max mem: 337
[19:44:42.365364] Test:  [550/800]  eta: 0:09:00  loss: 0.3256 (0.3016)  time: 2.1762  data: 2.1629  max mem: 337
[19:44:42.367702] Test:  [550/800]  eta: 0:09:00  loss: 0.3141 (0.2997)  time: 2.1745  data: 2.1609  max mem: 337
[19:44:42.396271] Test:  [550/800]  eta: 0:09:00  loss: 0.3115 (0.3007)  time: 2.1759  data: 2.1626  max mem: 337
[19:44:43.350882] Test:  [550/800]  eta: 0:09:00  loss: 0.3169 (0.3010)  time: 2.2143  data: 2.2025  max mem: 337
[19:44:43.384663] Test:  [550/800]  eta: 0:09:00  loss: 0.3254 (0.3003)  time: 2.2136  data: 2.2018  max mem: 337
[19:44:43.387660] Test:  [550/800]  eta: 0:09:00  loss: 0.3174 (0.3001)  time: 2.2189  data: 2.2052  max mem: 337
[19:44:43.408095] Test:  [550/800]  eta: 0:09:00  loss: 0.3304 (0.3006)  time: 2.2146  data: 2.2028  max mem: 337
[19:44:45.505898] Test:  [550/800]  eta: 0:09:01  loss: 0.3166 (0.3022)  time: 2.2252  data: 2.2135  max mem: 337
[19:44:46.694743] Test:  [550/800]  eta: 0:09:02  loss: 0.3056 (0.3006)  time: 2.1394  data: 2.1276  max mem: 337
[19:44:48.929099] Test:  [570/800]  eta: 0:08:02  loss: 0.2947 (0.3018)  time: 2.0341  data: 2.0204  max mem: 337
[19:44:48.933197] Test:  [570/800]  eta: 0:08:02  loss: 0.3025 (0.3006)  time: 2.0358  data: 2.0222  max mem: 337
[19:44:49.384270] Test:  [570/800]  eta: 0:08:02  loss: 0.2824 (0.3006)  time: 2.0394  data: 2.0258  max mem: 337
[19:44:51.146227] Test:  [560/800]  eta: 0:08:32  loss: 0.3051 (0.3008)  time: 2.1476  data: 2.1340  max mem: 337
[19:45:06.361072] Test:  [560/800]  eta: 0:08:39  loss: 0.3055 (0.3008)  time: 2.1750  data: 2.1633  max mem: 337
[19:45:06.446124] Test:  [560/800]  eta: 0:08:39  loss: 0.2862 (0.3014)  time: 2.1760  data: 2.1644  max mem: 337
[19:45:06.519672] Test:  [560/800]  eta: 0:08:39  loss: 0.3160 (0.3006)  time: 2.1729  data: 2.1613  max mem: 337
[19:45:07.299852] Test:  [560/800]  eta: 0:08:39  loss: 0.3016 (0.2993)  time: 2.1659  data: 2.1523  max mem: 337
[19:45:07.314498] Test:  [560/800]  eta: 0:08:39  loss: 0.2878 (0.3004)  time: 2.1687  data: 2.1555  max mem: 337
[19:45:07.331824] Test:  [560/800]  eta: 0:08:39  loss: 0.2976 (0.3011)  time: 2.1693  data: 2.1562  max mem: 337
[19:45:07.586296] Test:  [560/800]  eta: 0:08:40  loss: 0.2917 (0.3004)  time: 2.1707  data: 2.1591  max mem: 337
[19:45:08.155742] Test:  [570/800]  eta: 0:08:09  loss: 0.2826 (0.3010)  time: 2.0860  data: 2.0724  max mem: 337
[19:45:08.390702] Test:  [560/800]  eta: 0:08:40  loss: 0.3112 (0.3000)  time: 2.1698  data: 2.1580  max mem: 337
[19:45:08.394704] Test:  [560/800]  eta: 0:08:40  loss: 0.2958 (0.3001)  time: 2.1702  data: 2.1585  max mem: 337
[19:45:08.405152] Test:  [560/800]  eta: 0:08:40  loss: 0.3017 (0.3006)  time: 2.1720  data: 2.1601  max mem: 337
[19:45:08.443561] Test:  [560/800]  eta: 0:08:40  loss: 0.2855 (0.2997)  time: 2.1745  data: 2.1609  max mem: 337
[19:45:10.529892] Test:  [560/800]  eta: 0:08:41  loss: 0.3101 (0.3017)  time: 2.1764  data: 2.1646  max mem: 337
[19:45:13.647542] Test:  [580/800]  eta: 0:07:42  loss: 0.3427 (0.3028)  time: 2.0728  data: 2.0592  max mem: 337
[19:45:13.695084] Test:  [580/800]  eta: 0:07:42  loss: 0.3146 (0.3014)  time: 2.0759  data: 2.0623  max mem: 337
[19:45:14.197177] Test:  [580/800]  eta: 0:07:42  loss: 0.3298 (0.3017)  time: 2.0776  data: 2.0641  max mem: 337
[19:45:23.665915] Test:  [570/800]  eta: 0:08:16  loss: 0.2807 (0.3017)  time: 2.1208  data: 2.1090  max mem: 337
[19:45:23.667748] Test:  [570/800]  eta: 0:08:16  loss: 0.2902 (0.3010)  time: 2.1217  data: 2.1099  max mem: 337
[19:45:23.685042] Test:  [570/800]  eta: 0:08:16  loss: 0.2747 (0.3009)  time: 2.1070  data: 2.0952  max mem: 337
[19:45:24.517116] Test:  [570/800]  eta: 0:08:16  loss: 0.2778 (0.3014)  time: 2.1075  data: 2.0940  max mem: 337
[19:45:24.519743] Test:  [570/800]  eta: 0:08:16  loss: 0.2882 (0.2997)  time: 2.1076  data: 2.0939  max mem: 337
[19:45:24.522704] Test:  [570/800]  eta: 0:08:16  loss: 0.2871 (0.3005)  time: 2.1063  data: 2.0927  max mem: 337
[19:45:25.555086] Test:  [570/800]  eta: 0:08:16  loss: 0.2831 (0.3004)  time: 2.1085  data: 2.0967  max mem: 337
[19:45:25.600890] Test:  [570/800]  eta: 0:08:16  loss: 0.2906 (0.3004)  time: 2.1096  data: 2.0979  max mem: 337
[19:45:25.602743] Test:  [570/800]  eta: 0:08:16  loss: 0.2863 (0.3009)  time: 2.1125  data: 2.1007  max mem: 337
[19:45:25.717966] Test:  [570/800]  eta: 0:08:16  loss: 0.2909 (0.3000)  time: 2.1165  data: 2.1028  max mem: 337
[19:45:27.801924] Test:  [570/800]  eta: 0:08:17  loss: 0.2843 (0.3019)  time: 2.1148  data: 2.1029  max mem: 337
[19:45:28.703652] Test:  [570/800]  eta: 0:08:18  loss: 0.2913 (0.3007)  time: 2.1004  data: 2.0886  max mem: 337
[19:45:29.368688] Test:  [590/800]  eta: 0:07:19  loss: 0.3437 (0.3037)  time: 2.0219  data: 2.0086  max mem: 337
[19:45:29.373249] Test:  [590/800]  eta: 0:07:19  loss: 0.3205 (0.3021)  time: 2.0220  data: 2.0083  max mem: 337
[19:45:29.973876] Test:  [590/800]  eta: 0:07:19  loss: 0.3299 (0.3027)  time: 2.0294  data: 2.0163  max mem: 337
[19:45:33.480345] Test:  [580/800]  eta: 0:07:50  loss: 0.3279 (0.3019)  time: 2.1166  data: 2.1030  max mem: 337
[19:45:49.253044] Test:  [580/800]  eta: 0:07:56  loss: 0.3246 (0.3024)  time: 2.1403  data: 2.1286  max mem: 337
[19:45:49.274205] Test:  [580/800]  eta: 0:07:56  loss: 0.3175 (0.3021)  time: 2.1456  data: 2.1339  max mem: 337
[19:45:49.327495] Test:  [580/800]  eta: 0:07:56  loss: 0.3338 (0.3018)  time: 2.1403  data: 2.1287  max mem: 337
[19:45:49.492208] Test:  [590/800]  eta: 0:07:26  loss: 0.3547 (0.3027)  time: 2.0668  data: 2.0532  max mem: 337
[19:45:50.152004] Test:  [580/800]  eta: 0:07:56  loss: 0.3148 (0.3007)  time: 2.1426  data: 2.1289  max mem: 337
[19:45:50.165174] Test:  [580/800]  eta: 0:07:56  loss: 0.3222 (0.3024)  time: 2.1416  data: 2.1283  max mem: 337
[19:45:50.294915] Test:  [580/800]  eta: 0:07:56  loss: 0.3088 (0.3013)  time: 2.1490  data: 2.1353  max mem: 337
[19:45:50.441015] Test:  [580/800]  eta: 0:07:56  loss: 0.3260 (0.3016)  time: 2.1427  data: 2.1309  max mem: 337
[19:45:51.335808] Test:  [580/800]  eta: 0:07:56  loss: 0.3092 (0.3012)  time: 2.1472  data: 2.1354  max mem: 337
[19:45:51.338427] Test:  [580/800]  eta: 0:07:56  loss: 0.3178 (0.3017)  time: 2.1466  data: 2.1348  max mem: 337
[19:45:51.355036] Test:  [580/800]  eta: 0:07:56  loss: 0.3082 (0.3012)  time: 2.1480  data: 2.1362  max mem: 337
[19:45:51.504075] Test:  [580/800]  eta: 0:07:56  loss: 0.3030 (0.3007)  time: 2.1530  data: 2.1393  max mem: 337
[19:45:53.479831] Test:  [580/800]  eta: 0:07:57  loss: 0.3118 (0.3029)  time: 2.1475  data: 2.1356  max mem: 337
[19:45:53.716883] Test:  [600/800]  eta: 0:06:59  loss: 0.3025 (0.3034)  time: 2.0034  data: 1.9900  max mem: 337
[19:45:53.718435] Test:  [600/800]  eta: 0:06:59  loss: 0.3095 (0.3020)  time: 2.0011  data: 1.9877  max mem: 337
[19:45:54.302370] Test:  [600/800]  eta: 0:07:00  loss: 0.3037 (0.3025)  time: 2.0052  data: 1.9919  max mem: 337
[19:46:05.485293] Test:  [590/800]  eta: 0:07:32  loss: 0.3378 (0.3026)  time: 2.0900  data: 2.0783  max mem: 337
[19:46:05.537628] Test:  [590/800]  eta: 0:07:32  loss: 0.3279 (0.3031)  time: 2.0935  data: 2.0818  max mem: 337
[19:46:05.545857] Test:  [590/800]  eta: 0:07:32  loss: 0.3331 (0.3028)  time: 2.0939  data: 2.0822  max mem: 337
[19:46:06.403745] Test:  [590/800]  eta: 0:07:32  loss: 0.3150 (0.3016)  time: 2.0942  data: 2.0805  max mem: 337
[19:46:06.444667] Test:  [590/800]  eta: 0:07:32  loss: 0.3326 (0.3032)  time: 2.0963  data: 2.0830  max mem: 337
[19:46:06.633218] Test:  [590/800]  eta: 0:07:32  loss: 0.3140 (0.3020)  time: 2.1055  data: 2.0918  max mem: 337
[19:46:07.556587] Test:  [590/800]  eta: 0:07:33  loss: 0.3283 (0.3025)  time: 2.0976  data: 2.0858  max mem: 337
[19:46:07.561773] Test:  [590/800]  eta: 0:07:33  loss: 0.3284 (0.3021)  time: 2.1003  data: 2.0885  max mem: 337
[19:46:07.573220] Test:  [590/800]  eta: 0:07:33  loss: 0.3356 (0.3022)  time: 2.0986  data: 2.0869  max mem: 337
[19:46:07.777986] Test:  [590/800]  eta: 0:07:33  loss: 0.3217 (0.3016)  time: 2.1030  data: 2.0893  max mem: 337
[19:46:09.791550] Test:  [590/800]  eta: 0:07:34  loss: 0.3354 (0.3037)  time: 2.0994  data: 2.0876  max mem: 337
[19:46:10.267139] Test:  [610/800]  eta: 0:06:37  loss: 0.3277 (0.3046)  time: 2.0449  data: 2.0312  max mem: 337
[19:46:10.269774] Test:  [610/800]  eta: 0:06:37  loss: 0.3125 (0.3031)  time: 2.0448  data: 2.0314  max mem: 337
[19:46:10.389700] Test:  [590/800]  eta: 0:07:34  loss: 0.3309 (0.3024)  time: 2.0843  data: 2.0725  max mem: 337
[19:46:10.951342] Test:  [610/800]  eta: 0:06:37  loss: 0.3084 (0.3036)  time: 2.0488  data: 2.0351  max mem: 337
[19:46:14.116623] Test:  [600/800]  eta: 0:07:06  loss: 0.3105 (0.3025)  time: 2.0318  data: 2.0183  max mem: 337
[19:46:30.476402] Test:  [600/800]  eta: 0:07:12  loss: 0.3166 (0.3027)  time: 2.0601  data: 2.0482  max mem: 337
[19:46:30.510420] Test:  [600/800]  eta: 0:07:12  loss: 0.2925 (0.3025)  time: 2.0591  data: 2.0473  max mem: 337
[19:46:30.533644] Test:  [600/800]  eta: 0:07:12  loss: 0.3063 (0.3029)  time: 2.0640  data: 2.0521  max mem: 337
[19:46:31.163010] Test:  [610/800]  eta: 0:06:43  loss: 0.3246 (0.3036)  time: 2.0835  data: 2.0703  max mem: 337
[19:46:31.517197] Test:  [600/800]  eta: 0:07:12  loss: 0.2929 (0.3030)  time: 2.0676  data: 2.0539  max mem: 337
[19:46:31.565174] Test:  [600/800]  eta: 0:07:12  loss: 0.3082 (0.3016)  time: 2.0706  data: 2.0570  max mem: 337
[19:46:31.769092] Test:  [600/800]  eta: 0:07:12  loss: 0.2967 (0.3019)  time: 2.0737  data: 2.0600  max mem: 337
[19:46:31.885822] Test:  [600/800]  eta: 0:07:12  loss: 0.2956 (0.3022)  time: 2.0722  data: 2.0603  max mem: 337
[19:46:32.527506] Test:  [600/800]  eta: 0:07:12  loss: 0.3173 (0.3020)  time: 2.0586  data: 2.0469  max mem: 337
[19:46:32.589557] Test:  [600/800]  eta: 0:07:12  loss: 0.3117 (0.3024)  time: 2.0625  data: 2.0507  max mem: 337
[19:46:32.619315] Test:  [600/800]  eta: 0:07:12  loss: 0.3047 (0.3019)  time: 2.0641  data: 2.0523  max mem: 337
[19:46:32.861597] Test:  [600/800]  eta: 0:07:12  loss: 0.2851 (0.3015)  time: 2.0678  data: 2.0542  max mem: 337
[19:46:34.871854] Test:  [620/800]  eta: 0:06:17  loss: 0.3179 (0.3037)  time: 2.0576  data: 2.0440  max mem: 337
[19:46:34.877242] Test:  [620/800]  eta: 0:06:17  loss: 0.3228 (0.3052)  time: 2.0580  data: 2.0443  max mem: 337
[19:46:34.910936] Test:  [600/800]  eta: 0:07:13  loss: 0.3094 (0.3035)  time: 2.0715  data: 2.0598  max mem: 337
[19:46:35.651635] Test:  [620/800]  eta: 0:06:17  loss: 0.3118 (0.3042)  time: 2.0674  data: 2.0537  max mem: 337
[19:46:47.542354] Test:  [610/800]  eta: 0:06:49  loss: 0.3218 (0.3038)  time: 2.1028  data: 2.0910  max mem: 337
[19:46:47.545865] Test:  [610/800]  eta: 0:06:49  loss: 0.3217 (0.3040)  time: 2.1004  data: 2.0885  max mem: 337
[19:46:47.547934] Test:  [610/800]  eta: 0:06:49  loss: 0.3206 (0.3040)  time: 2.1001  data: 2.0882  max mem: 337
[19:46:48.682757] Test:  [610/800]  eta: 0:06:49  loss: 0.3443 (0.3028)  time: 2.1139  data: 2.1007  max mem: 337
[19:46:48.683249] Test:  [610/800]  eta: 0:06:49  loss: 0.3222 (0.3041)  time: 2.1119  data: 2.0982  max mem: 337
[19:46:48.918796] Test:  [610/800]  eta: 0:06:49  loss: 0.3209 (0.3030)  time: 2.1142  data: 2.1006  max mem: 337
[19:46:49.704032] Test:  [610/800]  eta: 0:06:49  loss: 0.3183 (0.3030)  time: 2.1071  data: 2.0954  max mem: 337
[19:46:49.704168] Test:  [610/800]  eta: 0:06:49  loss: 0.3123 (0.3035)  time: 2.1073  data: 2.0956  max mem: 337
[19:46:49.707115] Test:  [610/800]  eta: 0:06:49  loss: 0.3256 (0.3031)  time: 2.1066  data: 2.0948  max mem: 337
[19:46:49.975810] Test:  [610/800]  eta: 0:06:49  loss: 0.2851 (0.3024)  time: 2.1098  data: 2.0967  max mem: 337
[19:46:51.806003] Test:  [630/800]  eta: 0:05:55  loss: 0.3154 (0.3055)  time: 2.0769  data: 2.0638  max mem: 337
[19:46:51.867811] Test:  [630/800]  eta: 0:05:55  loss: 0.3126 (0.3039)  time: 2.0799  data: 2.0662  max mem: 337
[19:46:52.072899] Test:  [610/800]  eta: 0:06:50  loss: 0.3210 (0.3046)  time: 2.1140  data: 2.1023  max mem: 337
[19:46:52.608293] Test:  [610/800]  eta: 0:06:50  loss: 0.3134 (0.3033)  time: 2.1109  data: 2.0990  max mem: 337
[19:46:52.638213] Test:  [630/800]  eta: 0:05:55  loss: 0.3108 (0.3045)  time: 2.0843  data: 2.0711  max mem: 337
[19:46:56.518667] Test:  [620/800]  eta: 0:06:23  loss: 0.3246 (0.3042)  time: 2.1201  data: 2.1068  max mem: 337
[19:47:13.004268] Test:  [620/800]  eta: 0:06:28  loss: 0.3244 (0.3045)  time: 2.1263  data: 2.1145  max mem: 337
[19:47:13.009928] Test:  [620/800]  eta: 0:06:28  loss: 0.3246 (0.3045)  time: 2.1238  data: 2.1119  max mem: 337
[19:47:13.017747] Test:  [620/800]  eta: 0:06:28  loss: 0.3281 (0.3044)  time: 2.1253  data: 2.1135  max mem: 337
[19:47:13.882230] Test:  [630/800]  eta: 0:06:01  loss: 0.3118 (0.3044)  time: 2.1359  data: 2.1227  max mem: 337
[19:47:14.283563] Test:  [620/800]  eta: 0:06:29  loss: 0.3171 (0.3033)  time: 2.1359  data: 2.1227  max mem: 337
[19:47:14.388376] Test:  [620/800]  eta: 0:06:29  loss: 0.3238 (0.3047)  time: 2.1435  data: 2.1299  max mem: 337
[19:47:14.533605] Test:  [620/800]  eta: 0:06:29  loss: 0.3367 (0.3038)  time: 2.1382  data: 2.1245  max mem: 337
[19:47:14.539492] Test:  [620/800]  eta: 0:06:29  loss: 0.3127 (0.3038)  time: 2.1326  data: 2.1208  max mem: 337
[19:47:15.263868] Test:  [620/800]  eta: 0:06:29  loss: 0.3293 (0.3037)  time: 2.1368  data: 2.1250  max mem: 337
[19:47:15.269264] Test:  [620/800]  eta: 0:06:29  loss: 0.3250 (0.3042)  time: 2.1339  data: 2.1223  max mem: 337
[19:47:15.269299] Test:  [620/800]  eta: 0:06:29  loss: 0.3282 (0.3037)  time: 2.1325  data: 2.1208  max mem: 337
[19:47:15.685763] Test:  [620/800]  eta: 0:06:29  loss: 0.3232 (0.3031)  time: 2.1412  data: 2.1280  max mem: 337
[19:47:16.522265] Test:  [640/800]  eta: 0:05:35  loss: 0.3471 (0.3065)  time: 2.0822  data: 2.0690  max mem: 337
[19:47:16.529555] Test:  [640/800]  eta: 0:05:35  loss: 0.3378 (0.3050)  time: 2.0828  data: 2.0692  max mem: 337
[19:47:17.416558] Test:  [640/800]  eta: 0:05:35  loss: 0.3227 (0.3055)  time: 2.0882  data: 2.0750  max mem: 337
[19:47:17.757629] Test:  [620/800]  eta: 0:06:30  loss: 0.3210 (0.3051)  time: 2.1423  data: 2.1304  max mem: 337
[19:47:30.450638] Test:  [630/800]  eta: 0:06:05  loss: 0.3116 (0.3049)  time: 2.1452  data: 2.1334  max mem: 337
[19:47:30.501413] Test:  [630/800]  eta: 0:06:05  loss: 0.3145 (0.3047)  time: 2.1479  data: 2.1361  max mem: 337
[19:47:30.696401] Test:  [630/800]  eta: 0:06:06  loss: 0.3148 (0.3047)  time: 2.1574  data: 2.1455  max mem: 337
[19:47:31.962774] Test:  [630/800]  eta: 0:06:06  loss: 0.3143 (0.3050)  time: 2.1639  data: 2.1503  max mem: 337
[19:47:31.985736] Test:  [630/800]  eta: 0:06:06  loss: 0.3038 (0.3036)  time: 2.1651  data: 2.1514  max mem: 337
[19:47:32.162460] Test:  [630/800]  eta: 0:06:06  loss: 0.3291 (0.3042)  time: 2.1621  data: 2.1485  max mem: 337
[19:47:32.798836] Test:  [630/800]  eta: 0:06:06  loss: 0.3237 (0.3040)  time: 2.1545  data: 2.1427  max mem: 337
[19:47:32.821199] Test:  [630/800]  eta: 0:06:06  loss: 0.3169 (0.3044)  time: 2.1558  data: 2.1440  max mem: 337
[19:47:32.825838] Test:  [630/800]  eta: 0:06:06  loss: 0.3160 (0.3040)  time: 2.1560  data: 2.1443  max mem: 337
[19:47:33.105508] Test:  [650/800]  eta: 0:05:13  loss: 0.3296 (0.3049)  time: 2.0618  data: 2.0481  max mem: 337
[19:47:33.175848] Test:  [650/800]  eta: 0:05:13  loss: 0.3396 (0.3063)  time: 2.0684  data: 2.0548  max mem: 337
[19:47:33.238749] Test:  [630/800]  eta: 0:06:06  loss: 0.3169 (0.3034)  time: 2.1631  data: 2.1495  max mem: 337
[19:47:33.972818] Test:  [650/800]  eta: 0:05:13  loss: 0.3237 (0.3054)  time: 2.0667  data: 2.0530  max mem: 337
[19:47:35.360432] Test:  [630/800]  eta: 0:06:07  loss: 0.3114 (0.3053)  time: 2.1643  data: 2.1525  max mem: 337
[19:47:35.459181] Test:  [630/800]  eta: 0:06:07  loss: 0.3110 (0.3041)  time: 2.1425  data: 2.1306  max mem: 337
[19:47:39.523046] Test:  [640/800]  eta: 0:05:41  loss: 0.3403 (0.3053)  time: 2.1502  data: 2.1370  max mem: 337
[19:47:55.989898] Test:  [640/800]  eta: 0:05:45  loss: 0.3358 (0.3058)  time: 2.1486  data: 2.1368  max mem: 337
[19:47:56.003327] Test:  [640/800]  eta: 0:05:45  loss: 0.3614 (0.3060)  time: 2.1496  data: 2.1378  max mem: 337
[19:47:56.219786] Test:  [640/800]  eta: 0:05:45  loss: 0.3543 (0.3057)  time: 2.1607  data: 2.1489  max mem: 337
[19:47:56.425890] Test:  [650/800]  eta: 0:05:18  loss: 0.3403 (0.3052)  time: 2.1271  data: 2.1136  max mem: 337
[19:47:57.467982] Test:  [640/800]  eta: 0:05:45  loss: 0.3584 (0.3046)  time: 2.1592  data: 2.1458  max mem: 337
[19:47:57.536003] Test:  [640/800]  eta: 0:05:45  loss: 0.3394 (0.3061)  time: 2.1573  data: 2.1437  max mem: 337
[19:47:57.728588] Test:  [640/800]  eta: 0:05:45  loss: 0.3487 (0.3051)  time: 2.1594  data: 2.1475  max mem: 337
[19:47:57.809230] Test:  [640/800]  eta: 0:05:45  loss: 0.3457 (0.3053)  time: 2.1637  data: 2.1501  max mem: 337
[19:47:58.291617] Test:  [640/800]  eta: 0:05:46  loss: 0.3327 (0.3055)  time: 2.1511  data: 2.1393  max mem: 337
[19:47:58.325009] Test:  [640/800]  eta: 0:05:46  loss: 0.3412 (0.3050)  time: 2.1530  data: 2.1412  max mem: 337
[19:47:58.326254] Test:  [640/800]  eta: 0:05:46  loss: 0.3468 (0.3051)  time: 2.1528  data: 2.1412  max mem: 337
[19:47:58.699865] Test:  [660/800]  eta: 0:04:53  loss: 0.3246 (0.3072)  time: 2.1088  data: 2.0952  max mem: 338
[19:47:58.723271] Test:  [660/800]  eta: 0:04:53  loss: 0.3264 (0.3057)  time: 2.1096  data: 2.0959  max mem: 338
[19:47:58.813120] Test:  [640/800]  eta: 0:05:46  loss: 0.3411 (0.3044)  time: 2.1563  data: 2.1432  max mem: 337
[19:47:59.612250] Test:  [660/800]  eta: 0:04:53  loss: 0.3237 (0.3061)  time: 2.1097  data: 2.0966  max mem: 338
[19:48:00.947139] Test:  [640/800]  eta: 0:05:46  loss: 0.3378 (0.3064)  time: 2.1594  data: 2.1476  max mem: 337
[19:48:12.947905] Test:  [650/800]  eta: 0:05:22  loss: 0.3369 (0.3059)  time: 2.1248  data: 2.1130  max mem: 337
[19:48:12.980396] Test:  [650/800]  eta: 0:05:22  loss: 0.3390 (0.3057)  time: 2.1239  data: 2.1121  max mem: 337
[19:48:13.155351] Test:  [650/800]  eta: 0:05:22  loss: 0.3450 (0.3057)  time: 2.1229  data: 2.1110  max mem: 337
[19:48:14.444831] Test:  [650/800]  eta: 0:05:23  loss: 0.3435 (0.3046)  time: 2.1229  data: 2.1095  max mem: 337
[19:48:14.479494] Test:  [650/800]  eta: 0:05:23  loss: 0.3332 (0.3060)  time: 2.1258  data: 2.1122  max mem: 337
[19:48:14.732894] Test:  [650/800]  eta: 0:05:23  loss: 0.3521 (0.3053)  time: 2.1285  data: 2.1148  max mem: 337
[19:48:15.237611] Test:  [670/800]  eta: 0:04:31  loss: 0.3453 (0.3063)  time: 2.1066  data: 2.0932  max mem: 338
[19:48:15.297298] Test:  [670/800]  eta: 0:04:31  loss: 0.3433 (0.3078)  time: 2.1060  data: 2.0923  max mem: 338
[19:48:15.419779] Test:  [650/800]  eta: 0:05:23  loss: 0.3412 (0.3049)  time: 2.1310  data: 2.1192  max mem: 337
[19:48:15.454820] Test:  [650/800]  eta: 0:05:23  loss: 0.3527 (0.3051)  time: 2.1314  data: 2.1198  max mem: 337
[19:48:15.468220] Test:  [650/800]  eta: 0:05:23  loss: 0.3291 (0.3054)  time: 2.1323  data: 2.1206  max mem: 337
[19:48:15.834082] Test:  [650/800]  eta: 0:05:23  loss: 0.3411 (0.3044)  time: 2.1297  data: 2.1166  max mem: 337
[19:48:16.213069] Test:  [670/800]  eta: 0:04:32  loss: 0.3515 (0.3068)  time: 2.1120  data: 2.0988  max mem: 338
[19:48:17.975308] Test:  [650/800]  eta: 0:05:23  loss: 0.3315 (0.3062)  time: 2.1307  data: 2.1188  max mem: 337
[19:48:19.249966] Test:  [650/800]  eta: 0:05:24  loss: 0.3464 (0.3050)  time: 2.1895  data: 2.1776  max mem: 337
[19:48:22.453172] Test:  [660/800]  eta: 0:04:58  loss: 0.3209 (0.3059)  time: 2.1465  data: 2.1331  max mem: 338
[19:48:39.290281] Test:  [660/800]  eta: 0:05:02  loss: 0.3281 (0.3065)  time: 2.1650  data: 2.1532  max mem: 338
[19:48:39.383252] Test:  [660/800]  eta: 0:05:02  loss: 0.3241 (0.3067)  time: 2.1689  data: 2.1571  max mem: 338
[19:48:39.544489] Test:  [660/800]  eta: 0:05:02  loss: 0.3450 (0.3065)  time: 2.1662  data: 2.1543  max mem: 338
[19:48:39.560881] Test:  [670/800]  eta: 0:04:36  loss: 0.3420 (0.3066)  time: 2.1567  data: 2.1434  max mem: 338
[19:48:40.130115] Test:  [680/800]  eta: 0:04:11  loss: 0.3281 (0.3077)  time: 2.0715  data: 2.0578  max mem: 338
[19:48:40.140278] Test:  [680/800]  eta: 0:04:11  loss: 0.3170 (0.3062)  time: 2.0708  data: 2.0576  max mem: 338
[19:48:40.824716] Test:  [660/800]  eta: 0:05:02  loss: 0.3273 (0.3069)  time: 2.1644  data: 2.1508  max mem: 338
[19:48:40.864715] Test:  [660/800]  eta: 0:05:02  loss: 0.3066 (0.3053)  time: 2.1698  data: 2.1563  max mem: 338
[19:48:41.024848] Test:  [660/800]  eta: 0:05:02  loss: 0.3444 (0.3059)  time: 2.1648  data: 2.1529  max mem: 338
[19:48:41.129882] Test:  [660/800]  eta: 0:05:02  loss: 0.3442 (0.3061)  time: 2.1660  data: 2.1523  max mem: 338
[19:48:41.173487] Test:  [680/800]  eta: 0:04:11  loss: 0.3296 (0.3066)  time: 2.0780  data: 2.0643  max mem: 338
[19:48:41.745928] Test:  [660/800]  eta: 0:05:02  loss: 0.3255 (0.3059)  time: 2.1709  data: 2.1591  max mem: 338
[19:48:41.769971] Test:  [660/800]  eta: 0:05:02  loss: 0.3094 (0.3058)  time: 2.1722  data: 2.1604  max mem: 338
[19:48:41.772885] Test:  [660/800]  eta: 0:05:02  loss: 0.3263 (0.3062)  time: 2.1740  data: 2.1622  max mem: 338
[19:48:42.251416] Test:  [660/800]  eta: 0:05:02  loss: 0.3258 (0.3051)  time: 2.1719  data: 2.1582  max mem: 338
[19:48:44.400970] Test:  [660/800]  eta: 0:05:03  loss: 0.3178 (0.3069)  time: 2.1726  data: 2.1608  max mem: 338
[19:48:56.188670] Test:  [690/800]  eta: 0:03:49  loss: 0.2827 (0.3066)  time: 2.0445  data: 2.0308  max mem: 338
[19:48:56.190017] Test:  [690/800]  eta: 0:03:49  loss: 0.2968 (0.3051)  time: 2.0476  data: 2.0340  max mem: 338
[19:48:56.500714] Test:  [670/800]  eta: 0:04:39  loss: 0.3464 (0.3071)  time: 2.1760  data: 2.1641  max mem: 338
[19:48:56.619061] Test:  [670/800]  eta: 0:04:39  loss: 0.3498 (0.3075)  time: 2.1835  data: 2.1717  max mem: 338
[19:48:56.746472] Test:  [670/800]  eta: 0:04:39  loss: 0.3543 (0.3072)  time: 2.1795  data: 2.1677  max mem: 338
[19:48:57.236494] Test:  [690/800]  eta: 0:03:50  loss: 0.2688 (0.3055)  time: 2.0511  data: 2.0374  max mem: 338
[19:48:58.069414] Test:  [670/800]  eta: 0:04:40  loss: 0.3524 (0.3060)  time: 2.1812  data: 2.1680  max mem: 338
[19:48:58.088998] Test:  [670/800]  eta: 0:04:40  loss: 0.3471 (0.3076)  time: 2.1804  data: 2.1668  max mem: 338
[19:48:58.326542] Test:  [670/800]  eta: 0:04:40  loss: 0.3448 (0.3067)  time: 2.1796  data: 2.1660  max mem: 338
[19:48:58.941736] Test:  [670/800]  eta: 0:04:40  loss: 0.3521 (0.3069)  time: 2.1736  data: 2.1620  max mem: 338
[19:48:58.970100] Test:  [670/800]  eta: 0:04:40  loss: 0.3469 (0.3066)  time: 2.1757  data: 2.1641  max mem: 338
[19:48:58.976301] Test:  [670/800]  eta: 0:04:40  loss: 0.3493 (0.3064)  time: 2.1778  data: 2.1661  max mem: 338
[19:48:59.687282] Test:  [670/800]  eta: 0:04:40  loss: 0.3455 (0.3057)  time: 2.1926  data: 2.1790  max mem: 338
[19:49:01.153539] Test:  [670/800]  eta: 0:04:40  loss: 0.3526 (0.3066)  time: 2.0951  data: 2.0833  max mem: 338
[19:49:01.673156] Test:  [670/800]  eta: 0:04:40  loss: 0.3496 (0.3076)  time: 2.1848  data: 2.1730  max mem: 338
[19:49:05.090975] Test:  [680/800]  eta: 0:04:16  loss: 0.3263 (0.3064)  time: 2.1318  data: 2.1182  max mem: 338
[19:49:20.548722] Test:  [700/800]  eta: 0:03:29  loss: 0.1584 (0.3045)  time: 2.0209  data: 2.0072  max mem: 338
[19:49:20.566570] Test:  [700/800]  eta: 0:03:29  loss: 0.1648 (0.3030)  time: 2.0213  data: 2.0076  max mem: 338
[19:49:21.600935] Test:  [690/800]  eta: 0:03:53  loss: 0.2723 (0.3053)  time: 2.1020  data: 2.0883  max mem: 338
[19:49:21.769223] Test:  [700/800]  eta: 0:03:29  loss: 0.1657 (0.3034)  time: 2.0297  data: 2.0166  max mem: 338
[19:49:22.323144] Test:  [680/800]  eta: 0:04:19  loss: 0.3295 (0.3070)  time: 2.1516  data: 2.1398  max mem: 338
[19:49:22.345315] Test:  [680/800]  eta: 0:04:19  loss: 0.3359 (0.3074)  time: 2.1480  data: 2.1362  max mem: 338
[19:49:22.539196] Test:  [680/800]  eta: 0:04:19  loss: 0.3294 (0.3070)  time: 2.1497  data: 2.1378  max mem: 338
[19:49:23.929997] Test:  [680/800]  eta: 0:04:19  loss: 0.3278 (0.3074)  time: 2.1552  data: 2.1416  max mem: 338
[19:49:23.938826] Test:  [680/800]  eta: 0:04:19  loss: 0.3175 (0.3059)  time: 2.1537  data: 2.1404  max mem: 338
[19:49:24.022800] Test:  [680/800]  eta: 0:04:19  loss: 0.3276 (0.3064)  time: 2.1499  data: 2.1380  max mem: 338
[19:49:24.200244] Test:  [680/800]  eta: 0:04:19  loss: 0.3255 (0.3064)  time: 2.1535  data: 2.1398  max mem: 338
[19:49:24.743212] Test:  [680/800]  eta: 0:04:19  loss: 0.3213 (0.3067)  time: 2.1485  data: 2.1368  max mem: 338
[19:49:24.778507] Test:  [680/800]  eta: 0:04:19  loss: 0.3169 (0.3062)  time: 2.1504  data: 2.1387  max mem: 338
[19:49:24.780609] Test:  [680/800]  eta: 0:04:19  loss: 0.3239 (0.3064)  time: 2.1517  data: 2.1401  max mem: 338
[19:49:25.588595] Test:  [680/800]  eta: 0:04:19  loss: 0.3168 (0.3055)  time: 2.1668  data: 2.1532  max mem: 338
[19:49:27.490871] Test:  [680/800]  eta: 0:04:19  loss: 0.3293 (0.3073)  time: 2.1545  data: 2.1428  max mem: 338
[19:49:36.970649] Test:  [710/800]  eta: 0:03:07  loss: 0.1584 (0.3028)  time: 2.0391  data: 2.0253  max mem: 338
[19:49:36.981617] Test:  [710/800]  eta: 0:03:07  loss: 0.1648 (0.3013)  time: 2.0395  data: 2.0258  max mem: 338
[19:49:38.246156] Test:  [710/800]  eta: 0:03:08  loss: 0.1657 (0.3018)  time: 2.0504  data: 2.0373  max mem: 338
[19:49:38.876335] Test:  [690/800]  eta: 0:03:56  loss: 0.2937 (0.3060)  time: 2.1187  data: 2.1069  max mem: 338
[19:49:38.886892] Test:  [690/800]  eta: 0:03:56  loss: 0.2916 (0.3062)  time: 2.1133  data: 2.1015  max mem: 338
[19:49:39.084858] Test:  [690/800]  eta: 0:03:56  loss: 0.2856 (0.3058)  time: 2.1169  data: 2.1050  max mem: 338
[19:49:40.547896] Test:  [690/800]  eta: 0:03:56  loss: 0.2795 (0.3062)  time: 2.1229  data: 2.1093  max mem: 338
[19:49:40.559953] Test:  [690/800]  eta: 0:03:56  loss: 0.2921 (0.3048)  time: 2.1245  data: 2.1112  max mem: 338
[19:49:40.790757] Test:  [690/800]  eta: 0:03:56  loss: 0.2878 (0.3053)  time: 2.1232  data: 2.1095  max mem: 338
[19:49:41.310889] Test:  [690/800]  eta: 0:03:57  loss: 0.2668 (0.3054)  time: 2.1170  data: 2.1052  max mem: 338
[19:49:41.315555] Test:  [690/800]  eta: 0:03:57  loss: 0.2783 (0.3056)  time: 2.1186  data: 2.1068  max mem: 338
[19:49:41.377387] Test:  [690/800]  eta: 0:03:57  loss: 0.2862 (0.3051)  time: 2.1200  data: 2.1082  max mem: 338
[19:49:42.289926] Test:  [690/800]  eta: 0:03:57  loss: 0.2689 (0.3044)  time: 2.1301  data: 2.1164  max mem: 338
[19:49:43.079431] Test:  [690/800]  eta: 0:03:57  loss: 0.2842 (0.3053)  time: 2.0962  data: 2.0844  max mem: 338
[19:49:44.128728] Test:  [690/800]  eta: 0:03:57  loss: 0.2776 (0.3062)  time: 2.1227  data: 2.1110  max mem: 338
[19:49:46.574400] Test:  [700/800]  eta: 0:03:33  loss: 0.1590 (0.3033)  time: 2.0741  data: 2.0605  max mem: 338
[19:50:01.701086] Test:  [720/800]  eta: 0:02:47  loss: 0.1826 (0.3011)  time: 2.0576  data: 2.0439  max mem: 338
[19:50:01.712305] Test:  [720/800]  eta: 0:02:47  loss: 0.1724 (0.2997)  time: 2.0572  data: 2.0435  max mem: 338
[19:50:03.179982] Test:  [720/800]  eta: 0:02:47  loss: 0.1775 (0.3001)  time: 2.0705  data: 2.0568  max mem: 338
[19:50:03.367305] Test:  [710/800]  eta: 0:03:11  loss: 0.1550 (0.3017)  time: 2.0883  data: 2.0747  max mem: 338
[19:50:04.178301] Test:  [700/800]  eta: 0:03:35  loss: 0.1455 (0.3041)  time: 2.0916  data: 2.0798  max mem: 338
[19:50:04.272081] Test:  [700/800]  eta: 0:03:35  loss: 0.1517 (0.3038)  time: 2.0974  data: 2.0856  max mem: 338
[19:50:04.376995] Test:  [700/800]  eta: 0:03:35  loss: 0.1471 (0.3037)  time: 2.0918  data: 2.0800  max mem: 338
[19:50:05.834357] Test:  [700/800]  eta: 0:03:35  loss: 0.1457 (0.3041)  time: 2.0952  data: 2.0815  max mem: 338
[19:50:05.866035] Test:  [700/800]  eta: 0:03:35  loss: 0.1657 (0.3027)  time: 2.0963  data: 2.0831  max mem: 338
[19:50:05.983901] Test:  [700/800]  eta: 0:03:35  loss: 0.1480 (0.3031)  time: 2.0980  data: 2.0862  max mem: 338
[19:50:06.192600] Test:  [700/800]  eta: 0:03:35  loss: 0.1569 (0.3032)  time: 2.0996  data: 2.0859  max mem: 338
[19:50:06.571485] Test:  [700/800]  eta: 0:03:36  loss: 0.1518 (0.3030)  time: 2.0896  data: 2.0780  max mem: 338
[19:50:06.577254] Test:  [700/800]  eta: 0:03:36  loss: 0.1512 (0.3032)  time: 2.0898  data: 2.0782  max mem: 338
[19:50:06.581701] Test:  [700/800]  eta: 0:03:36  loss: 0.1624 (0.3035)  time: 2.0919  data: 2.0802  max mem: 338
[19:50:07.751526] Test:  [700/800]  eta: 0:03:36  loss: 0.1489 (0.3023)  time: 2.1081  data: 2.0945  max mem: 338
[19:50:09.522128] Test:  [700/800]  eta: 0:03:36  loss: 0.1580 (0.3040)  time: 2.1015  data: 2.0897  max mem: 338
[19:50:18.042703] Test:  [730/800]  eta: 0:02:26  loss: 0.1724 (0.2976)  time: 2.0530  data: 2.0393  max mem: 338
[19:50:18.070613] Test:  [730/800]  eta: 0:02:26  loss: 0.1826 (0.2990)  time: 2.0550  data: 2.0413  max mem: 338
[19:50:19.508326] Test:  [730/800]  eta: 0:02:26  loss: 0.1775 (0.2981)  time: 2.0631  data: 2.0494  max mem: 338
[19:50:21.149375] Test:  [710/800]  eta: 0:03:13  loss: 0.1448 (0.3024)  time: 2.1131  data: 2.1012  max mem: 338
[19:50:21.251541] Test:  [710/800]  eta: 0:03:13  loss: 0.1517 (0.3021)  time: 2.1187  data: 2.1069  max mem: 338
[19:50:21.327789] Test:  [710/800]  eta: 0:03:13  loss: 0.1446 (0.3022)  time: 2.1121  data: 2.1003  max mem: 338
[19:50:22.826664] Test:  [710/800]  eta: 0:03:13  loss: 0.1570 (0.3024)  time: 2.1139  data: 2.1003  max mem: 338
[19:50:22.829172] Test:  [710/800]  eta: 0:03:13  loss: 0.1657 (0.3011)  time: 2.1134  data: 2.0999  max mem: 338
[19:50:23.170586] Test:  [710/800]  eta: 0:03:13  loss: 0.1569 (0.3017)  time: 2.1189  data: 2.1053  max mem: 338
[19:50:23.520779] Test:  [710/800]  eta: 0:03:13  loss: 0.1624 (0.3019)  time: 2.1102  data: 2.0985  max mem: 338
[19:50:23.521930] Test:  [710/800]  eta: 0:03:13  loss: 0.1512 (0.3015)  time: 2.1105  data: 2.0989  max mem: 338
[19:50:23.538768] Test:  [710/800]  eta: 0:03:13  loss: 0.1518 (0.3013)  time: 2.1080  data: 2.0964  max mem: 338
[19:50:24.716392] Test:  [710/800]  eta: 0:03:14  loss: 0.1600 (0.3007)  time: 2.1213  data: 2.1076  max mem: 338
[19:50:26.464173] Test:  [710/800]  eta: 0:03:14  loss: 0.1580 (0.3023)  time: 2.1167  data: 2.1049  max mem: 338
[19:50:27.201034] Test:  [710/800]  eta: 0:03:14  loss: 0.1480 (0.3015)  time: 2.2060  data: 2.1942  max mem: 338
[19:50:28.511836] Test:  [720/800]  eta: 0:02:50  loss: 0.1839 (0.3000)  time: 2.0968  data: 2.0832  max mem: 338
[19:50:41.596002] Test:  [740/800]  eta: 0:02:05  loss: 0.0854 (0.2963)  time: 1.9947  data: 1.9810  max mem: 338
[19:50:41.648464] Test:  [740/800]  eta: 0:02:05  loss: 0.0812 (0.2948)  time: 1.9968  data: 1.9831  max mem: 338
[19:50:43.163837] Test:  [740/800]  eta: 0:02:05  loss: 0.0880 (0.2954)  time: 1.9991  data: 1.9856  max mem: 338
[19:50:45.437969] Test:  [730/800]  eta: 0:02:28  loss: 0.1776 (0.2979)  time: 2.1035  data: 2.0902  max mem: 338
[19:50:46.730356] Test:  [720/800]  eta: 0:02:52  loss: 0.1607 (0.3007)  time: 2.1275  data: 2.1157  max mem: 338
[19:50:46.754786] Test:  [720/800]  eta: 0:02:52  loss: 0.1757 (0.3005)  time: 2.1241  data: 2.1123  max mem: 338
[19:50:46.945633] Test:  [720/800]  eta: 0:02:52  loss: 0.1906 (0.3006)  time: 2.1284  data: 2.1165  max mem: 338
[19:50:48.453024] Test:  [720/800]  eta: 0:02:52  loss: 0.1798 (0.2998)  time: 2.1234  data: 2.1116  max mem: 338
[19:50:48.473877] Test:  [720/800]  eta: 0:02:52  loss: 0.1734 (0.2995)  time: 2.1303  data: 2.1171  max mem: 338
[19:50:48.480416] Test:  [720/800]  eta: 0:02:52  loss: 0.1840 (0.3008)  time: 2.1323  data: 2.1186  max mem: 338
[19:50:48.642964] Test:  [720/800]  eta: 0:02:52  loss: 0.1789 (0.3000)  time: 2.1225  data: 2.1088  max mem: 338
[19:50:49.010974] Test:  [720/800]  eta: 0:02:52  loss: 0.1834 (0.3003)  time: 2.1214  data: 2.1096  max mem: 338
[19:50:49.014363] Test:  [720/800]  eta: 0:02:52  loss: 0.1784 (0.2997)  time: 2.1221  data: 2.1103  max mem: 338
[19:50:49.014967] Test:  [720/800]  eta: 0:02:52  loss: 0.1776 (0.2998)  time: 2.1218  data: 2.1100  max mem: 338
[19:50:50.320606] Test:  [720/800]  eta: 0:02:52  loss: 0.1855 (0.2991)  time: 2.1284  data: 2.1148  max mem: 338
[19:50:52.143080] Test:  [720/800]  eta: 0:02:53  loss: 0.1821 (0.3007)  time: 2.1310  data: 2.1191  max mem: 338
[19:50:57.726862] Test:  [750/800]  eta: 0:01:44  loss: 0.1204 (0.2953)  time: 1.9828  data: 1.9691  max mem: 338
[19:50:57.738609] Test:  [750/800]  eta: 0:01:44  loss: 0.1138 (0.2938)  time: 1.9847  data: 1.9711  max mem: 338
[19:50:59.290167] Test:  [750/800]  eta: 0:01:44  loss: 0.1024 (0.2942)  time: 1.9890  data: 1.9758  max mem: 338
[19:51:03.727823] Test:  [730/800]  eta: 0:02:30  loss: 0.1757 (0.2984)  time: 2.1238  data: 2.1120  max mem: 338
[19:51:03.740877] Test:  [730/800]  eta: 0:02:30  loss: 0.1822 (0.2987)  time: 2.1295  data: 2.1178  max mem: 338
[19:51:03.886170] Test:  [730/800]  eta: 0:02:30  loss: 0.1842 (0.2985)  time: 2.1279  data: 2.1160  max mem: 338
[19:51:05.555307] Test:  [730/800]  eta: 0:02:30  loss: 0.1653 (0.2987)  time: 2.1364  data: 2.1228  max mem: 338
[19:51:05.647467] Test:  [730/800]  eta: 0:02:30  loss: 0.1724 (0.2974)  time: 2.1409  data: 2.1276  max mem: 338
[19:51:05.690670] Test:  [730/800]  eta: 0:02:30  loss: 0.1778 (0.2979)  time: 2.1260  data: 2.1123  max mem: 338
[19:51:05.954499] Test:  [730/800]  eta: 0:02:30  loss: 0.1808 (0.2982)  time: 2.1216  data: 2.1100  max mem: 338
[19:51:05.963978] Test:  [730/800]  eta: 0:02:30  loss: 0.1784 (0.2977)  time: 2.1212  data: 2.1094  max mem: 338
[19:51:05.965370] Test:  [730/800]  eta: 0:02:30  loss: 0.1776 (0.2978)  time: 2.1221  data: 2.1103  max mem: 338
[19:51:07.342349] Test:  [730/800]  eta: 0:02:30  loss: 0.1714 (0.2970)  time: 2.1313  data: 2.1176  max mem: 338
[19:51:09.017863] Test:  [730/800]  eta: 0:02:31  loss: 0.1788 (0.2978)  time: 2.0908  data: 2.0789  max mem: 338
[19:51:09.213083] Test:  [730/800]  eta: 0:02:31  loss: 0.1807 (0.2987)  time: 2.1374  data: 2.1255  max mem: 338
[19:51:09.288621] Test:  [740/800]  eta: 0:02:07  loss: 0.0842 (0.2953)  time: 2.0388  data: 2.0257  max mem: 338
[19:51:21.755025] Test:  [760/800]  eta: 0:01:23  loss: 0.1903 (0.2922)  time: 2.0053  data: 1.9919  max mem: 338
[19:51:21.808600] Test:  [760/800]  eta: 0:01:23  loss: 0.1962 (0.2938)  time: 2.0106  data: 1.9974  max mem: 338
[19:51:23.444749] Test:  [760/800]  eta: 0:01:23  loss: 0.1919 (0.2926)  time: 2.0140  data: 2.0007  max mem: 338
[19:51:25.363692] Test:  [750/800]  eta: 0:01:46  loss: 0.1061 (0.2942)  time: 1.9962  data: 1.9827  max mem: 338
[19:51:28.024730] Test:  [740/800]  eta: 0:02:09  loss: 0.0843 (0.2958)  time: 2.0635  data: 2.0517  max mem: 338
[19:51:28.035788] Test:  [740/800]  eta: 0:02:09  loss: 0.0855 (0.2960)  time: 2.0652  data: 2.0536  max mem: 338
[19:51:28.114391] Test:  [740/800]  eta: 0:02:09  loss: 0.0839 (0.2958)  time: 2.0584  data: 2.0465  max mem: 338
[19:51:29.678895] Test:  [740/800]  eta: 0:02:09  loss: 0.0848 (0.2951)  time: 2.0612  data: 2.0494  max mem: 338
[19:51:29.852967] Test:  [740/800]  eta: 0:02:09  loss: 0.1021 (0.2953)  time: 2.0605  data: 2.0468  max mem: 338
[19:51:29.861402] Test:  [740/800]  eta: 0:02:09  loss: 0.0786 (0.2960)  time: 2.0690  data: 2.0554  max mem: 338
[19:51:29.877166] Test:  [740/800]  eta: 0:02:09  loss: 0.0898 (0.2947)  time: 2.0701  data: 2.0565  max mem: 338
[19:51:30.254265] Test:  [740/800]  eta: 0:02:09  loss: 0.0850 (0.2950)  time: 2.0619  data: 2.0501  max mem: 338
[19:51:30.255666] Test:  [740/800]  eta: 0:02:09  loss: 0.0913 (0.2956)  time: 2.0622  data: 2.0505  max mem: 338
[19:51:30.255963] Test:  [740/800]  eta: 0:02:09  loss: 0.0797 (0.2951)  time: 2.0620  data: 2.0502  max mem: 338
[19:51:31.595730] Test:  [740/800]  eta: 0:02:09  loss: 0.0873 (0.2943)  time: 2.0637  data: 2.0501  max mem: 338
[19:51:33.603910] Test:  [740/800]  eta: 0:02:09  loss: 0.0834 (0.2960)  time: 2.0730  data: 2.0611  max mem: 338
[19:51:37.856879] Test:  [770/800]  eta: 0:01:02  loss: 0.1585 (0.2907)  time: 2.0059  data: 1.9925  max mem: 338
[19:51:37.911055] Test:  [770/800]  eta: 0:01:02  loss: 0.1623 (0.2922)  time: 2.0092  data: 1.9960  max mem: 338
[19:51:39.544547] Test:  [770/800]  eta: 0:01:02  loss: 0.1667 (0.2911)  time: 2.0127  data: 1.9990  max mem: 338
[19:51:44.447759] Test:  [750/800]  eta: 0:01:47  loss: 0.1106 (0.2949)  time: 2.0353  data: 2.0235  max mem: 338
[19:51:44.485136] Test:  [750/800]  eta: 0:01:47  loss: 0.1201 (0.2946)  time: 2.0378  data: 2.0260  max mem: 338
[19:51:44.681130] Test:  [750/800]  eta: 0:01:47  loss: 0.1129 (0.2947)  time: 2.0397  data: 2.0279  max mem: 338
[19:51:46.383210] Test:  [750/800]  eta: 0:01:47  loss: 0.1184 (0.2941)  time: 2.0346  data: 2.0209  max mem: 338
[19:51:46.389139] Test:  [750/800]  eta: 0:01:47  loss: 0.1236 (0.2937)  time: 2.0370  data: 2.0234  max mem: 338
[19:51:46.394259] Test:  [750/800]  eta: 0:01:47  loss: 0.1174 (0.2949)  time: 2.0419  data: 2.0283  max mem: 338
[19:51:46.662632] Test:  [750/800]  eta: 0:01:47  loss: 0.1311 (0.2945)  time: 2.0354  data: 2.0235  max mem: 338
[19:51:46.715792] Test:  [750/800]  eta: 0:01:47  loss: 0.1259 (0.2939)  time: 2.0375  data: 2.0257  max mem: 338
[19:51:46.717372] Test:  [750/800]  eta: 0:01:47  loss: 0.1230 (0.2939)  time: 2.0376  data: 2.0258  max mem: 338
[19:51:48.130625] Test:  [750/800]  eta: 0:01:47  loss: 0.1139 (0.2933)  time: 2.0394  data: 2.0257  max mem: 338
[19:51:49.656909] Test:  [760/800]  eta: 0:01:25  loss: 0.2009 (0.2926)  time: 2.0184  data: 2.0047  max mem: 338
[19:51:49.669077] Test:  [750/800]  eta: 0:01:47  loss: 0.1185 (0.2940)  time: 2.0325  data: 2.0207  max mem: 338
[19:51:50.275323] Test:  [750/800]  eta: 0:01:47  loss: 0.1152 (0.2949)  time: 2.0531  data: 2.0412  max mem: 338
[19:52:02.058748] Test:  [780/800]  eta: 0:00:41  loss: 0.1608 (0.2894)  time: 2.0151  data: 2.0014  max mem: 338
[19:52:02.085155] Test:  [780/800]  eta: 0:00:41  loss: 0.1623 (0.2908)  time: 2.0138  data: 2.0001  max mem: 338
[19:52:03.800382] Test:  [780/800]  eta: 0:00:41  loss: 0.1585 (0.2897)  time: 2.0177  data: 2.0041  max mem: 338
[19:52:05.871109] Test:  [770/800]  eta: 0:01:03  loss: 0.1456 (0.2910)  time: 2.0253  data: 2.0117  max mem: 338
[19:52:09.142874] Test:  [760/800]  eta: 0:01:26  loss: 0.1896 (0.2932)  time: 2.0553  data: 2.0435  max mem: 338
[19:52:09.156166] Test:  [760/800]  eta: 0:01:26  loss: 0.1889 (0.2930)  time: 2.0565  data: 2.0449  max mem: 338
[19:52:09.389919] Test:  [760/800]  eta: 0:01:26  loss: 0.1809 (0.2931)  time: 2.0637  data: 2.0519  max mem: 338
[19:52:10.940267] Test:  [760/800]  eta: 0:01:26  loss: 0.1718 (0.2923)  time: 2.0630  data: 2.0513  max mem: 338
[19:52:11.054232] Test:  [760/800]  eta: 0:01:26  loss: 0.1873 (0.2932)  time: 2.0596  data: 2.0460  max mem: 338
[19:52:11.081925] Test:  [760/800]  eta: 0:01:26  loss: 0.1817 (0.2921)  time: 2.0602  data: 2.0466  max mem: 338
[19:52:11.113831] Test:  [760/800]  eta: 0:01:26  loss: 0.1831 (0.2925)  time: 2.0630  data: 2.0493  max mem: 338
[19:52:11.439082] Test:  [760/800]  eta: 0:01:26  loss: 0.1738 (0.2923)  time: 2.0591  data: 2.0473  max mem: 338
[19:52:11.445118] Test:  [760/800]  eta: 0:01:26  loss: 0.1870 (0.2922)  time: 2.0595  data: 2.0477  max mem: 338
[19:52:11.445811] Test:  [760/800]  eta: 0:01:26  loss: 0.1842 (0.2928)  time: 2.0595  data: 2.0476  max mem: 338
[19:52:12.976419] Test:  [760/800]  eta: 0:01:26  loss: 0.1962 (0.2917)  time: 2.0690  data: 2.0554  max mem: 338
[19:52:15.200970] Test:  [760/800]  eta: 0:01:26  loss: 0.1937 (0.2933)  time: 2.0798  data: 2.0679  max mem: 338
[19:52:17.848074] Test:  [790/800]  eta: 0:00:20  loss: 0.1400 (0.2874)  time: 1.9995  data: 1.9859  max mem: 338
[19:52:17.851469] Test:  [790/800]  eta: 0:00:20  loss: 0.1355 (0.2887)  time: 1.9970  data: 1.9833  max mem: 338
[19:52:19.671308] Test:  [790/800]  eta: 0:00:20  loss: 0.1453 (0.2876)  time: 2.0063  data: 1.9926  max mem: 338
[19:52:25.611046] Test:  [770/800]  eta: 0:01:04  loss: 0.1657 (0.2917)  time: 2.0581  data: 2.0463  max mem: 338
[19:52:25.647530] Test:  [770/800]  eta: 0:01:04  loss: 0.1610 (0.2915)  time: 2.0581  data: 2.0464  max mem: 338
[19:52:25.885181] Test:  [770/800]  eta: 0:01:04  loss: 0.1638 (0.2916)  time: 2.0602  data: 2.0483  max mem: 338
[19:52:27.622376] Test:  [770/800]  eta: 0:01:04  loss: 0.1622 (0.2909)  time: 2.0619  data: 2.0482  max mem: 338
[19:52:27.624010] Test:  [770/800]  eta: 0:01:04  loss: 0.1576 (0.2905)  time: 2.0617  data: 2.0484  max mem: 338
[19:52:27.626063] Test:  [770/800]  eta: 0:01:04  loss: 0.1656 (0.2917)  time: 2.0615  data: 2.0479  max mem: 338
[19:52:27.889469] Test:  [770/800]  eta: 0:01:04  loss: 0.1591 (0.2908)  time: 2.0586  data: 2.0470  max mem: 338
[19:52:27.982432] Test:  [770/800]  eta: 0:01:04  loss: 0.1571 (0.2913)  time: 2.0659  data: 2.0541  max mem: 338
[19:52:27.982645] Test:  [770/800]  eta: 0:01:04  loss: 0.1897 (0.2907)  time: 2.0632  data: 2.0514  max mem: 338
[19:52:29.548824] Test:  [770/800]  eta: 0:01:04  loss: 0.1753 (0.2902)  time: 2.0709  data: 2.0572  max mem: 338
[19:52:29.957866] Test:  [780/800]  eta: 0:00:42  loss: 0.1456 (0.2896)  time: 2.0150  data: 2.0019  max mem: 338
[19:52:31.796032] Test:  [770/800]  eta: 0:01:04  loss: 0.1467 (0.2918)  time: 2.0760  data: 2.0641  max mem: 338
[19:52:32.025414] Test:  [770/800]  eta: 0:01:04  loss: 0.1459 (0.2908)  time: 2.1178  data: 2.1060  max mem: 338
[19:52:32.855572] Test:  [799/800]  eta: 0:00:02  loss: 0.0750 (0.2870)  time: 1.8690  data: 1.8548  max mem: 338
[19:52:32.859628] Test:  [799/800]  eta: 0:00:02  loss: 0.0674 (0.2856)  time: 1.8686  data: 1.8544  max mem: 338
[19:52:33.037844] Test: Total time: 0:27:41 (2.0763 s / it)
[19:52:33.045274] (25596, 14) (25596, 14)
[19:52:33.047263] Test: Total time: 0:27:41 (2.0763 s / it)
[19:52:33.054806] (25596, 14) (25596, 14)
[19:52:33.136081] [0.727745344969214, 0.8748605181598043, 0.7968439432767236, 0.6558984647491672, 0.7714231753431535, 0.6687084610153561, 0.6854670412595415, 0.8308558598546272, 0.7129287878614117, 0.822138806148372, 0.8858924569573964, 0.7727137007784809, 0.745077411882219, 0.8920156710090889]
[19:52:33.146232] [0.7261591936192687, 0.8779078080577319, 0.7987797275572964, 0.6575947275103428, 0.7738892131094184, 0.6682495343716806, 0.6886741779517627, 0.8302256308038234, 0.7137700502391948, 0.8261006740598164, 0.8890362735052068, 0.7737721715828227, 0.7440330386869964, 0.906046420464387]
[19:52:34.144808] Test:  [799/800]  eta: 0:00:02  loss: 0.0617 (0.2859)  time: 1.7636  data: 1.7494  max mem: 338
[19:52:34.230341] Test: Total time: 0:27:42 (2.0777 s / it)
[19:52:34.237658] (25596, 14) (25596, 14)
[19:52:34.328306] [0.7268538749024067, 0.8746988437514672, 0.7988395045242038, 0.6529532622283177, 0.7751732319564719, 0.6664467771673405, 0.6827626116592211, 0.8343869278117411, 0.7174596522005264, 0.8250885874322297, 0.8899029074954281, 0.7800580354471227, 0.7445546528292764, 0.8988037067087234]
[19:52:36.216978] Test:  [790/800]  eta: 0:00:21  loss: 0.1266 (0.2875)  time: 1.5172  data: 1.5041  max mem: 338
[19:52:38.744458] Test:  [799/800]  eta: 0:00:02  loss: 0.0668 (0.2857)  time: 0.8265  data: 0.8124  max mem: 338
[19:52:38.816396] Test: Total time: 0:27:46 (2.0835 s / it)
[19:52:38.823749] (25596, 14) (25596, 14)
[19:52:38.915433] [0.7315749813777997, 0.8785775611711084, 0.7989802418565828, 0.6567468291542208, 0.7743127095172554, 0.6655117681040792, 0.688652124030104, 0.8338407669374057, 0.7132067257897275, 0.8238750606631924, 0.8839318702465583, 0.7769451171238831, 0.7469930978702908, 0.8963265659613648]
[19:52:50.411087] Test:  [780/800]  eta: 0:00:42  loss: 0.1615 (0.2903)  time: 2.0634  data: 2.0517  max mem: 338
[19:52:50.444289] Test:  [780/800]  eta: 0:00:42  loss: 0.1610 (0.2901)  time: 2.0644  data: 2.0525  max mem: 338
[19:52:50.684734] Test:  [780/800]  eta: 0:00:42  loss: 0.1557 (0.2902)  time: 2.0647  data: 2.0528  max mem: 338
[19:52:52.335490] Test:  [780/800]  eta: 0:00:43  loss: 0.1665 (0.2894)  time: 2.0697  data: 2.0578  max mem: 338
[19:52:52.427353] Test:  [780/800]  eta: 0:00:43  loss: 0.1409 (0.2891)  time: 2.0672  data: 2.0538  max mem: 338
[19:52:52.494421] Test:  [780/800]  eta: 0:00:43  loss: 0.1600 (0.2896)  time: 2.0690  data: 2.0553  max mem: 338
[19:52:52.496537] Test:  [780/800]  eta: 0:00:43  loss: 0.1656 (0.2903)  time: 2.0721  data: 2.0584  max mem: 338
[19:52:52.759112] Test:  [780/800]  eta: 0:00:43  loss: 0.1591 (0.2895)  time: 2.0660  data: 2.0543  max mem: 338
[19:52:52.778659] Test:  [780/800]  eta: 0:00:43  loss: 0.1551 (0.2893)  time: 2.0666  data: 2.0549  max mem: 338
[19:52:52.789117] Test:  [780/800]  eta: 0:00:43  loss: 0.1571 (0.2899)  time: 2.0671  data: 2.0553  max mem: 338
[19:52:54.446344] Test:  [780/800]  eta: 0:00:43  loss: 0.1544 (0.2888)  time: 2.0735  data: 2.0598  max mem: 338
[19:52:56.640736] Test:  [780/800]  eta: 0:00:43  loss: 0.1546 (0.2904)  time: 2.0719  data: 2.0601  max mem: 338
[19:53:06.604813] Test:  [790/800]  eta: 0:00:21  loss: 0.1486 (0.2883)  time: 2.0496  data: 2.0380  max mem: 338
[19:53:06.614843] Test:  [790/800]  eta: 0:00:21  loss: 0.1366 (0.2880)  time: 2.0483  data: 2.0365  max mem: 338
[19:53:06.907177] Test:  [790/800]  eta: 0:00:21  loss: 0.1396 (0.2882)  time: 2.0511  data: 2.0392  max mem: 338
[19:53:08.671809] Test:  [790/800]  eta: 0:00:21  loss: 0.1427 (0.2882)  time: 2.0522  data: 2.0386  max mem: 338
[19:53:08.701446] Test:  [790/800]  eta: 0:00:21  loss: 0.1468 (0.2875)  time: 2.0539  data: 2.0402  max mem: 338
[19:53:08.709282] Test:  [790/800]  eta: 0:00:21  loss: 0.1273 (0.2870)  time: 2.0542  data: 2.0406  max mem: 338
[19:53:08.980618] Test:  [790/800]  eta: 0:00:21  loss: 0.1459 (0.2873)  time: 2.0499  data: 2.0380  max mem: 338
[19:53:09.002435] Test:  [790/800]  eta: 0:00:21  loss: 0.1477 (0.2874)  time: 2.0556  data: 2.0438  max mem: 338
[19:53:09.006097] Test:  [790/800]  eta: 0:00:21  loss: 0.1340 (0.2878)  time: 2.0511  data: 2.0393  max mem: 338
[19:53:10.739792] Test:  [790/800]  eta: 0:00:21  loss: 0.1403 (0.2868)  time: 2.0595  data: 2.0462  max mem: 338
[19:53:12.937475] Test:  [790/800]  eta: 0:00:21  loss: 0.1385 (0.2883)  time: 2.0570  data: 2.0452  max mem: 338
[19:53:13.471678] Test:  [790/800]  eta: 0:00:21  loss: 0.1484 (0.2874)  time: 2.0723  data: 2.0604  max mem: 338
[19:53:21.161717] Test:  [799/800]  eta: 0:00:02  loss: 0.0718 (0.2863)  time: 1.9490  data: 1.9366  max mem: 338
[19:53:21.179708] Test:  [799/800]  eta: 0:00:02  loss: 0.0630 (0.2865)  time: 1.9522  data: 1.9399  max mem: 338
[19:53:21.310741] Test: Total time: 0:28:29 (2.1366 s / it)
[19:53:21.314803] Test: Total time: 0:28:29 (2.1366 s / it)
[19:53:21.319064] (25596, 14) (25596, 14)
[19:53:21.322826] (25596, 14) (25596, 14)
[19:53:21.376223] Test:  [799/800]  eta: 0:00:02  loss: 0.0680 (0.2864)  time: 1.9480  data: 1.9356  max mem: 338
[19:53:21.412175] [0.7256594562890096, 0.8764464453236336, 0.7992634442089778, 0.6564833897473669, 0.7759323541852019, 0.6717936735101377, 0.6842667035071491, 0.8312210061950267, 0.7106573261544189, 0.821344000560895, 0.887089670182104, 0.776412866655977, 0.7453916361914704, 0.9092895626885945]
[19:53:21.417246] [0.729943720498679, 0.8789184542736602, 0.7988178648520499, 0.6543893605210773, 0.7733928270541807, 0.6692412924285148, 0.6944000307963408, 0.8290480626969414, 0.711543743454245, 0.821026569108933, 0.8866313548476372, 0.7695320298199138, 0.7434508158365044, 0.9057020502675649]
[19:53:21.484934] Test: Total time: 0:28:29 (2.1368 s / it)
[19:53:21.492828] (25596, 14) (25596, 14)
[19:53:21.585169] [0.7283168120536817, 0.8738140968565866, 0.7993976194043376, 0.6541681300323208, 0.7735503248261107, 0.672139513391259, 0.6863801023978333, 0.8306417253227993, 0.7147892216197318, 0.8232516785765539, 0.8859130679855135, 0.7741956055873737, 0.742911437689722, 0.9083412341717338]
[19:53:22.091976] Test:  [799/800]  eta: 0:00:02  loss: 0.0667 (0.2857)  time: 1.6702  data: 1.6578  max mem: 338
[19:53:22.170636] Test: Total time: 0:28:30 (2.1376 s / it)
[19:53:22.178645] (25596, 14) (25596, 14)
[19:53:22.270270] [0.7279128897685042, 0.8756777004841804, 0.798775159694731, 0.6594004194003726, 0.7766410521786723, 0.664178179605887, 0.6899798204817973, 0.8319952352366669, 0.7129600418325948, 0.8232575066250232, 0.8882849791270401, 0.7782441536276494, 0.750534163436498, 0.9030564849169956]
[19:53:23.301366] Test:  [799/800]  eta: 0:00:02  loss: 0.0611 (0.2853)  time: 1.9414  data: 1.9272  max mem: 338
[19:53:23.301392] Test:  [799/800]  eta: 0:00:02  loss: 0.0692 (0.2857)  time: 1.9494  data: 1.9352  max mem: 338
[19:53:23.301978] Test:  [799/800]  eta: 0:00:02  loss: 0.0701 (0.2864)  time: 1.9494  data: 1.9352  max mem: 338
[19:53:23.531373] Test: Total time: 0:28:31 (2.1393 s / it)
[19:53:23.539292] (25596, 14) (25596, 14)
[19:53:23.539349] Test: Total time: 0:28:31 (2.1393 s / it)
[19:53:23.539587] Test: Total time: 0:28:31 (2.1394 s / it)
[19:53:23.547318] (25596, 14) (25596, 14)
[19:53:23.547441] (25596, 14) (25596, 14)
[19:53:23.631870] [0.7282010318398253, 0.8774142987379213, 0.7997498728688486, 0.6572279973848861, 0.7780196584470526, 0.6617146615882485, 0.6825212057630892, 0.8317530288884436, 0.7140972207018057, 0.8240318921329014, 0.8880305150751935, 0.776062342422843, 0.7509581739447744, 0.9052061207187331]
[19:53:23.639830] [0.7259405811706211, 0.8805537342764581, 0.7978577678128081, 0.6585008525924345, 0.7751753549559106, 0.6711130839610869, 0.6876854930886319, 0.8347919441496036, 0.7129576670868228, 0.821954477683066, 0.8886474270435881, 0.7794612808456072, 0.748002533401069, 0.897608780870247]
[19:53:23.641238] [0.725186352849197, 0.874818430943574, 0.7988473688352875, 0.6533073586489545, 0.7746214560062701, 0.6650317533493408, 0.6809513478975562, 0.8328393370011331, 0.7171121052607802, 0.8197415939712563, 0.8881505407090395, 0.7768069266110159, 0.7481055395822628, 0.8999799440255987]
[19:53:23.699254] Test:  [799/800]  eta: 0:00:02  loss: 0.0563 (0.2850)  time: 1.8748  data: 1.8611  max mem: 338
[19:53:23.783148] Test: Total time: 0:28:31 (2.1397 s / it)
[19:53:23.790831] (25596, 14) (25596, 14)
[19:53:23.882324] [0.7269347878143269, 0.8761544473830275, 0.8008971138532003, 0.6571856163225888, 0.7774062675357355, 0.6673459454373335, 0.6924893984675943, 0.8337122878546726, 0.7120472474785122, 0.826386095065111, 0.8872217562545042, 0.7760574543617266, 0.7446718093906933, 0.9105674473302763]
[19:53:24.163465] Test:  [799/800]  eta: 0:00:02  loss: 0.0691 (0.2856)  time: 1.9324  data: 1.9200  max mem: 338
[19:53:24.163522] Test:  [799/800]  eta: 0:00:02  loss: 0.0706 (0.2861)  time: 1.9300  data: 1.9176  max mem: 338
[19:53:24.349298] Test: Total time: 0:28:32 (2.1404 s / it)
[19:53:24.350774] Test: Total time: 0:28:32 (2.1404 s / it)
[19:53:24.357011] (25596, 14) (25596, 14)
[19:53:24.358677] (25596, 14) (25596, 14)
[19:53:24.439681] Test:  [799/800]  eta: 0:00:02  loss: 0.0805 (0.2857)  time: 1.9208  data: 1.9084  max mem: 338
[19:53:24.448687] [0.7288011347977819, 0.8770011308055043, 0.7969467996478266, 0.6554295229553367, 0.7747743359545619, 0.6677820105638972, 0.6823761463632076, 0.8305344126023555, 0.7150628965897841, 0.8230600321857264, 0.8880024549526749, 0.7791053203575867, 0.7480234637991234, 0.9120967153783741]
[19:53:24.451343] [0.7295263856103854, 0.8819803516965687, 0.7976939502141328, 0.6531168412046507, 0.7686104769566523, 0.6641780253966654, 0.6989915277683338, 0.8325941933803694, 0.7144558768181141, 0.8250971542252803, 0.8853021675669865, 0.7799055005306059, 0.7461106221984796, 0.8997353978831831]
[19:53:24.538926] Test: Total time: 0:28:32 (2.1406 s / it)
[19:53:24.546679] (25596, 14) (25596, 14)
[19:53:24.638398] [0.7300044974241584, 0.8781613420585389, 0.7987955638395244, 0.6571487732651626, 0.7770044329187832, 0.6646892675187908, 0.6907166301319889, 0.8302957735266325, 0.7156478022654611, 0.8241016972547919, 0.88575785051471, 0.7762423327106767, 0.7476610459066599, 0.8877116133208136]
[19:53:24.906925] Test:  [799/800]  eta: 0:00:02  loss: 0.0662 (0.2866)  time: 1.8264  data: 1.8141  max mem: 338
[19:53:24.986425] Test: Total time: 0:28:32 (2.1412 s / it)
[19:53:24.994360] (25596, 14) (25596, 14)
[19:53:25.087680] [0.7255363514136453, 0.878179134252804, 0.7972854110446754, 0.6583382770908282, 0.7733544691321159, 0.6691645604899679, 0.6904806207909119, 0.8316734688935065, 0.711016769991276, 0.8223154003989802, 0.8889306233166959, 0.7711908641680909, 0.7465933093782244, 0.902922474542587]
[19:53:25.088901] Loss 0.286
[19:53:25.088908] Loss 0.286
[19:53:25.088911] Loss 0.286
[19:53:25.088931] Loss 0.286
[19:53:25.088987] Average AUC of the network on the test set images: 0.7762
[19:53:25.088976] [19:53:25.088976] [19:53:25.088974] Loss 0.286Loss 0.286Loss 0.286


[19:53:25.088982] Loss 0.286
[19:53:25.088990] [19:53:25.088993] Loss 0.286Loss 0.286

[19:53:25.088998] Average AUC of the network on the test set images: 0.7771
[19:53:25.089004] Average AUC of the network on the test set images: 0.7769
[19:53:25.089009] Loss 0.286
[19:53:25.089050] Average AUC of the network on the test set images: 0.7760
[19:53:25.089076] Average AUC of the network on the test set images: 0.7768
[19:53:25.089080] Average AUC of the network on the test set images: 0.7778
[19:53:25.089119] Average AUC of the network on the test set images: 0.7772
[19:53:25.089135] Average AUC of the network on the test set images: 0.7754
[19:53:25.089048] Loss 0.286
[19:53:25.089058] [19:53:25.089058] Loss 0.286[19:53:25.089063] Loss 0.286
Loss 0.286

[19:53:25.089095] Average AUC of the network on the test set images: 0.7761
[19:53:25.089119] Average AUC of the network on the test set images: 0.7772
[19:53:25.089166] Average AUC of the network on the test set images: 0.7763
[19:53:25.089304] Average AUC of the network on the test set images: 0.7767
[19:53:25.089309] [19:53:25.089310] Average AUC of the network on the test set images: 0.7764Average AUC of the network on the test set images: 0.7745

[19:53:25.089331] Average AUC of the network on the test set images: 0.7763
[19:53:25.089295] Loss 0.286
[19:53:25.089788] Average AUC of the network on the test set images: 0.7765
