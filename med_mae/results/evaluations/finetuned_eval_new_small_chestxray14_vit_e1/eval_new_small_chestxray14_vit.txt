W0429 18:45:41.085838 23456247911936 torch/distributed/run.py:757] 
W0429 18:45:41.085838 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 18:45:41.085838 23456247911936 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0429 18:45:41.085838 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 18:45:41.124816 23456247911936 torch/distributed/run.py:757] 
W0429 18:45:41.124816 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 18:45:41.124816 23456247911936 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0429 18:45:41.124816 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 18:45:41.246654 23456247911936 torch/distributed/run.py:757] 
W0429 18:45:41.246654 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 18:45:41.246654 23456247911936 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0429 18:45:41.246654 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 18:45:41.252640 23456247911936 torch/distributed/run.py:757] 
W0429 18:45:41.252640 23456247911936 torch/distributed/run.py:757] *****************************************
W0429 18:45:41.252640 23456247911936 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0429 18:45:41.252640 23456247911936 torch/distributed/run.py:757] *****************************************
| distributed init (rank 8): env://, gpu 0
| distributed init (rank 12): env://, gpu 0
| distributed init (rank 4): env://, gpu 0
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 15): env://, gpu 3
| distributed init (rank 13): env://, gpu 1
| distributed init (rank 14): env://, gpu 2
| distributed init (rank 9): env://, gpu 1
| distributed init (rank 11): env://, gpu 3
| distributed init (rank 10): env://, gpu 2
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 7): env://, gpu 3
| distributed init (rank 6): env://, gpu 2
| distributed init (rank 5): env://, gpu 1
[18:45:50.107908] [18:45:50.107908] job dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_mae[18:45:50.107922] 

job dir: /mnt/home/mpaez/cvii-final/med_mae
[18:45:50.107928] job dir: /mnt/home/mpaez/cvii-final/med_mae
[18:45:50.108013] [18:45:50.108015] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=2,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=2,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=3,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=3,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
[18:45:50.108042] [18:45:50.108046] job dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_mae

[18:45:50.108051] [18:45:50.108047] [18:45:50.108049] job dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_mae


[18:45:50.108186] [18:45:50.108185] job dir: /mnt/home/mpaez/cvii-final/med_maejob dir: /mnt/home/mpaez/cvii-final/med_mae

[18:45:50.108016] 
Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=0,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=0,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
[18:45:50.108149] [18:45:50.108149] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=3,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=15,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
[18:45:50.108103] job dir: /mnt/home/mpaez/cvii-final/med_mae
[18:45:50.108224] job dir: /mnt/home/mpaez/cvii-final/med_mae[18:45:50.108230] 
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
[18:45:50.108023] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=1,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=1,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=2,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=14,
[18:45:50.108160] [18:45:50.108160] [18:45:50.108161] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=1,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=5,
repeated_aug=False,
job dir: /mnt/home/mpaez/cvii-final/med_mae
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=0,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
[18:45:50.108318] [18:45:50.108318] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=2,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=10,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
[18:45:50.109262] Using Directly-Resize Mode. (no RandomResizedCrop)

output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=4,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=2,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=3,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=11,
[18:45:50.109293] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.108162] job dir: /mnt/home/mpaez/cvii-final/med_mae
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=6,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)

[18:45:50.109322] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.108177] job dir: /mnt/home/mpaez/cvii-final/med_mae
world_size=16)


[18:45:50.108382] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=1,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=9,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
[18:45:50.109353] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.108314] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=0,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=12,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
[18:45:50.108229] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=3,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=7,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
[18:45:50.108390] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=0,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=8,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
world_size=16)
[18:45:50.109322] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.109620] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.108338] Namespace(accum_iter=1,
batch_size=32,
blr=0.00025,
checkpoint_type=None,
clip_grad=None,
cutmix=0.0,
cutmix_minmax=None,
dataset='chestxray14',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=50,
eval=True,
eval_interval=10,
finetune='/mnt/home/mpaez/cvii-final/med_mae/vit-s_CXR_0.3M_mae.pth',
fixed_lr=False,
global_pool=True,
gpu=1,
input_size=224,
layer_decay=0.55,
local_rank=-1,
log_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
loss_func=None,
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_small_patch16',
nb_classes=14,
num_workers=4,
optimizer='adamw',
output_dir='/mnt/home/mpaez/cvii-final/med_mae/results/evaluations/finetuned_eval_new_small_chestxray14_vit_e1/',
pin_mem=True,
rank=13,
repeated_aug=False,
resume='finetuned_new_small_chestxray14_50epoch.pth',
seed=0,
smoothing=0.1,
[18:45:50.109390] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.109678] Using Directly-Resize Mode. (no RandomResizedCrop)
start_epoch=0,
vit_dropout_rate=0.0,
warmup_epochs=5,
weight_decay=0.05,
world_size=16)
[18:45:50.109473] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.109769] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.109386] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.109491] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.109777] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.109409] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.109700] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.109866] Using Directly-Resize Mode. (no RandomResizedCrop)
[18:45:50.627966] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[18:45:50.627992] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862feee0>
[18:45:50.628644] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[18:45:50.629004] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef70>
[18:45:50.629128] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[18:45:50.629253] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef40>
[18:45:50.629411] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[18:45:50.629197] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[18:45:50.629345] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[18:45:50.629536] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef40>
[18:45:50.629415] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef70>
[18:45:50.629427] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef40>
[18:45:50.629749] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862feeb0>
[18:45:50.629722] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef40>
[18:45:50.630311] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[18:45:50.632468] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554862fef10>
[18:45:50.903788] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.903822] number of params (M): 21.67
[18:45:50.903837] base lr: 2.50e-04
[18:45:50.903844] actual lr: 5.00e-04
[18:45:50.903850] accumulate grad iterations: 1
[18:45:50.903856] effective batch size: 512
[18:45:50.911408] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.911440] number of params (M): 21.67
[18:45:50.911454] base lr: 2.50e-04
[18:45:50.911461] actual lr: 5.00e-04
[18:45:50.911468] accumulate grad iterations: 1
[18:45:50.911474] effective batch size: 512
[18:45:50.911922] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.911949] number of params (M): 21.67
[18:45:50.911961] base lr: 2.50e-04
[18:45:50.911969] actual lr: 5.00e-04
[18:45:50.911976] accumulate grad iterations: 1
[18:45:50.911983] effective batch size: 512
[18:45:50.914271] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.914297] number of params (M): 21.67
[18:45:50.914310] base lr: 2.50e-04
[18:45:50.914317] actual lr: 5.00e-04
[18:45:50.914324] accumulate grad iterations: 1
[18:45:50.914331] effective batch size: 512
[18:45:50.915581] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.915615] number of params (M): 21.67
[18:45:50.915628] base lr: 2.50e-04
[18:45:50.915635] actual lr: 5.00e-04
[18:45:50.915642] accumulate grad iterations: 1
[18:45:50.915648] effective batch size: 512
[18:45:50.915689] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.915717] number of params (M): 21.67
[18:45:50.915730] base lr: 2.50e-04
[18:45:50.915738] actual lr: 5.00e-04
[18:45:50.915745] accumulate grad iterations: 1
[18:45:50.915751] effective batch size: 512
[18:45:50.917172] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.917202] number of params (M): 21.67
[18:45:50.917215] base lr: 2.50e-04
[18:45:50.917222] actual lr: 5.00e-04
[18:45:50.917228] accumulate grad iterations: 1
[18:45:50.917234] effective batch size: 512
[18:45:50.917421] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.917448] number of params (M): 21.67
[18:45:50.917460] base lr: 2.50e-04
[18:45:50.917467] actual lr: 5.00e-04
[18:45:50.917473] accumulate grad iterations: 1
[18:45:50.917478] effective batch size: 512
[18:45:50.918161] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.918212] number of params (M): 21.67
[18:45:50.918227] base lr: 2.50e-04
[18:45:50.918234] actual lr: 5.00e-04
[18:45:50.918242] accumulate grad iterations: 1
[18:45:50.918248] effective batch size: 512
[18:45:50.919296] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.919324] number of params (M): 21.67
[18:45:50.919336] base lr: 2.50e-04
[18:45:50.919344] actual lr: 5.00e-04
[18:45:50.919350] accumulate grad iterations: 1
[18:45:50.919356] effective batch size: 512
[18:45:50.936214] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.936239] number of params (M): 21.67
[18:45:50.936252] base lr: 2.50e-04
[18:45:50.936258] actual lr: 5.00e-04
[18:45:50.936265] accumulate grad iterations: 1
[18:45:50.936272] effective batch size: 512
[18:45:50.936503] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.936535] number of params (M): 21.67
[18:45:50.936548] base lr: 2.50e-04
[18:45:50.936555] actual lr: 5.00e-04
[18:45:50.936562] accumulate grad iterations: 1
[18:45:50.936568] effective batch size: 512
[18:45:50.936554] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.936584] number of params (M): 21.67
[18:45:50.936599] base lr: 2.50e-04
[18:45:50.936606] actual lr: 5.00e-04
[18:45:50.936612] accumulate grad iterations: 1
[18:45:50.936618] effective batch size: 512
[18:45:50.937409] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.937440] number of params (M): 21.67
[18:45:50.937453] base lr: 2.50e-04
[18:45:50.937460] actual lr: 5.00e-04
[18:45:50.937467] accumulate grad iterations: 1
[18:45:50.937474] effective batch size: 512
[18:45:50.938141] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.938168] number of params (M): 21.67
[18:45:50.938181] base lr: 2.50e-04
[18:45:50.938189] actual lr: 5.00e-04
[18:45:50.938196] accumulate grad iterations: 1
[18:45:50.938203] effective batch size: 512
[18:45:50.944170] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1-11): 11 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MaskedAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=384, out_features=14, bias=True)
  (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
)
[18:45:50.944198] number of params (M): 21.67
[18:45:50.944212] base lr: 2.50e-04
[18:45:50.944219] actual lr: 5.00e-04
[18:45:50.944225] accumulate grad iterations: 1
[18:45:50.944231] effective batch size: 512
[18:45:50.955011] criterion = BCEWithLogitsLoss()
[18:45:50.955094] criterion = BCEWithLogitsLoss()
[18:45:50.955181] criterion = BCEWithLogitsLoss()
[18:45:50.955386] criterion = BCEWithLogitsLoss()
[18:45:50.955518] criterion = BCEWithLogitsLoss()
[18:45:50.955594] criterion = BCEWithLogitsLoss()
[18:45:50.955604] criterion = BCEWithLogitsLoss()
[18:45:50.955662] criterion = BCEWithLogitsLoss()
[18:45:50.955539] criterion = BCEWithLogitsLoss()
[18:45:50.955661] criterion = BCEWithLogitsLoss()
[18:45:50.955670] criterion = BCEWithLogitsLoss()
[18:45:50.955678] criterion = BCEWithLogitsLoss()
[18:45:50.955743] criterion = BCEWithLogitsLoss()
[18:45:50.955778] [18:45:50.955780] criterion = BCEWithLogitsLoss()
criterion = BCEWithLogitsLoss()
[18:45:50.956255] criterion = BCEWithLogitsLoss()
[18:45:51.433318] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.433704] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.433914] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.434146] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.438931] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.439034] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.439596] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.439645] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.444357] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.444612] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.444872] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.445256] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.474735] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.474839] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.474880] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:45:51.475010] Resume checkpoint finetuned_new_small_chestxray14_50epoch.pth
[18:46:05.624003] [18:46:05.624008] Test:  [  0/800]  eta: 3:08:38  loss: 0.3124 (0.3124)  time: 14.1475  data: 13.5407  max mem: 337
Test:  [  0/800]  eta: 3:08:37  loss: 0.3131 (0.3131)  time: 14.1474  data: 13.5405  max mem: 337
[18:46:05.745543] Test:  [  0/800]  eta: 3:10:10  loss: 0.3176 (0.3176)  time: 14.2635  data: 13.7134  max mem: 337
[18:46:05.810730] Test:  [  0/800]  eta: 3:11:34  loss: 0.3027 (0.3027)  time: 14.3682  data: 13.7945  max mem: 337
[18:46:05.817619] Test:  [  0/800]  eta: 3:11:12  loss: 0.3150 (0.3150)  time: 14.3410  data: 13.7705  max mem: 337
[18:46:05.867788] Test:  [  0/800]  eta: 3:12:26  loss: 0.3006 (0.3006)  time: 14.4327  data: 13.8041  max mem: 337
[18:46:05.885647] Test:  [  0/800]  eta: 3:12:40  loss: 0.3044 (0.3044)  time: 14.4500  data: 13.8564  max mem: 337
[18:46:05.902321] Test:  [  0/800]  eta: 3:12:53  loss: 0.3010 (0.3010)  time: 14.4664  data: 13.8503  max mem: 337
[18:46:05.999628] Test:  [  0/800]  eta: 3:14:01  loss: 0.3189 (0.3189)  time: 14.5524  data: 13.9191  max mem: 337
[18:46:05.999689] Test:  [  0/800]  eta: 3:14:02  loss: 0.3038 (0.3038)  time: 14.5530  data: 13.9144  max mem: 337
[18:46:06.001945] Test:  [  0/800]  eta: 3:14:04  loss: 0.3181 (0.3181)  time: 14.5558  data: 13.9341  max mem: 337
[18:46:06.039508] Test:  [  0/800]  eta: 3:14:34  loss: 0.3128 (0.3128)  time: 14.5930  data: 13.9781  max mem: 337
[18:46:06.314485] Test:  [  0/800]  eta: 3:18:13  loss: 0.3041 (0.3041)  time: 14.8665  data: 14.3071  max mem: 337
[18:46:06.331285] Test:  [  0/800]  eta: 3:18:31  loss: 0.2983 (0.2983)  time: 14.8899  data: 14.2909  max mem: 337
[18:46:06.358531] Test:  [  0/800]  eta: 3:18:53  loss: 0.3042 (0.3042)  time: 14.9172  data: 14.3251  max mem: 337
[18:46:06.592866] Test:  [  0/800]  eta: 3:22:01  loss: 0.3120 (0.3120)  time: 15.1520  data: 14.5683  max mem: 337
[18:46:22.528097] Test:  [ 10/800]  eta: 0:37:09  loss: 0.2290 (0.2460)  time: 2.8223  data: 2.7608  max mem: 337
[18:46:22.592121] Test:  [ 10/800]  eta: 0:37:14  loss: 0.2213 (0.2466)  time: 2.8287  data: 2.7620  max mem: 337
[18:46:22.652711] Test:  [ 10/800]  eta: 0:37:18  loss: 0.2238 (0.2453)  time: 2.8342  data: 2.7708  max mem: 337
[18:46:22.701480] Test:  [ 10/800]  eta: 0:37:22  loss: 0.2278 (0.2453)  time: 2.8386  data: 2.7719  max mem: 337
[18:46:22.858467] Test:  [ 10/800]  eta: 0:37:36  loss: 0.2234 (0.2455)  time: 2.8566  data: 2.7919  max mem: 337
[18:46:22.935567] Test:  [ 10/800]  eta: 0:37:41  loss: 0.2296 (0.2475)  time: 2.8626  data: 2.7943  max mem: 337
[18:46:22.966482] Test:  [ 10/800]  eta: 0:37:43  loss: 0.2282 (0.2449)  time: 2.8654  data: 2.7966  max mem: 337
[18:46:22.998758] Test:  [ 10/800]  eta: 0:37:46  loss: 0.2293 (0.2435)  time: 2.8693  data: 2.8025  max mem: 337
[18:46:23.013358] Test:  [ 10/800]  eta: 0:37:47  loss: 0.2289 (0.2437)  time: 2.8700  data: 2.8071  max mem: 337
[18:46:23.015095] Test:  [ 10/800]  eta: 0:37:47  loss: 0.2249 (0.2439)  time: 2.8709  data: 2.8030  max mem: 337
[18:46:23.017352] Test:  [ 10/800]  eta: 0:37:47  loss: 0.2192 (0.2430)  time: 2.8700  data: 2.7995  max mem: 337
[18:46:23.089322] Test:  [ 10/800]  eta: 0:37:52  loss: 0.2273 (0.2468)  time: 2.8765  data: 2.8066  max mem: 337
[18:46:23.370066] Test:  [ 10/800]  eta: 0:38:13  loss: 0.2252 (0.2449)  time: 2.9026  data: 2.8380  max mem: 337
[18:46:23.381863] Test:  [ 10/800]  eta: 0:38:13  loss: 0.2220 (0.2456)  time: 2.9030  data: 2.8415  max mem: 337
[18:46:23.418943] Test:  [ 10/800]  eta: 0:38:16  loss: 0.2198 (0.2426)  time: 2.9070  data: 2.8418  max mem: 337
[18:46:23.630221] Test:  [ 10/800]  eta: 0:38:31  loss: 0.2210 (0.2452)  time: 2.9263  data: 2.8624  max mem: 337
[18:46:47.301366] Test:  [ 20/800]  eta: 0:34:33  loss: 0.1932 (0.2195)  time: 2.0838  data: 2.0707  max mem: 337
[18:46:47.337641] Test:  [ 20/800]  eta: 0:34:34  loss: 0.1898 (0.2193)  time: 2.0796  data: 2.0664  max mem: 337
[18:46:47.596777] Test:  [ 20/800]  eta: 0:34:44  loss: 0.1882 (0.2182)  time: 2.0889  data: 2.0757  max mem: 337
[18:46:47.818141] Test:  [ 20/800]  eta: 0:34:52  loss: 0.1931 (0.2191)  time: 2.1097  data: 2.0966  max mem: 337
[18:46:48.411347] Test:  [ 20/800]  eta: 0:35:16  loss: 0.1915 (0.2162)  time: 2.1254  data: 2.1136  max mem: 337
[18:46:48.419165] Test:  [ 20/800]  eta: 0:35:16  loss: 0.1902 (0.2182)  time: 2.1266  data: 2.1149  max mem: 337
[18:46:48.424133] Test:  [ 20/800]  eta: 0:35:16  loss: 0.1949 (0.2179)  time: 2.1278  data: 2.1159  max mem: 337
[18:46:48.435066] Test:  [ 20/800]  eta: 0:35:16  loss: 0.1912 (0.2180)  time: 2.1312  data: 2.1193  max mem: 337
[18:46:48.443976] Test:  [ 20/800]  eta: 0:35:17  loss: 0.1926 (0.2175)  time: 2.1222  data: 2.1085  max mem: 337
[18:46:48.470979] Test:  [ 20/800]  eta: 0:35:18  loss: 0.1925 (0.2192)  time: 2.1234  data: 2.1098  max mem: 337
[18:46:48.513760] Test:  [ 20/800]  eta: 0:35:19  loss: 0.1934 (0.2199)  time: 2.1237  data: 2.1106  max mem: 337
[18:46:48.754134] Test:  [ 20/800]  eta: 0:35:28  loss: 0.1884 (0.2189)  time: 2.1377  data: 2.1241  max mem: 337
[18:46:48.890247] Test:  [ 20/800]  eta: 0:35:33  loss: 0.1929 (0.2188)  time: 2.1265  data: 2.1147  max mem: 337
[18:46:48.893503] Test:  [ 20/800]  eta: 0:35:33  loss: 0.1914 (0.2192)  time: 2.1289  data: 2.1171  max mem: 337
[18:46:48.955123] Test:  [ 20/800]  eta: 0:35:36  loss: 0.1877 (0.2161)  time: 2.1311  data: 2.1195  max mem: 337
[18:46:49.210986] Test:  [ 20/800]  eta: 0:35:45  loss: 0.1947 (0.2182)  time: 2.1309  data: 2.1192  max mem: 337
[18:47:04.069421] Test:  [ 30/800]  eta: 0:30:03  loss: 0.1932 (0.2148)  time: 2.0738  data: 2.0602  max mem: 337
[18:47:04.100448] Test:  [ 30/800]  eta: 0:30:03  loss: 0.1898 (0.2153)  time: 2.0786  data: 2.0649  max mem: 337
[18:47:04.266409] Test:  [ 30/800]  eta: 0:30:07  loss: 0.1898 (0.2142)  time: 2.0806  data: 2.0670  max mem: 337
[18:47:04.512148] Test:  [ 30/800]  eta: 0:30:14  loss: 0.1931 (0.2150)  time: 2.0905  data: 2.0769  max mem: 337
[18:47:05.577386] Test:  [ 30/800]  eta: 0:30:41  loss: 0.1907 (0.2134)  time: 2.1280  data: 2.1143  max mem: 337
[18:47:05.593503] Test:  [ 30/800]  eta: 0:30:41  loss: 0.1924 (0.2153)  time: 2.1313  data: 2.1177  max mem: 337
[18:47:05.593955] Test:  [ 30/800]  eta: 0:30:41  loss: 0.1915 (0.2152)  time: 2.1329  data: 2.1198  max mem: 337
[18:47:05.711167] Test:  [ 30/800]  eta: 0:30:44  loss: 0.1902 (0.2140)  time: 2.1348  data: 2.1229  max mem: 337
[18:47:05.747639] Test:  [ 30/800]  eta: 0:30:45  loss: 0.1902 (0.2144)  time: 2.1444  data: 2.1328  max mem: 337
[18:47:05.752307] Test:  [ 30/800]  eta: 0:30:45  loss: 0.1899 (0.2142)  time: 2.1368  data: 2.1250  max mem: 337
[18:47:05.765813] Test:  [ 30/800]  eta: 0:30:46  loss: 0.1898 (0.2123)  time: 2.1383  data: 2.1265  max mem: 337
[18:47:05.871688] Test:  [ 30/800]  eta: 0:30:48  loss: 0.1891 (0.2149)  time: 2.1391  data: 2.1255  max mem: 337
[18:47:06.095191] Test:  [ 30/800]  eta: 0:30:54  loss: 0.1914 (0.2148)  time: 2.1356  data: 2.1238  max mem: 337
[18:47:06.099355] Test:  [ 30/800]  eta: 0:30:54  loss: 0.1958 (0.2145)  time: 2.1364  data: 2.1246  max mem: 337
[18:47:06.325109] Test:  [ 30/800]  eta: 0:30:59  loss: 0.1925 (0.2141)  time: 2.1347  data: 2.1230  max mem: 337
[18:47:06.391155] Test:  [ 30/800]  eta: 0:31:01  loss: 0.1877 (0.2122)  time: 2.1486  data: 2.1369  max mem: 337
[18:47:30.065271] Test:  [ 40/800]  eta: 0:30:27  loss: 0.2111 (0.2170)  time: 2.1381  data: 2.1245  max mem: 337
[18:47:30.065322] Test:  [ 40/800]  eta: 0:30:27  loss: 0.2085 (0.2177)  time: 2.1363  data: 2.1227  max mem: 337
[18:47:30.327168] Test:  [ 40/800]  eta: 0:30:32  loss: 0.2071 (0.2165)  time: 2.1365  data: 2.1228  max mem: 337
[18:47:30.748915] Test:  [ 40/800]  eta: 0:30:40  loss: 0.2047 (0.2177)  time: 2.1465  data: 2.1329  max mem: 337
[18:47:32.394936] Test:  [ 40/800]  eta: 0:31:11  loss: 0.2104 (0.2167)  time: 2.1987  data: 2.1871  max mem: 337
[18:47:32.412363] Test:  [ 40/800]  eta: 0:31:11  loss: 0.2085 (0.2170)  time: 2.1988  data: 2.1869  max mem: 337
[18:47:32.440270] Test:  [ 40/800]  eta: 0:31:12  loss: 0.2095 (0.2166)  time: 2.2008  data: 2.1889  max mem: 337
[18:47:32.455842] Test:  [ 40/800]  eta: 0:31:12  loss: 0.2110 (0.2177)  time: 2.1992  data: 2.1856  max mem: 337
[18:47:32.603415] Test:  [ 40/800]  eta: 0:31:15  loss: 0.2079 (0.2170)  time: 2.2079  data: 2.1942  max mem: 337
[18:47:32.629073] Test:  [ 40/800]  eta: 0:31:15  loss: 0.2035 (0.2177)  time: 2.2057  data: 2.1921  max mem: 337
[18:47:32.692742] Test:  [ 40/800]  eta: 0:31:16  loss: 0.2057 (0.2152)  time: 2.2140  data: 2.2022  max mem: 337
[18:47:32.760289] Test:  [ 40/800]  eta: 0:31:17  loss: 0.2111 (0.2174)  time: 2.2003  data: 2.1867  max mem: 337
[18:47:32.841148] Test:  [ 40/800]  eta: 0:31:19  loss: 0.2046 (0.2169)  time: 2.1975  data: 2.1857  max mem: 337
[18:47:32.985257] Test:  [ 40/800]  eta: 0:31:22  loss: 0.2077 (0.2170)  time: 2.2045  data: 2.1928  max mem: 337
[18:47:33.154957] Test:  [ 40/800]  eta: 0:31:25  loss: 0.2089 (0.2154)  time: 2.2099  data: 2.1981  max mem: 337
[18:47:33.302277] Test:  [ 40/800]  eta: 0:31:28  loss: 0.2104 (0.2166)  time: 2.2045  data: 2.1927  max mem: 337
[18:47:47.105783] Test:  [ 50/800]  eta: 0:28:20  loss: 0.2429 (0.2252)  time: 2.1518  data: 2.1381  max mem: 337
[18:47:47.107973] Test:  [ 50/800]  eta: 0:28:20  loss: 0.2391 (0.2257)  time: 2.1503  data: 2.1367  max mem: 337
[18:47:47.177753] Test:  [ 50/800]  eta: 0:28:21  loss: 0.2402 (0.2246)  time: 2.1455  data: 2.1319  max mem: 337
[18:47:47.709518] Test:  [ 50/800]  eta: 0:28:29  loss: 0.2389 (0.2258)  time: 2.1598  data: 2.1462  max mem: 337
[18:47:49.898425] Test:  [ 50/800]  eta: 0:29:01  loss: 0.2413 (0.2254)  time: 2.2160  data: 2.2023  max mem: 337
[18:47:49.901736] Test:  [ 50/800]  eta: 0:29:01  loss: 0.2394 (0.2254)  time: 2.2154  data: 2.2022  max mem: 337
[18:47:49.973425] Test:  [ 50/800]  eta: 0:29:03  loss: 0.2435 (0.2258)  time: 2.2189  data: 2.2058  max mem: 337
[18:47:50.029382] Test:  [ 50/800]  eta: 0:29:03  loss: 0.2399 (0.2235)  time: 2.2131  data: 2.2014  max mem: 337
[18:47:50.034735] Test:  [ 50/800]  eta: 0:29:03  loss: 0.2396 (0.2254)  time: 2.2161  data: 2.2044  max mem: 337
[18:47:50.081283] Test:  [ 50/800]  eta: 0:29:04  loss: 0.2397 (0.2251)  time: 2.2164  data: 2.2046  max mem: 337
[18:47:50.152456] Test:  [ 50/800]  eta: 0:29:05  loss: 0.2346 (0.2256)  time: 2.2140  data: 2.2004  max mem: 337
[18:47:50.331152] Test:  [ 50/800]  eta: 0:29:08  loss: 0.2368 (0.2248)  time: 2.2291  data: 2.2174  max mem: 337
[18:47:50.381729] Test:  [ 50/800]  eta: 0:29:09  loss: 0.2428 (0.2254)  time: 2.2028  data: 2.1911  max mem: 337
[18:47:50.407682] Test:  [ 50/800]  eta: 0:29:09  loss: 0.2401 (0.2251)  time: 2.2154  data: 2.2037  max mem: 337
[18:47:50.510984] Test:  [ 50/800]  eta: 0:29:10  loss: 0.2375 (0.2254)  time: 2.2207  data: 2.2091  max mem: 337
[18:47:50.888066] Test:  [ 50/800]  eta: 0:29:16  loss: 0.2362 (0.2240)  time: 2.2248  data: 2.2130  max mem: 337
[18:48:11.490782] Test:  [ 60/800]  eta: 0:28:18  loss: 0.2015 (0.2159)  time: 2.0712  data: 2.0576  max mem: 337
[18:48:11.610313] Test:  [ 60/800]  eta: 0:28:19  loss: 0.1991 (0.2163)  time: 2.0772  data: 2.0636  max mem: 337
[18:48:12.002313] Test:  [ 60/800]  eta: 0:28:24  loss: 0.2036 (0.2152)  time: 2.0837  data: 2.0700  max mem: 337
[18:48:12.280055] Test:  [ 60/800]  eta: 0:28:28  loss: 0.2051 (0.2166)  time: 2.0765  data: 2.0629  max mem: 337
[18:48:15.024719] Test:  [ 60/800]  eta: 0:29:01  loss: 0.2062 (0.2161)  time: 2.1306  data: 2.1189  max mem: 337
[18:48:15.025764] Test:  [ 60/800]  eta: 0:29:01  loss: 0.2071 (0.2159)  time: 2.1292  data: 2.1175  max mem: 337
[18:48:15.101985] Test:  [ 60/800]  eta: 0:29:02  loss: 0.2054 (0.2162)  time: 2.1249  data: 2.1112  max mem: 337
[18:48:15.116425] Test:  [ 60/800]  eta: 0:29:02  loss: 0.2023 (0.2159)  time: 2.1330  data: 2.1199  max mem: 337
[18:48:15.128991] Test:  [ 60/800]  eta: 0:29:03  loss: 0.2030 (0.2155)  time: 2.1367  data: 2.1249  max mem: 337
[18:48:15.261757] Test:  [ 60/800]  eta: 0:29:04  loss: 0.2032 (0.2166)  time: 2.1316  data: 2.1185  max mem: 337
[18:48:15.384269] Test:  [ 60/800]  eta: 0:29:06  loss: 0.2031 (0.2160)  time: 2.1312  data: 2.1176  max mem: 337
[18:48:15.555121] Test:  [ 60/800]  eta: 0:29:08  loss: 0.2070 (0.2146)  time: 2.1431  data: 2.1314  max mem: 337
[18:48:15.607807] Test:  [ 60/800]  eta: 0:29:08  loss: 0.2111 (0.2159)  time: 2.1383  data: 2.1266  max mem: 337
[18:48:15.609669] Test:  [ 60/800]  eta: 0:29:08  loss: 0.2059 (0.2160)  time: 2.1312  data: 2.1195  max mem: 337
[18:48:15.720007] Test:  [ 60/800]  eta: 0:29:10  loss: 0.2049 (0.2151)  time: 2.1282  data: 2.1164  max mem: 337
[18:48:16.007645] Test:  [ 60/800]  eta: 0:29:13  loss: 0.2044 (0.2161)  time: 2.1352  data: 2.1235  max mem: 337
[18:48:28.957307] Test:  [ 70/800]  eta: 0:26:59  loss: 0.1885 (0.2168)  time: 2.0889  data: 2.0753  max mem: 337
[18:48:29.008212] Test:  [ 70/800]  eta: 0:26:59  loss: 0.1884 (0.2175)  time: 2.0950  data: 2.0813  max mem: 337
[18:48:29.028251] Test:  [ 70/800]  eta: 0:26:59  loss: 0.1881 (0.2174)  time: 2.0961  data: 2.0825  max mem: 337
[18:48:29.609037] Test:  [ 70/800]  eta: 0:27:05  loss: 0.1854 (0.2180)  time: 2.0949  data: 2.0813  max mem: 337
[18:48:32.933377] Test:  [ 70/800]  eta: 0:27:40  loss: 0.1870 (0.2164)  time: 2.1452  data: 2.1334  max mem: 337
[18:48:32.951473] Test:  [ 70/800]  eta: 0:27:40  loss: 0.1882 (0.2174)  time: 2.1458  data: 2.1339  max mem: 337
[18:48:32.959066] Test:  [ 70/800]  eta: 0:27:40  loss: 0.1868 (0.2174)  time: 2.1438  data: 2.1320  max mem: 337
[18:48:32.979170] Test:  [ 70/800]  eta: 0:27:40  loss: 0.1847 (0.2180)  time: 2.1502  data: 2.1366  max mem: 337
[18:48:32.988754] Test:  [ 70/800]  eta: 0:27:40  loss: 0.1868 (0.2173)  time: 2.1543  data: 2.1407  max mem: 337
[18:48:33.028677] Test:  [ 70/800]  eta: 0:27:41  loss: 0.1896 (0.2178)  time: 2.1565  data: 2.1428  max mem: 337
[18:48:33.044400] Test:  [ 70/800]  eta: 0:27:41  loss: 0.1872 (0.2177)  time: 2.1446  data: 2.1310  max mem: 337
[18:48:33.195831] Test:  [ 70/800]  eta: 0:27:43  loss: 0.1890 (0.2171)  time: 2.1432  data: 2.1314  max mem: 337
[18:48:33.567279] Test:  [ 70/800]  eta: 0:27:46  loss: 0.1930 (0.2176)  time: 2.1528  data: 2.1411  max mem: 337
[18:48:33.589637] Test:  [ 70/800]  eta: 0:27:47  loss: 0.1891 (0.2174)  time: 2.1604  data: 2.1486  max mem: 337
[18:48:33.602986] Test:  [ 70/800]  eta: 0:27:47  loss: 0.1854 (0.2175)  time: 2.1597  data: 2.1479  max mem: 337
[18:48:33.856187] Test:  [ 70/800]  eta: 0:27:49  loss: 0.1891 (0.2165)  time: 2.1484  data: 2.1365  max mem: 337
[18:48:54.212077] Test:  [ 80/800]  eta: 0:27:04  loss: 0.1949 (0.2147)  time: 2.1360  data: 2.1224  max mem: 337
[18:48:54.216781] Test:  [ 80/800]  eta: 0:27:04  loss: 0.1938 (0.2147)  time: 2.1303  data: 2.1167  max mem: 337
[18:48:54.473119] Test:  [ 80/800]  eta: 0:27:06  loss: 0.1971 (0.2143)  time: 2.1235  data: 2.1098  max mem: 337
[18:48:54.854095] Test:  [ 80/800]  eta: 0:27:09  loss: 0.1920 (0.2152)  time: 2.1287  data: 2.1151  max mem: 337
[18:48:59.128725] Test:  [ 80/800]  eta: 0:27:48  loss: 0.1970 (0.2148)  time: 2.1872  data: 2.1740  max mem: 337
[18:48:59.160415] Test:  [ 80/800]  eta: 0:27:48  loss: 0.1970 (0.2150)  time: 2.2029  data: 2.1896  max mem: 337
[18:48:59.188239] Test:  [ 80/800]  eta: 0:27:48  loss: 0.1975 (0.2148)  time: 2.2035  data: 2.1904  max mem: 337
[18:48:59.300372] Test:  [ 80/800]  eta: 0:27:49  loss: 0.1975 (0.2143)  time: 2.2085  data: 2.1967  max mem: 337
[18:48:59.314181] Test:  [ 80/800]  eta: 0:27:49  loss: 0.2009 (0.2135)  time: 2.1879  data: 2.1763  max mem: 337
[18:48:59.316321] Test:  [ 80/800]  eta: 0:27:49  loss: 0.1955 (0.2148)  time: 2.2027  data: 2.1891  max mem: 337
[18:48:59.344791] Test:  [ 80/800]  eta: 0:27:50  loss: 0.1971 (0.2146)  time: 2.2159  data: 2.2041  max mem: 337
[18:48:59.367564] Test:  [ 80/800]  eta: 0:27:50  loss: 0.1953 (0.2148)  time: 2.2171  data: 2.2053  max mem: 337
[18:48:59.987065] Test:  [ 80/800]  eta: 0:27:55  loss: 0.1946 (0.2144)  time: 2.1989  data: 2.1872  max mem: 337
[18:49:00.038905] Test:  [ 80/800]  eta: 0:27:56  loss: 0.1931 (0.2137)  time: 2.2159  data: 2.2041  max mem: 337
[18:49:00.042342] Test:  [ 80/800]  eta: 0:27:56  loss: 0.1963 (0.2147)  time: 2.2216  data: 2.2100  max mem: 337
[18:49:00.043980] Test:  [ 80/800]  eta: 0:27:56  loss: 0.1961 (0.2147)  time: 2.2218  data: 2.2100  max mem: 337
[18:49:11.001437] Test:  [ 90/800]  eta: 0:25:56  loss: 0.1841 (0.2121)  time: 2.0986  data: 2.0850  max mem: 337
[18:49:11.006153] Test:  [ 90/800]  eta: 0:25:56  loss: 0.1841 (0.2122)  time: 2.0999  data: 2.0862  max mem: 337
[18:49:11.048779] Test:  [ 90/800]  eta: 0:25:57  loss: 0.1880 (0.2121)  time: 2.1045  data: 2.0909  max mem: 337
[18:49:11.453081] Test:  [ 90/800]  eta: 0:26:00  loss: 0.1882 (0.2127)  time: 2.0922  data: 2.0785  max mem: 337
[18:49:16.192836] Test:  [ 90/800]  eta: 0:26:37  loss: 0.1873 (0.2125)  time: 2.1574  data: 2.1442  max mem: 337
[18:49:16.234000] Test:  [ 90/800]  eta: 0:26:37  loss: 0.1892 (0.2125)  time: 2.1622  data: 2.1491  max mem: 337
[18:49:16.243959] Test:  [ 90/800]  eta: 0:26:37  loss: 0.1853 (0.2125)  time: 2.1607  data: 2.1474  max mem: 337
[18:49:16.286062] Test:  [ 90/800]  eta: 0:26:38  loss: 0.1872 (0.2123)  time: 2.1663  data: 2.1545  max mem: 337
[18:49:16.300798] Test:  [ 90/800]  eta: 0:26:38  loss: 0.1909 (0.2126)  time: 2.1674  data: 2.1556  max mem: 337
[18:49:16.317026] Test:  [ 90/800]  eta: 0:26:38  loss: 0.1889 (0.2119)  time: 2.1560  data: 2.1442  max mem: 337
[18:49:16.375582] Test:  [ 90/800]  eta: 0:26:38  loss: 0.1889 (0.2114)  time: 2.1721  data: 2.1604  max mem: 337
[18:49:16.492824] Test:  [ 90/800]  eta: 0:26:39  loss: 0.1896 (0.2125)  time: 2.1756  data: 2.1620  max mem: 337
[18:49:17.003456] Test:  [ 90/800]  eta: 0:26:43  loss: 0.1876 (0.2123)  time: 2.1700  data: 2.1582  max mem: 337
[18:49:17.012199] Test:  [ 90/800]  eta: 0:26:43  loss: 0.1877 (0.2113)  time: 2.1578  data: 2.1459  max mem: 337
[18:49:17.012336] Test:  [ 90/800]  eta: 0:26:43  loss: 0.1910 (0.2122)  time: 2.1711  data: 2.1592  max mem: 337
[18:49:17.179519] Test:  [ 90/800]  eta: 0:26:45  loss: 0.1883 (0.2122)  time: 2.1806  data: 2.1688  max mem: 337
[18:49:35.151135] Test:  [100/800]  eta: 0:25:50  loss: 0.1863 (0.2117)  time: 2.0467  data: 2.0330  max mem: 337
[18:49:35.182467] Test:  [100/800]  eta: 0:25:50  loss: 0.1844 (0.2114)  time: 2.0485  data: 2.0349  max mem: 337
[18:49:35.345133] Test:  [100/800]  eta: 0:25:51  loss: 0.1880 (0.2115)  time: 2.0436  data: 2.0299  max mem: 337
[18:49:35.668139] Test:  [100/800]  eta: 0:25:53  loss: 0.1882 (0.2123)  time: 2.0407  data: 2.0270  max mem: 337
[18:49:41.123818] Test:  [100/800]  eta: 0:26:31  loss: 0.1892 (0.2119)  time: 2.0967  data: 2.0832  max mem: 337
[18:49:41.124849] Test:  [100/800]  eta: 0:26:31  loss: 0.1860 (0.2113)  time: 2.0912  data: 2.0794  max mem: 337
[18:49:41.137596] Test:  [100/800]  eta: 0:26:31  loss: 0.1880 (0.2118)  time: 2.0896  data: 2.0778  max mem: 337
[18:49:41.167690] Test:  [100/800]  eta: 0:26:32  loss: 0.1883 (0.2121)  time: 2.1003  data: 2.0866  max mem: 337
[18:49:41.241124] Test:  [100/800]  eta: 0:26:32  loss: 0.1876 (0.2117)  time: 2.0936  data: 2.0817  max mem: 337
[18:49:41.265375] Test:  [100/800]  eta: 0:26:32  loss: 0.1873 (0.2118)  time: 2.1068  data: 2.0932  max mem: 337
[18:49:41.305289] Test:  [100/800]  eta: 0:26:33  loss: 0.1889 (0.2110)  time: 2.0995  data: 2.0877  max mem: 337
[18:49:41.330773] Test:  [100/800]  eta: 0:26:33  loss: 0.1896 (0.2119)  time: 2.1007  data: 2.0872  max mem: 337
[18:49:41.750504] Test:  [100/800]  eta: 0:26:36  loss: 0.1895 (0.2121)  time: 2.0853  data: 2.0736  max mem: 337
[18:49:41.777354] Test:  [100/800]  eta: 0:26:36  loss: 0.1910 (0.2118)  time: 2.0895  data: 2.0778  max mem: 337
[18:49:41.807352] Test:  [100/800]  eta: 0:26:36  loss: 0.1881 (0.2110)  time: 2.0884  data: 2.0765  max mem: 337
[18:49:41.975016] Test:  [100/800]  eta: 0:26:37  loss: 0.1887 (0.2119)  time: 2.0966  data: 2.0850  max mem: 337
[18:49:52.804627] Test:  [110/800]  eta: 0:25:00  loss: 0.1953 (0.2113)  time: 2.0899  data: 2.0765  max mem: 337
[18:49:52.823743] Test:  [110/800]  eta: 0:25:00  loss: 0.1925 (0.2110)  time: 2.0911  data: 2.0777  max mem: 337
[18:49:53.017412] Test:  [110/800]  eta: 0:25:01  loss: 0.1942 (0.2111)  time: 2.0984  data: 2.0852  max mem: 337
[18:49:53.570895] Test:  [110/800]  eta: 0:25:04  loss: 0.1967 (0.2117)  time: 2.1058  data: 2.0925  max mem: 337
[18:49:59.470748] Test:  [110/800]  eta: 0:25:41  loss: 0.1952 (0.2116)  time: 2.1639  data: 2.1505  max mem: 337
[18:49:59.548885] Test:  [110/800]  eta: 0:25:42  loss: 0.1942 (0.2113)  time: 2.1624  data: 2.1507  max mem: 337
[18:49:59.561192] Test:  [110/800]  eta: 0:25:42  loss: 0.1907 (0.2114)  time: 2.1637  data: 2.1520  max mem: 337
[18:49:59.561330] Test:  [110/800]  eta: 0:25:42  loss: 0.1951 (0.2116)  time: 2.1534  data: 2.1400  max mem: 337
[18:49:59.561689] Test:  [110/800]  eta: 0:25:42  loss: 0.1960 (0.2116)  time: 2.1658  data: 2.1523  max mem: 337
[18:49:59.596074] Test:  [110/800]  eta: 0:25:42  loss: 0.1895 (0.2111)  time: 2.1639  data: 2.1522  max mem: 337
[18:49:59.664143] Test:  [110/800]  eta: 0:25:43  loss: 0.1976 (0.2105)  time: 2.1644  data: 2.1526  max mem: 337
[18:49:59.702589] Test:  [110/800]  eta: 0:25:43  loss: 0.1973 (0.2115)  time: 2.1734  data: 2.1600  max mem: 337
[18:49:59.801608] Test:  [110/800]  eta: 0:25:43  loss: 0.1951 (0.2116)  time: 2.1399  data: 2.1282  max mem: 337
[18:49:59.903825] Test:  [110/800]  eta: 0:25:44  loss: 0.1951 (0.2113)  time: 2.1445  data: 2.1329  max mem: 337
[18:50:00.040717] Test:  [110/800]  eta: 0:25:45  loss: 0.1971 (0.2114)  time: 2.1430  data: 2.1314  max mem: 337
[18:50:00.237591] Test:  [110/800]  eta: 0:25:46  loss: 0.1933 (0.2105)  time: 2.1612  data: 2.1494  max mem: 337
[18:50:17.905649] Test:  [120/800]  eta: 0:24:57  loss: 0.1890 (0.2071)  time: 2.1361  data: 2.1227  max mem: 337
[18:50:17.997518] Test:  [120/800]  eta: 0:24:57  loss: 0.1782 (0.2071)  time: 2.1423  data: 2.1289  max mem: 337
[18:50:18.151493] Test:  [120/800]  eta: 0:24:58  loss: 0.1896 (0.2072)  time: 2.1403  data: 2.1271  max mem: 337
[18:50:18.437361] Test:  [120/800]  eta: 0:25:00  loss: 0.1795 (0.2076)  time: 2.1384  data: 2.1250  max mem: 337
[18:50:25.121242] Test:  [120/800]  eta: 0:25:37  loss: 0.1832 (0.2073)  time: 2.1940  data: 2.1822  max mem: 337
[18:50:25.157518] Test:  [120/800]  eta: 0:25:38  loss: 0.1843 (0.2070)  time: 2.2016  data: 2.1899  max mem: 337
[18:50:25.157559] Test:  [120/800]  eta: 0:25:38  loss: 0.1883 (0.2074)  time: 2.2010  data: 2.1892  max mem: 337
[18:50:25.176298] Test:  [120/800]  eta: 0:25:38  loss: 0.1819 (0.2077)  time: 2.2004  data: 2.1868  max mem: 337
[18:50:25.187833] Test:  [120/800]  eta: 0:25:38  loss: 0.1817 (0.2076)  time: 2.1961  data: 2.1827  max mem: 337
[18:50:25.193217] Test:  [120/800]  eta: 0:25:38  loss: 0.1841 (0.2075)  time: 2.2034  data: 2.1899  max mem: 337
[18:50:25.217183] Test:  [120/800]  eta: 0:25:38  loss: 0.1819 (0.2066)  time: 2.1956  data: 2.1837  max mem: 337
[18:50:25.229801] Test:  [120/800]  eta: 0:25:38  loss: 0.1817 (0.2074)  time: 2.1949  data: 2.1814  max mem: 337
[18:50:25.858331] Test:  [120/800]  eta: 0:25:42  loss: 0.1811 (0.2076)  time: 2.2053  data: 2.1935  max mem: 337
[18:50:25.882907] Test:  [120/800]  eta: 0:25:42  loss: 0.1834 (0.2073)  time: 2.2052  data: 2.1934  max mem: 337
[18:50:25.883426] Test:  [120/800]  eta: 0:25:42  loss: 0.1872 (0.2065)  time: 2.2038  data: 2.1919  max mem: 337
[18:50:26.072953] Test:  [120/800]  eta: 0:25:43  loss: 0.1825 (0.2075)  time: 2.2048  data: 2.1930  max mem: 337
[18:50:35.667397] Test:  [130/800]  eta: 0:24:13  loss: 0.1782 (0.2070)  time: 2.1431  data: 2.1294  max mem: 337
[18:50:35.685668] Test:  [130/800]  eta: 0:24:13  loss: 0.1807 (0.2071)  time: 2.1431  data: 2.1294  max mem: 337
[18:50:35.857368] Test:  [130/800]  eta: 0:24:14  loss: 0.1802 (0.2072)  time: 2.1420  data: 2.1283  max mem: 337
[18:50:36.171589] Test:  [130/800]  eta: 0:24:16  loss: 0.1795 (0.2076)  time: 2.1300  data: 2.1164  max mem: 337
[18:50:43.226947] Test:  [130/800]  eta: 0:24:52  loss: 0.1823 (0.2075)  time: 2.1839  data: 2.1720  max mem: 337
[18:50:43.262266] Test:  [130/800]  eta: 0:24:52  loss: 0.1828 (0.2074)  time: 2.1850  data: 2.1732  max mem: 337
[18:50:43.277768] Test:  [130/800]  eta: 0:24:52  loss: 0.1780 (0.2070)  time: 2.1840  data: 2.1722  max mem: 337
[18:50:43.434007] Test:  [130/800]  eta: 0:24:53  loss: 0.1819 (0.2077)  time: 2.1936  data: 2.1801  max mem: 337
[18:50:43.488779] Test:  [130/800]  eta: 0:24:53  loss: 0.1817 (0.2077)  time: 2.2009  data: 2.1872  max mem: 337
[18:50:43.490912] Test:  [130/800]  eta: 0:24:53  loss: 0.1798 (0.2074)  time: 2.1964  data: 2.1828  max mem: 337
[18:50:43.492730] Test:  [130/800]  eta: 0:24:53  loss: 0.1782 (0.2075)  time: 2.1895  data: 2.1758  max mem: 337
[18:50:43.531829] Test:  [130/800]  eta: 0:24:53  loss: 0.1811 (0.2067)  time: 2.1933  data: 2.1815  max mem: 337
[18:50:43.998166] Test:  [130/800]  eta: 0:24:56  loss: 0.1826 (0.2078)  time: 2.2098  data: 2.1980  max mem: 337
[18:50:44.001196] Test:  [130/800]  eta: 0:24:56  loss: 0.1846 (0.2066)  time: 2.1881  data: 2.1763  max mem: 337
[18:50:44.015046] Test:  [130/800]  eta: 0:24:56  loss: 0.1816 (0.2074)  time: 2.2055  data: 2.1937  max mem: 337
[18:50:44.217269] Test:  [130/800]  eta: 0:24:57  loss: 0.1789 (0.2074)  time: 2.2088  data: 2.1970  max mem: 337
[18:51:00.408687] Test:  [140/800]  eta: 0:24:06  loss: 0.2175 (0.2078)  time: 2.1251  data: 2.1119  max mem: 337
[18:51:00.431803] Test:  [140/800]  eta: 0:24:06  loss: 0.2159 (0.2077)  time: 2.1217  data: 2.1085  max mem: 337
[18:51:00.706694] Test:  [140/800]  eta: 0:24:07  loss: 0.2173 (0.2079)  time: 2.1277  data: 2.1146  max mem: 337
[18:51:00.941519] Test:  [140/800]  eta: 0:24:08  loss: 0.2148 (0.2082)  time: 2.1252  data: 2.1120  max mem: 337
[18:51:08.701686] Test:  [140/800]  eta: 0:24:45  loss: 0.2188 (0.2076)  time: 2.1772  data: 2.1653  max mem: 337
[18:51:08.702996] Test:  [140/800]  eta: 0:24:45  loss: 0.2164 (0.2079)  time: 2.1772  data: 2.1654  max mem: 337
[18:51:08.711978] Test:  [140/800]  eta: 0:24:45  loss: 0.2224 (0.2082)  time: 2.1795  data: 2.1676  max mem: 337
[18:51:08.807386] Test:  [140/800]  eta: 0:24:45  loss: 0.2173 (0.2081)  time: 2.1807  data: 2.1670  max mem: 337
[18:51:08.872112] Test:  [140/800]  eta: 0:24:45  loss: 0.2170 (0.2082)  time: 2.1847  data: 2.1714  max mem: 337
[18:51:08.872147] Test:  [140/800]  eta: 0:24:45  loss: 0.2180 (0.2080)  time: 2.1821  data: 2.1684  max mem: 337
[18:51:08.873916] Test:  [140/800]  eta: 0:24:45  loss: 0.2198 (0.2081)  time: 2.1843  data: 2.1707  max mem: 337
[18:51:08.970186] Test:  [140/800]  eta: 0:24:46  loss: 0.2152 (0.2073)  time: 2.1876  data: 2.1758  max mem: 337
[18:51:09.369855] Test:  [140/800]  eta: 0:24:48  loss: 0.2163 (0.2071)  time: 2.1743  data: 2.1626  max mem: 337
[18:51:09.396724] Test:  [140/800]  eta: 0:24:48  loss: 0.2173 (0.2084)  time: 2.1769  data: 2.1651  max mem: 337
[18:51:09.397783] Test:  [140/800]  eta: 0:24:48  loss: 0.2162 (0.2078)  time: 2.1757  data: 2.1639  max mem: 337
[18:51:09.642381] Test:  [140/800]  eta: 0:24:49  loss: 0.2135 (0.2080)  time: 2.1784  data: 2.1666  max mem: 337
[18:51:18.869022] Test:  [150/800]  eta: 0:23:29  loss: 0.2133 (0.2070)  time: 2.1600  data: 2.1468  max mem: 337
[18:51:18.900584] Test:  [150/800]  eta: 0:23:29  loss: 0.2155 (0.2070)  time: 2.1607  data: 2.1475  max mem: 337
[18:51:19.196224] Test:  [150/800]  eta: 0:23:30  loss: 0.2152 (0.2070)  time: 2.1669  data: 2.1538  max mem: 337
[18:51:19.439285] Test:  [150/800]  eta: 0:23:31  loss: 0.2105 (0.2074)  time: 2.1633  data: 2.1502  max mem: 337
[18:51:27.553893] Test:  [150/800]  eta: 0:24:06  loss: 0.2124 (0.2068)  time: 2.2138  data: 2.2020  max mem: 337
[18:51:27.568341] Test:  [150/800]  eta: 0:24:06  loss: 0.2125 (0.2070)  time: 2.2153  data: 2.2034  max mem: 337
[18:51:27.578469] Test:  [150/800]  eta: 0:24:06  loss: 0.2169 (0.2073)  time: 2.2175  data: 2.2056  max mem: 337
[18:51:27.686617] Test:  [150/800]  eta: 0:24:07  loss: 0.2110 (0.2073)  time: 2.2126  data: 2.1990  max mem: 337
[18:51:27.703584] Test:  [150/800]  eta: 0:24:07  loss: 0.2119 (0.2073)  time: 2.2106  data: 2.1970  max mem: 337
[18:51:27.712660] Test:  [150/800]  eta: 0:24:07  loss: 0.2116 (0.2072)  time: 2.2110  data: 2.1973  max mem: 337
[18:51:27.728482] Test:  [150/800]  eta: 0:24:07  loss: 0.2124 (0.2073)  time: 2.2119  data: 2.1983  max mem: 337
[18:51:27.936858] Test:  [150/800]  eta: 0:24:08  loss: 0.2118 (0.2066)  time: 2.2202  data: 2.2084  max mem: 337
[18:51:28.283512] Test:  [150/800]  eta: 0:24:09  loss: 0.2121 (0.2069)  time: 2.2134  data: 2.2015  max mem: 337
[18:51:28.294756] Test:  [150/800]  eta: 0:24:10  loss: 0.2153 (0.2075)  time: 2.2148  data: 2.2030  max mem: 337
[18:51:28.460830] Test:  [150/800]  eta: 0:24:10  loss: 0.2108 (0.2063)  time: 2.2229  data: 2.2113  max mem: 337
[18:51:28.592026] Test:  [150/800]  eta: 0:24:11  loss: 0.2105 (0.2072)  time: 2.2187  data: 2.2069  max mem: 337
[18:51:45.115451] Test:  [160/800]  eta: 0:23:25  loss: 0.2034 (0.2084)  time: 2.2341  data: 2.2205  max mem: 337
[18:51:45.120983] Test:  [160/800]  eta: 0:23:25  loss: 0.2001 (0.2084)  time: 2.2356  data: 2.2221  max mem: 337
[18:51:45.410500] Test:  [160/800]  eta: 0:23:26  loss: 0.1993 (0.2084)  time: 2.2351  data: 2.2215  max mem: 337
[18:51:45.660034] Test:  [160/800]  eta: 0:23:27  loss: 0.1959 (0.2088)  time: 2.2359  data: 2.2224  max mem: 337
[18:51:54.627550] Test:  [160/800]  eta: 0:24:03  loss: 0.2004 (0.2083)  time: 2.2962  data: 2.2845  max mem: 337
[18:51:54.634532] Test:  [160/800]  eta: 0:24:03  loss: 0.2014 (0.2084)  time: 2.2965  data: 2.2847  max mem: 337
[18:51:54.693767] Test:  [160/800]  eta: 0:24:03  loss: 0.1976 (0.2079)  time: 2.2861  data: 2.2743  max mem: 337
[18:51:54.701260] Test:  [160/800]  eta: 0:24:03  loss: 0.2003 (0.2087)  time: 2.2994  data: 2.2875  max mem: 337
[18:51:54.968118] Test:  [160/800]  eta: 0:24:05  loss: 0.2018 (0.2086)  time: 2.3047  data: 2.2911  max mem: 337
[18:51:54.996316] Test:  [160/800]  eta: 0:24:05  loss: 0.2058 (0.2085)  time: 2.3062  data: 2.2926  max mem: 337
[18:51:55.019944] Test:  [160/800]  eta: 0:24:05  loss: 0.2015 (0.2085)  time: 2.3106  data: 2.2970  max mem: 337
[18:51:55.093109] Test:  [160/800]  eta: 0:24:05  loss: 0.1979 (0.2089)  time: 2.3110  data: 2.2973  max mem: 337
[18:51:55.534415] Test:  [160/800]  eta: 0:24:07  loss: 0.2028 (0.2076)  time: 2.3082  data: 2.2963  max mem: 337
[18:51:55.543723] Test:  [160/800]  eta: 0:24:07  loss: 0.2037 (0.2088)  time: 2.3073  data: 2.2955  max mem: 337
[18:51:55.555608] Test:  [160/800]  eta: 0:24:07  loss: 0.1954 (0.2085)  time: 2.3078  data: 2.2960  max mem: 337
[18:51:55.565987] Test:  [160/800]  eta: 0:24:07  loss: 0.2005 (0.2085)  time: 2.2961  data: 2.2843  max mem: 337
[18:52:02.943309] Test:  [170/800]  eta: 0:22:48  loss: 0.2098 (0.2093)  time: 2.1752  data: 2.1620  max mem: 337
[18:52:02.948916] Test:  [170/800]  eta: 0:22:48  loss: 0.2138 (0.2087)  time: 2.2024  data: 2.1892  max mem: 337
[18:52:02.953117] Test:  [170/800]  eta: 0:22:48  loss: 0.2222 (0.2088)  time: 2.1878  data: 2.1742  max mem: 337
[18:52:02.958511] Test:  [170/800]  eta: 0:22:48  loss: 0.2156 (0.2088)  time: 2.2044  data: 2.1908  max mem: 337
[18:52:12.538951] Test:  [170/800]  eta: 0:23:24  loss: 0.2134 (0.2088)  time: 2.2492  data: 2.2374  max mem: 337
[18:52:12.584002] Test:  [170/800]  eta: 0:23:24  loss: 0.2171 (0.2089)  time: 2.2507  data: 2.2389  max mem: 337
[18:52:12.593893] Test:  [170/800]  eta: 0:23:24  loss: 0.2213 (0.2084)  time: 2.2328  data: 2.2210  max mem: 337
[18:52:12.690713] Test:  [170/800]  eta: 0:23:24  loss: 0.2139 (0.2090)  time: 2.2556  data: 2.2437  max mem: 337
[18:52:12.923660] Test:  [170/800]  eta: 0:23:25  loss: 0.2195 (0.2092)  time: 2.2597  data: 2.2465  max mem: 337
[18:52:12.941757] Test:  [170/800]  eta: 0:23:25  loss: 0.2165 (0.2088)  time: 2.2614  data: 2.2478  max mem: 337
[18:52:13.014349] Test:  [170/800]  eta: 0:23:25  loss: 0.2072 (0.2093)  time: 2.2663  data: 2.2530  max mem: 337
[18:52:13.058239] Test:  [170/800]  eta: 0:23:25  loss: 0.2145 (0.2090)  time: 2.2677  data: 2.2541  max mem: 337
[18:52:13.293911] Test:  [170/800]  eta: 0:23:26  loss: 0.2158 (0.2089)  time: 2.2505  data: 2.2386  max mem: 337
[18:52:13.298834] Test:  [170/800]  eta: 0:23:26  loss: 0.2173 (0.2093)  time: 2.2502  data: 2.2383  max mem: 337
[18:52:13.332063] Test:  [170/800]  eta: 0:23:26  loss: 0.2100 (0.2091)  time: 2.2370  data: 2.2252  max mem: 337
[18:52:13.333503] Test:  [170/800]  eta: 0:23:26  loss: 0.2153 (0.2080)  time: 2.2436  data: 2.2317  max mem: 337
[18:52:29.329162] Test:  [180/800]  eta: 0:22:42  loss: 0.2292 (0.2107)  time: 2.2106  data: 2.1970  max mem: 337
[18:52:29.344285] Test:  [180/800]  eta: 0:22:42  loss: 0.2289 (0.2106)  time: 2.2111  data: 2.1978  max mem: 337
[18:52:29.345574] Test:  [180/800]  eta: 0:22:42  loss: 0.2252 (0.2107)  time: 2.1967  data: 2.1831  max mem: 337
[18:52:29.346056] Test:  [180/800]  eta: 0:22:42  loss: 0.2278 (0.2112)  time: 2.1843  data: 2.1710  max mem: 337
[18:52:39.548973] Test:  [180/800]  eta: 0:23:17  loss: 0.2290 (0.2108)  time: 2.2423  data: 2.2304  max mem: 337
[18:52:39.574136] Test:  [180/800]  eta: 0:23:18  loss: 0.2312 (0.2108)  time: 2.2469  data: 2.2352  max mem: 337
[18:52:39.585414] Test:  [180/800]  eta: 0:23:18  loss: 0.2323 (0.2106)  time: 2.2478  data: 2.2360  max mem: 337
[18:52:39.613411] Test:  [180/800]  eta: 0:23:18  loss: 0.2259 (0.2103)  time: 2.2459  data: 2.2341  max mem: 337
[18:52:39.840409] Test:  [180/800]  eta: 0:23:18  loss: 0.2239 (0.2109)  time: 2.2436  data: 2.2305  max mem: 337
[18:52:39.851890] Test:  [180/800]  eta: 0:23:18  loss: 0.2293 (0.2109)  time: 2.2427  data: 2.2292  max mem: 337
[18:52:39.900177] Test:  [180/800]  eta: 0:23:19  loss: 0.2233 (0.2106)  time: 2.2440  data: 2.2304  max mem: 337
[18:52:39.900605] Test:  [180/800]  eta: 0:23:19  loss: 0.2211 (0.2110)  time: 2.2403  data: 2.2272  max mem: 337
[18:52:40.232997] Test:  [180/800]  eta: 0:23:20  loss: 0.2207 (0.2111)  time: 2.2344  data: 2.2226  max mem: 337
[18:52:40.243656] Test:  [180/800]  eta: 0:23:20  loss: 0.2338 (0.2108)  time: 2.2344  data: 2.2225  max mem: 337
[18:52:40.244935] Test:  [180/800]  eta: 0:23:20  loss: 0.2262 (0.2098)  time: 2.2355  data: 2.2238  max mem: 337
[18:52:40.375789] Test:  [180/800]  eta: 0:23:20  loss: 0.2194 (0.2109)  time: 2.2404  data: 2.2286  max mem: 337
[18:52:46.480575] Test:  [190/800]  eta: 0:22:05  loss: 0.2137 (0.2105)  time: 2.1765  data: 2.1629  max mem: 337
[18:52:46.480606] Test:  [190/800]  eta: 0:22:05  loss: 0.2078 (0.2104)  time: 2.1763  data: 2.1627  max mem: 337
[18:52:46.480903] Test:  [190/800]  eta: 0:22:05  loss: 0.2116 (0.2106)  time: 2.1761  data: 2.1624  max mem: 337
[18:52:46.608702] Test:  [190/800]  eta: 0:22:05  loss: 0.2076 (0.2109)  time: 2.1832  data: 2.1696  max mem: 337
[18:52:57.113957] Test:  [190/800]  eta: 0:22:39  loss: 0.2051 (0.2106)  time: 2.2265  data: 2.2148  max mem: 337
[18:52:57.124678] Test:  [190/800]  eta: 0:22:39  loss: 0.2095 (0.2107)  time: 2.2217  data: 2.2098  max mem: 337
[18:52:57.129679] Test:  [190/800]  eta: 0:22:39  loss: 0.2086 (0.2106)  time: 2.2295  data: 2.2179  max mem: 337
[18:52:57.306994] Test:  [190/800]  eta: 0:22:40  loss: 0.2068 (0.2103)  time: 2.2356  data: 2.2238  max mem: 337
[18:52:57.413101] Test:  [190/800]  eta: 0:22:40  loss: 0.2054 (0.2105)  time: 2.2235  data: 2.2099  max mem: 337
[18:52:57.422358] Test:  [190/800]  eta: 0:22:40  loss: 0.2046 (0.2107)  time: 2.2182  data: 2.2050  max mem: 337
[18:52:57.422394] Test:  [190/800]  eta: 0:22:40  loss: 0.2118 (0.2109)  time: 2.2204  data: 2.2068  max mem: 337
[18:52:57.579421] Test:  [190/800]  eta: 0:22:40  loss: 0.2099 (0.2108)  time: 2.2327  data: 2.2193  max mem: 337
[18:52:57.751712] Test:  [190/800]  eta: 0:22:41  loss: 0.2138 (0.2109)  time: 2.2226  data: 2.2108  max mem: 337
[18:52:57.807002] Test:  [190/800]  eta: 0:22:41  loss: 0.2093 (0.2108)  time: 2.2256  data: 2.2138  max mem: 337
[18:52:57.823950] Test:  [190/800]  eta: 0:22:41  loss: 0.2097 (0.2097)  time: 2.2245  data: 2.2128  max mem: 337
[18:52:58.041561] Test:  [190/800]  eta: 0:22:42  loss: 0.2106 (0.2107)  time: 2.2354  data: 2.2236  max mem: 337
[18:53:12.469714] Test:  [200/800]  eta: 0:21:56  loss: 0.2147 (0.2121)  time: 2.1561  data: 2.1425  max mem: 337
[18:53:12.474654] Test:  [200/800]  eta: 0:21:56  loss: 0.2123 (0.2118)  time: 2.1564  data: 2.1428  max mem: 337
[18:53:12.532524] Test:  [200/800]  eta: 0:21:56  loss: 0.2126 (0.2119)  time: 2.1601  data: 2.1464  max mem: 337
[18:53:12.539603] Test:  [200/800]  eta: 0:21:56  loss: 0.2179 (0.2119)  time: 2.1597  data: 2.1460  max mem: 337
[18:53:23.170510] Test:  [200/800]  eta: 0:22:28  loss: 0.2109 (0.2119)  time: 2.1798  data: 2.1680  max mem: 337
[18:53:23.196308] Test:  [200/800]  eta: 0:22:28  loss: 0.2102 (0.2120)  time: 2.1823  data: 2.1704  max mem: 337
[18:53:23.240050] Test:  [200/800]  eta: 0:22:28  loss: 0.2088 (0.2118)  time: 2.1827  data: 2.1710  max mem: 337
[18:53:23.473614] Test:  [200/800]  eta: 0:22:29  loss: 0.2142 (0.2119)  time: 2.1786  data: 2.1650  max mem: 337
[18:53:23.519664] Test:  [200/800]  eta: 0:22:29  loss: 0.2116 (0.2120)  time: 2.1833  data: 2.1701  max mem: 337
[18:53:23.527433] Test:  [200/800]  eta: 0:22:29  loss: 0.2130 (0.2121)  time: 2.1813  data: 2.1676  max mem: 337
[18:53:23.593215] Test:  [200/800]  eta: 0:22:29  loss: 0.2164 (0.2117)  time: 2.1989  data: 2.1871  max mem: 337
[18:53:23.765368] Test:  [200/800]  eta: 0:22:30  loss: 0.2126 (0.2121)  time: 2.1962  data: 2.1826  max mem: 337
[18:53:23.922688] Test:  [200/800]  eta: 0:22:30  loss: 0.2144 (0.2122)  time: 2.1844  data: 2.1726  max mem: 337
[18:53:23.984544] Test:  [200/800]  eta: 0:22:30  loss: 0.2128 (0.2121)  time: 2.1870  data: 2.1752  max mem: 337
[18:53:24.060253] Test:  [200/800]  eta: 0:22:31  loss: 0.2104 (0.2110)  time: 2.1907  data: 2.1789  max mem: 337
[18:53:24.162737] Test:  [200/800]  eta: 0:22:31  loss: 0.2130 (0.2120)  time: 2.1893  data: 2.1775  max mem: 337
[18:53:29.739198] Test:  [210/800]  eta: 0:21:21  loss: 0.2257 (0.2114)  time: 2.1565  data: 2.1428  max mem: 337
[18:53:29.749729] Test:  [210/800]  eta: 0:21:21  loss: 0.2307 (0.2114)  time: 2.1634  data: 2.1497  max mem: 337
[18:53:29.779881] Test:  [210/800]  eta: 0:21:21  loss: 0.2275 (0.2112)  time: 2.1649  data: 2.1513  max mem: 337
[18:53:29.781130] Test:  [210/800]  eta: 0:21:21  loss: 0.2272 (0.2114)  time: 2.1650  data: 2.1513  max mem: 337
[18:53:40.402947] Test:  [210/800]  eta: 0:21:51  loss: 0.2273 (0.2114)  time: 2.1644  data: 2.1526  max mem: 337
[18:53:40.444343] Test:  [210/800]  eta: 0:21:51  loss: 0.2235 (0.2114)  time: 2.1659  data: 2.1540  max mem: 337
[18:53:40.455528] Test:  [210/800]  eta: 0:21:51  loss: 0.2235 (0.2112)  time: 2.1662  data: 2.1544  max mem: 337
[18:53:40.729759] Test:  [210/800]  eta: 0:21:52  loss: 0.2264 (0.2113)  time: 2.1658  data: 2.1524  max mem: 337
[18:53:40.781284] Test:  [210/800]  eta: 0:21:52  loss: 0.2238 (0.2113)  time: 2.1679  data: 2.1547  max mem: 337
[18:53:40.792819] Test:  [210/800]  eta: 0:21:52  loss: 0.2284 (0.2115)  time: 2.1685  data: 2.1548  max mem: 337
[18:53:41.075912] Test:  [210/800]  eta: 0:21:53  loss: 0.2238 (0.2111)  time: 2.1884  data: 2.1766  max mem: 337
[18:53:41.099175] Test:  [210/800]  eta: 0:21:53  loss: 0.2245 (0.2115)  time: 2.1759  data: 2.1623  max mem: 337
[18:53:41.200669] Test:  [210/800]  eta: 0:21:53  loss: 0.2224 (0.2116)  time: 2.1724  data: 2.1606  max mem: 337
[18:53:41.211045] Test:  [210/800]  eta: 0:21:53  loss: 0.2266 (0.2115)  time: 2.1702  data: 2.1585  max mem: 337
[18:53:41.330446] Test:  [210/800]  eta: 0:21:53  loss: 0.2248 (0.2103)  time: 2.1753  data: 2.1636  max mem: 337
[18:53:41.454014] Test:  [210/800]  eta: 0:21:54  loss: 0.2239 (0.2115)  time: 2.1706  data: 2.1590  max mem: 337
[18:53:55.324385] Test:  [220/800]  eta: 0:21:09  loss: 0.2307 (0.2121)  time: 2.1392  data: 2.1255  max mem: 337
[18:53:55.324462] Test:  [220/800]  eta: 0:21:09  loss: 0.2272 (0.2120)  time: 2.1396  data: 2.1259  max mem: 337
[18:53:55.324583] Test:  [220/800]  eta: 0:21:09  loss: 0.2275 (0.2118)  time: 2.1425  data: 2.1288  max mem: 337
[18:53:55.386355] Test:  [220/800]  eta: 0:21:09  loss: 0.2252 (0.2120)  time: 2.1458  data: 2.1321  max mem: 337
[18:54:06.165721] Test:  [220/800]  eta: 0:21:38  loss: 0.2235 (0.2119)  time: 2.1484  data: 2.1367  max mem: 337
[18:54:06.195808] Test:  [220/800]  eta: 0:21:38  loss: 0.2235 (0.2118)  time: 2.1477  data: 2.1361  max mem: 337
[18:54:06.232946] Test:  [220/800]  eta: 0:21:38  loss: 0.2247 (0.2121)  time: 2.1531  data: 2.1413  max mem: 337
[18:54:06.535029] Test:  [220/800]  eta: 0:21:39  loss: 0.2284 (0.2122)  time: 2.1503  data: 2.1366  max mem: 337
[18:54:06.606617] Test:  [220/800]  eta: 0:21:39  loss: 0.2238 (0.2119)  time: 2.1543  data: 2.1412  max mem: 337
[18:54:06.760469] Test:  [220/800]  eta: 0:21:39  loss: 0.2264 (0.2119)  time: 2.1643  data: 2.1510  max mem: 337
[18:54:06.913674] Test:  [220/800]  eta: 0:21:40  loss: 0.2245 (0.2121)  time: 2.1574  data: 2.1438  max mem: 337
[18:54:06.999921] Test:  [220/800]  eta: 0:21:40  loss: 0.2266 (0.2121)  time: 2.1507  data: 2.1390  max mem: 337
[18:54:07.017653] Test:  [220/800]  eta: 0:21:40  loss: 0.2224 (0.2122)  time: 2.1547  data: 2.1429  max mem: 337
[18:54:07.192930] Test:  [220/800]  eta: 0:21:41  loss: 0.2237 (0.2117)  time: 2.1799  data: 2.1683  max mem: 337
[18:54:07.234655] Test:  [220/800]  eta: 0:21:41  loss: 0.2248 (0.2110)  time: 2.1587  data: 2.1470  max mem: 337
[18:54:07.255349] Test:  [220/800]  eta: 0:21:41  loss: 0.2239 (0.2121)  time: 2.1546  data: 2.1430  max mem: 337
[18:54:11.622398] Test:  [230/800]  eta: 0:20:34  loss: 0.2329 (0.2127)  time: 2.0936  data: 2.0799  max mem: 337
[18:54:11.623153] Test:  [230/800]  eta: 0:20:34  loss: 0.2272 (0.2127)  time: 2.0921  data: 2.0784  max mem: 337
[18:54:11.625946] Test:  [230/800]  eta: 0:20:34  loss: 0.2296 (0.2125)  time: 2.0923  data: 2.0786  max mem: 337
[18:54:11.811453] Test:  [230/800]  eta: 0:20:34  loss: 0.2264 (0.2127)  time: 2.1036  data: 2.0899  max mem: 337
[18:54:22.809142] Test:  [230/800]  eta: 0:21:01  loss: 0.2247 (0.2127)  time: 2.1203  data: 2.1084  max mem: 337
[18:54:22.844738] Test:  [230/800]  eta: 0:21:01  loss: 0.2271 (0.2126)  time: 2.1200  data: 2.1083  max mem: 337
[18:54:22.851005] Test:  [230/800]  eta: 0:21:01  loss: 0.2262 (0.2125)  time: 2.1197  data: 2.1081  max mem: 337
[18:54:23.270826] Test:  [230/800]  eta: 0:21:02  loss: 0.2261 (0.2126)  time: 2.1244  data: 2.1109  max mem: 337
[18:54:23.278904] Test:  [230/800]  eta: 0:21:02  loss: 0.2303 (0.2129)  time: 2.1243  data: 2.1106  max mem: 337
[18:54:23.402687] Test:  [230/800]  eta: 0:21:03  loss: 0.2313 (0.2126)  time: 2.1336  data: 2.1201  max mem: 337
[18:54:23.615483] Test:  [230/800]  eta: 0:21:03  loss: 0.2316 (0.2128)  time: 2.1202  data: 2.1083  max mem: 337
[18:54:23.625074] Test:  [230/800]  eta: 0:21:03  loss: 0.2310 (0.2128)  time: 2.1212  data: 2.1094  max mem: 337
[18:54:23.626859] Test:  [230/800]  eta: 0:21:03  loss: 0.2304 (0.2128)  time: 2.1263  data: 2.1127  max mem: 337
[18:54:23.823352] Test:  [230/800]  eta: 0:21:04  loss: 0.2252 (0.2128)  time: 2.1184  data: 2.1066  max mem: 337
[18:54:23.854338] Test:  [230/800]  eta: 0:21:04  loss: 0.2309 (0.2117)  time: 2.1261  data: 2.1143  max mem: 337
[18:54:24.002443] Test:  [230/800]  eta: 0:21:04  loss: 0.2237 (0.2124)  time: 2.1463  data: 2.1346  max mem: 337
[18:54:36.013479] Test:  [240/800]  eta: 0:20:18  loss: 0.2066 (0.2119)  time: 2.0344  data: 2.0207  max mem: 337
[18:54:36.027414] Test:  [240/800]  eta: 0:20:18  loss: 0.2070 (0.2117)  time: 2.0351  data: 2.0214  max mem: 337
[18:54:36.054333] Test:  [240/800]  eta: 0:20:18  loss: 0.2096 (0.2119)  time: 2.0364  data: 2.0228  max mem: 337
[18:54:36.073370] Test:  [240/800]  eta: 0:20:18  loss: 0.2085 (0.2119)  time: 2.0343  data: 2.0207  max mem: 337
[18:54:47.462442] Test:  [240/800]  eta: 0:20:45  loss: 0.2112 (0.2119)  time: 2.0648  data: 2.0529  max mem: 337
[18:54:47.465859] Test:  [240/800]  eta: 0:20:45  loss: 0.2076 (0.2120)  time: 2.0616  data: 2.0498  max mem: 337
[18:54:47.488432] Test:  [240/800]  eta: 0:20:45  loss: 0.2090 (0.2117)  time: 2.0646  data: 2.0528  max mem: 337
[18:54:47.952718] Test:  [240/800]  eta: 0:20:46  loss: 0.2109 (0.2121)  time: 2.0708  data: 2.0571  max mem: 337
[18:54:47.957560] Test:  [240/800]  eta: 0:20:46  loss: 0.2073 (0.2118)  time: 2.0675  data: 2.0539  max mem: 337
[18:54:48.154498] Test:  [240/800]  eta: 0:20:47  loss: 0.2074 (0.2120)  time: 2.0697  data: 2.0560  max mem: 337
[18:54:48.364854] Test:  [240/800]  eta: 0:20:47  loss: 0.2057 (0.2122)  time: 2.0725  data: 2.0589  max mem: 337
[18:54:48.480417] Test:  [240/800]  eta: 0:20:47  loss: 0.2062 (0.2121)  time: 2.0740  data: 2.0621  max mem: 337
[18:54:48.507014] Test:  [240/800]  eta: 0:20:47  loss: 0.2076 (0.2109)  time: 2.0636  data: 2.0518  max mem: 337
[18:54:48.529298] Test:  [240/800]  eta: 0:20:47  loss: 0.2056 (0.2121)  time: 2.0755  data: 2.0637  max mem: 337
[18:54:48.529959] Test:  [240/800]  eta: 0:20:47  loss: 0.2110 (0.2120)  time: 2.0637  data: 2.0519  max mem: 337
[18:54:49.124856] Test:  [240/800]  eta: 0:20:49  loss: 0.2107 (0.2117)  time: 2.0966  data: 2.0847  max mem: 337
[18:54:52.410987] Test:  [250/800]  eta: 0:19:45  loss: 0.1874 (0.2124)  time: 2.0394  data: 2.0262  max mem: 337
[18:54:52.419259] Test:  [250/800]  eta: 0:19:45  loss: 0.1840 (0.2120)  time: 2.0396  data: 2.0264  max mem: 337
[18:54:52.436128] Test:  [250/800]  eta: 0:19:45  loss: 0.1923 (0.2124)  time: 2.0406  data: 2.0274  max mem: 337
[18:54:52.511835] Test:  [250/800]  eta: 0:19:45  loss: 0.1887 (0.2123)  time: 2.0350  data: 2.0218  max mem: 337
[18:55:04.095992] Test:  [250/800]  eta: 0:20:10  loss: 0.1872 (0.2121)  time: 2.0622  data: 2.0504  max mem: 337
[18:55:04.163634] Test:  [250/800]  eta: 0:20:11  loss: 0.1896 (0.2123)  time: 2.0659  data: 2.0540  max mem: 337
[18:55:04.164881] Test:  [250/800]  eta: 0:20:11  loss: 0.1886 (0.2124)  time: 2.0677  data: 2.0559  max mem: 337
[18:55:04.638950] Test:  [250/800]  eta: 0:20:12  loss: 0.1896 (0.2122)  time: 2.0684  data: 2.0548  max mem: 337
[18:55:04.688582] Test:  [250/800]  eta: 0:20:12  loss: 0.1959 (0.2126)  time: 2.0704  data: 2.0567  max mem: 337
[18:55:04.819697] Test:  [250/800]  eta: 0:20:12  loss: 0.1897 (0.2124)  time: 2.0708  data: 2.0572  max mem: 337
[18:55:05.021837] Test:  [250/800]  eta: 0:20:13  loss: 0.1909 (0.2125)  time: 2.0698  data: 2.0580  max mem: 337
[18:55:05.037606] Test:  [250/800]  eta: 0:20:13  loss: 0.1892 (0.2114)  time: 2.0591  data: 2.0473  max mem: 337
[18:55:05.038908] Test:  [250/800]  eta: 0:20:13  loss: 0.1894 (0.2125)  time: 2.0711  data: 2.0593  max mem: 337
[18:55:05.040179] Test:  [250/800]  eta: 0:20:13  loss: 0.1894 (0.2124)  time: 2.0608  data: 2.0490  max mem: 337
[18:55:05.139725] Test:  [250/800]  eta: 0:20:13  loss: 0.1912 (0.2126)  time: 2.0756  data: 2.0620  max mem: 337
[18:55:05.977099] Test:  [250/800]  eta: 0:20:15  loss: 0.1858 (0.2121)  time: 2.0987  data: 2.0869  max mem: 337
[18:55:17.145509] Test:  [260/800]  eta: 0:19:30  loss: 0.2011 (0.2125)  time: 2.0545  data: 2.0414  max mem: 337
[18:55:17.154348] Test:  [260/800]  eta: 0:19:30  loss: 0.2040 (0.2126)  time: 2.0570  data: 2.0438  max mem: 337
[18:55:17.155955] Test:  [260/800]  eta: 0:19:30  loss: 0.2031 (0.2122)  time: 2.0564  data: 2.0432  max mem: 337
[18:55:17.302685] Test:  [260/800]  eta: 0:19:30  loss: 0.2066 (0.2125)  time: 2.0614  data: 2.0482  max mem: 337
[18:55:29.385352] Test:  [260/800]  eta: 0:19:55  loss: 0.2062 (0.2125)  time: 2.0959  data: 2.0843  max mem: 337
[18:55:29.393893] Test:  [260/800]  eta: 0:19:55  loss: 0.2001 (0.2123)  time: 2.0952  data: 2.0834  max mem: 337
[18:55:29.436666] Test:  [260/800]  eta: 0:19:55  loss: 0.2060 (0.2125)  time: 2.0987  data: 2.0868  max mem: 337
[18:55:29.931040] Test:  [260/800]  eta: 0:19:56  loss: 0.1999 (0.2127)  time: 2.0989  data: 2.0857  max mem: 337
[18:55:29.951198] Test:  [260/800]  eta: 0:19:56  loss: 0.2036 (0.2124)  time: 2.0996  data: 2.0860  max mem: 337
[18:55:30.192414] Test:  [260/800]  eta: 0:19:57  loss: 0.2070 (0.2126)  time: 2.1019  data: 2.0882  max mem: 337
[18:55:30.293194] Test:  [260/800]  eta: 0:19:57  loss: 0.2043 (0.2126)  time: 2.0906  data: 2.0787  max mem: 337
[18:55:30.307177] Test:  [260/800]  eta: 0:19:57  loss: 0.2029 (0.2116)  time: 2.0900  data: 2.0781  max mem: 337
[18:55:30.315051] Test:  [260/800]  eta: 0:19:57  loss: 0.2030 (0.2126)  time: 2.0892  data: 2.0774  max mem: 337
[18:55:30.315176] Test:  [260/800]  eta: 0:19:57  loss: 0.2018 (0.2127)  time: 2.0892  data: 2.0774  max mem: 337
[18:55:30.460112] Test:  [260/800]  eta: 0:19:57  loss: 0.2036 (0.2128)  time: 2.1047  data: 2.0911  max mem: 337
[18:55:31.544364] Test:  [260/800]  eta: 0:20:00  loss: 0.2101 (0.2123)  time: 2.1209  data: 2.1091  max mem: 337
[18:55:33.876444] Test:  [270/800]  eta: 0:18:58  loss: 0.2098 (0.2131)  time: 2.0720  data: 2.0583  max mem: 337
[18:55:33.879925] Test:  [270/800]  eta: 0:18:58  loss: 0.2129 (0.2131)  time: 2.0734  data: 2.0598  max mem: 337
[18:55:33.883723] Test:  [270/800]  eta: 0:18:59  loss: 0.2119 (0.2130)  time: 2.0685  data: 2.0549  max mem: 337
[18:55:33.901080] Test:  [270/800]  eta: 0:18:59  loss: 0.2120 (0.2127)  time: 2.0740  data: 2.0604  max mem: 337
[18:55:46.094659] Test:  [270/800]  eta: 0:19:22  loss: 0.2093 (0.2128)  time: 2.0999  data: 2.0881  max mem: 337
[18:55:46.153541] Test:  [270/800]  eta: 0:19:23  loss: 0.2090 (0.2130)  time: 2.0994  data: 2.0878  max mem: 337
[18:55:46.164912] Test:  [270/800]  eta: 0:19:23  loss: 0.2138 (0.2130)  time: 2.1000  data: 2.0881  max mem: 337
[18:55:46.641487] Test:  [270/800]  eta: 0:19:24  loss: 0.2125 (0.2129)  time: 2.1001  data: 2.0865  max mem: 337
[18:55:46.673772] Test:  [270/800]  eta: 0:19:24  loss: 0.2107 (0.2132)  time: 2.0992  data: 2.0860  max mem: 337
[18:55:46.944093] Test:  [270/800]  eta: 0:19:24  loss: 0.2096 (0.2121)  time: 2.0953  data: 2.0834  max mem: 337
[18:55:46.953669] Test:  [270/800]  eta: 0:19:24  loss: 0.2116 (0.2131)  time: 2.0965  data: 2.0847  max mem: 337
[18:55:46.962774] Test:  [270/800]  eta: 0:19:24  loss: 0.2098 (0.2131)  time: 2.1071  data: 2.0935  max mem: 337
[18:55:46.987683] Test:  [270/800]  eta: 0:19:24  loss: 0.2092 (0.2130)  time: 2.0973  data: 2.0855  max mem: 337
[18:55:46.995479] Test:  [270/800]  eta: 0:19:24  loss: 0.2141 (0.2131)  time: 2.0978  data: 2.0859  max mem: 337
[18:55:47.115559] Test:  [270/800]  eta: 0:19:24  loss: 0.2108 (0.2132)  time: 2.0987  data: 2.0851  max mem: 337
[18:55:48.539557] Test:  [270/800]  eta: 0:19:27  loss: 0.2101 (0.2127)  time: 2.1281  data: 2.1162  max mem: 337
[18:55:58.096585] Test:  [280/800]  eta: 0:18:42  loss: 0.2147 (0.2135)  time: 2.0471  data: 2.0334  max mem: 337
[18:55:58.124523] Test:  [280/800]  eta: 0:18:42  loss: 0.2162 (0.2132)  time: 2.0484  data: 2.0347  max mem: 337
[18:55:58.134670] Test:  [280/800]  eta: 0:18:42  loss: 0.2161 (0.2136)  time: 2.0494  data: 2.0357  max mem: 337
[18:55:58.210669] Test:  [280/800]  eta: 0:18:42  loss: 0.2126 (0.2135)  time: 2.0454  data: 2.0317  max mem: 337
[18:56:11.305792] Test:  [280/800]  eta: 0:19:07  loss: 0.2149 (0.2132)  time: 2.0955  data: 2.0837  max mem: 337
[18:56:11.344736] Test:  [280/800]  eta: 0:19:07  loss: 0.2149 (0.2134)  time: 2.0979  data: 2.0861  max mem: 337
[18:56:11.352875] Test:  [280/800]  eta: 0:19:07  loss: 0.2140 (0.2134)  time: 2.0958  data: 2.0839  max mem: 337
[18:56:11.871715] Test:  [280/800]  eta: 0:19:08  loss: 0.2158 (0.2133)  time: 2.0960  data: 2.0824  max mem: 337
[18:56:11.886319] Test:  [280/800]  eta: 0:19:08  loss: 0.2147 (0.2136)  time: 2.0977  data: 2.0841  max mem: 337
[18:56:12.094138] Test:  [280/800]  eta: 0:19:08  loss: 0.2142 (0.2136)  time: 2.0889  data: 2.0771  max mem: 337
[18:56:12.105549] Test:  [280/800]  eta: 0:19:08  loss: 0.2130 (0.2134)  time: 2.0895  data: 2.0779  max mem: 337
[18:56:12.117856] Test:  [280/800]  eta: 0:19:08  loss: 0.2169 (0.2136)  time: 2.0912  data: 2.0793  max mem: 337
[18:56:12.229907] Test:  [280/800]  eta: 0:19:08  loss: 0.2147 (0.2135)  time: 2.1018  data: 2.0882  max mem: 337
[18:56:12.249094] Test:  [280/800]  eta: 0:19:08  loss: 0.2118 (0.2126)  time: 2.0970  data: 2.0852  max mem: 337
[18:56:12.388833] Test:  [280/800]  eta: 0:19:09  loss: 0.2111 (0.2136)  time: 2.0964  data: 2.0829  max mem: 337
[18:56:14.105262] Test:  [280/800]  eta: 0:19:12  loss: 0.2124 (0.2132)  time: 2.1280  data: 2.1162  max mem: 337
[18:56:16.042162] Test:  [290/800]  eta: 0:18:14  loss: 0.2393 (0.2147)  time: 2.1070  data: 2.0934  max mem: 337
[18:56:16.061811] Test:  [290/800]  eta: 0:18:14  loss: 0.2424 (0.2150)  time: 2.1090  data: 2.0956  max mem: 337
[18:56:16.062167] Test:  [290/800]  eta: 0:18:14  loss: 0.2486 (0.2151)  time: 2.1092  data: 2.0958  max mem: 337
[18:56:16.064779] Test:  [290/800]  eta: 0:18:14  loss: 0.2498 (0.2151)  time: 2.1090  data: 2.0956  max mem: 337
[18:56:28.194995] Test:  [290/800]  eta: 0:18:35  loss: 0.2412 (0.2150)  time: 2.1015  data: 2.0897  max mem: 337
[18:56:28.221228] Test:  [290/800]  eta: 0:18:35  loss: 0.2441 (0.2149)  time: 2.1033  data: 2.0915  max mem: 337
[18:56:28.226890] Test:  [290/800]  eta: 0:18:36  loss: 0.2469 (0.2148)  time: 2.1066  data: 2.0949  max mem: 337
[18:56:28.766719] Test:  [290/800]  eta: 0:18:36  loss: 0.2411 (0.2152)  time: 2.1046  data: 2.0911  max mem: 337
[18:56:28.844708] Test:  [290/800]  eta: 0:18:37  loss: 0.2433 (0.2149)  time: 2.1101  data: 2.0965  max mem: 337
[18:56:28.904848] Test:  [290/800]  eta: 0:18:37  loss: 0.2436 (0.2149)  time: 2.0958  data: 2.0842  max mem: 337
[18:56:28.923406] Test:  [290/800]  eta: 0:18:37  loss: 0.2477 (0.2151)  time: 2.0964  data: 2.0845  max mem: 337
[18:56:28.946333] Test:  [290/800]  eta: 0:18:37  loss: 0.2451 (0.2151)  time: 2.0996  data: 2.0878  max mem: 337
[18:56:29.113036] Test:  [290/800]  eta: 0:18:37  loss: 0.2423 (0.2149)  time: 2.1075  data: 2.0938  max mem: 337
[18:56:29.146678] Test:  [290/800]  eta: 0:18:37  loss: 0.2462 (0.2141)  time: 2.1101  data: 2.0982  max mem: 337
[18:56:29.332668] Test:  [290/800]  eta: 0:18:37  loss: 0.2447 (0.2151)  time: 2.1108  data: 2.0977  max mem: 337
[18:56:31.237719] Test:  [290/800]  eta: 0:18:41  loss: 0.2437 (0.2148)  time: 2.1349  data: 2.1230  max mem: 337
[18:56:38.870276] Test:  [300/800]  eta: 0:17:55  loss: 0.2338 (0.2144)  time: 2.0372  data: 2.0236  max mem: 337
[18:56:38.874877] Test:  [300/800]  eta: 0:17:55  loss: 0.2399 (0.2147)  time: 2.0389  data: 2.0255  max mem: 337
[18:56:38.879115] Test:  [300/800]  eta: 0:17:55  loss: 0.2439 (0.2149)  time: 2.0372  data: 2.0238  max mem: 337
[18:56:39.129654] Test:  [300/800]  eta: 0:17:55  loss: 0.2429 (0.2149)  time: 2.0459  data: 2.0325  max mem: 337
[18:56:53.024344] Test:  [300/800]  eta: 0:18:18  loss: 0.2398 (0.2147)  time: 2.0835  data: 2.0718  max mem: 337
[18:56:53.041087] Test:  [300/800]  eta: 0:18:18  loss: 0.2326 (0.2145)  time: 2.0867  data: 2.0751  max mem: 337
[18:56:53.071141] Test:  [300/800]  eta: 0:18:19  loss: 0.2397 (0.2147)  time: 2.0863  data: 2.0744  max mem: 337
[18:56:53.682917] Test:  [300/800]  eta: 0:18:20  loss: 0.2353 (0.2149)  time: 2.0794  data: 2.0676  max mem: 337
[18:56:53.703379] Test:  [300/800]  eta: 0:18:20  loss: 0.2411 (0.2150)  time: 2.0908  data: 2.0773  max mem: 337
[18:56:53.704603] Test:  [300/800]  eta: 0:18:20  loss: 0.2398 (0.2146)  time: 2.0799  data: 2.0681  max mem: 337
[18:56:53.713977] Test:  [300/800]  eta: 0:18:20  loss: 0.2411 (0.2149)  time: 2.0798  data: 2.0679  max mem: 337
[18:56:53.720030] Test:  [300/800]  eta: 0:18:20  loss: 0.2415 (0.2147)  time: 2.0924  data: 2.0787  max mem: 337
[18:56:53.967530] Test:  [300/800]  eta: 0:18:20  loss: 0.2401 (0.2147)  time: 2.0868  data: 2.0732  max mem: 337
[18:56:53.994925] Test:  [300/800]  eta: 0:18:20  loss: 0.2393 (0.2139)  time: 2.0872  data: 2.0754  max mem: 337
[18:56:54.210278] Test:  [300/800]  eta: 0:18:20  loss: 0.2447 (0.2149)  time: 2.0910  data: 2.0778  max mem: 337
[18:56:56.403456] Test:  [300/800]  eta: 0:18:24  loss: 0.2371 (0.2145)  time: 2.1149  data: 2.1030  max mem: 337
[18:56:58.518778] Test:  [310/800]  eta: 0:17:30  loss: 0.1882 (0.2147)  time: 2.1228  data: 2.1091  max mem: 337
[18:56:58.519759] Test:  [310/800]  eta: 0:17:30  loss: 0.1938 (0.2142)  time: 2.1238  data: 2.1102  max mem: 337
[18:56:58.522195] Test:  [310/800]  eta: 0:17:30  loss: 0.1847 (0.2146)  time: 2.1230  data: 2.1094  max mem: 337
[18:56:58.522663] Test:  [310/800]  eta: 0:17:30  loss: 0.1892 (0.2147)  time: 2.1228  data: 2.1092  max mem: 337
[18:57:09.895592] Test:  [310/800]  eta: 0:17:48  loss: 0.1877 (0.2142)  time: 2.0834  data: 2.0716  max mem: 337
[18:57:09.959838] Test:  [310/800]  eta: 0:17:49  loss: 0.1872 (0.2145)  time: 2.0882  data: 2.0763  max mem: 337
[18:57:09.962645] Test:  [310/800]  eta: 0:17:49  loss: 0.1906 (0.2145)  time: 2.0870  data: 2.0752  max mem: 337
[18:57:10.573191] Test:  [310/800]  eta: 0:17:49  loss: 0.1894 (0.2144)  time: 2.0834  data: 2.0716  max mem: 337
[18:57:10.582713] Test:  [310/800]  eta: 0:17:50  loss: 0.1828 (0.2146)  time: 2.0818  data: 2.0700  max mem: 337
[18:57:10.583160] Test:  [310/800]  eta: 0:17:50  loss: 0.1920 (0.2146)  time: 2.0829  data: 2.0713  max mem: 337
[18:57:10.623131] Test:  [310/800]  eta: 0:17:50  loss: 0.1909 (0.2146)  time: 2.0928  data: 2.0791  max mem: 337
[18:57:10.625927] Test:  [310/800]  eta: 0:17:50  loss: 0.1855 (0.2145)  time: 2.0890  data: 2.0758  max mem: 337
[18:57:10.836265] Test:  [310/800]  eta: 0:17:50  loss: 0.1892 (0.2137)  time: 2.0844  data: 2.0726  max mem: 337
[18:57:10.872066] Test:  [310/800]  eta: 0:17:50  loss: 0.1899 (0.2144)  time: 2.0879  data: 2.0747  max mem: 337
[18:57:11.147993] Test:  [310/800]  eta: 0:17:50  loss: 0.1946 (0.2146)  time: 2.0907  data: 2.0771  max mem: 337
[18:57:13.514717] Test:  [310/800]  eta: 0:17:54  loss: 0.1943 (0.2143)  time: 2.1138  data: 2.1020  max mem: 337
[18:57:19.543199] Test:  [320/800]  eta: 0:17:08  loss: 0.2182 (0.2150)  time: 2.0334  data: 2.0200  max mem: 337
[18:57:19.551660] Test:  [320/800]  eta: 0:17:08  loss: 0.2219 (0.2152)  time: 2.0336  data: 2.0199  max mem: 337
[18:57:19.551853] Test:  [320/800]  eta: 0:17:08  loss: 0.2198 (0.2147)  time: 2.0340  data: 2.0203  max mem: 337
[18:57:19.802742] Test:  [320/800]  eta: 0:17:09  loss: 0.2204 (0.2150)  time: 2.0336  data: 2.0202  max mem: 337
[18:57:34.511443] Test:  [320/800]  eta: 0:17:31  loss: 0.2195 (0.2146)  time: 2.0735  data: 2.0618  max mem: 337
[18:57:34.522847] Test:  [320/800]  eta: 0:17:31  loss: 0.2217 (0.2150)  time: 2.0725  data: 2.0607  max mem: 337
[18:57:34.526437] Test:  [320/800]  eta: 0:17:31  loss: 0.2265 (0.2150)  time: 2.0751  data: 2.0632  max mem: 337
[18:57:35.129272] Test:  [320/800]  eta: 0:17:32  loss: 0.2221 (0.2149)  time: 2.0712  data: 2.0594  max mem: 337
[18:57:35.204889] Test:  [320/800]  eta: 0:17:32  loss: 0.2189 (0.2150)  time: 2.0745  data: 2.0628  max mem: 337
[18:57:35.217834] Test:  [320/800]  eta: 0:17:32  loss: 0.2173 (0.2151)  time: 2.0767  data: 2.0649  max mem: 337
[18:57:35.330899] Test:  [320/800]  eta: 0:17:32  loss: 0.2208 (0.2151)  time: 2.0813  data: 2.0676  max mem: 337
[18:57:35.522802] Test:  [320/800]  eta: 0:17:32  loss: 0.2193 (0.2149)  time: 2.0901  data: 2.0770  max mem: 337
[18:57:35.562979] Test:  [320/800]  eta: 0:17:32  loss: 0.2267 (0.2140)  time: 2.0784  data: 2.0665  max mem: 337
[18:57:35.596918] Test:  [320/800]  eta: 0:17:32  loss: 0.2139 (0.2148)  time: 2.0814  data: 2.0683  max mem: 337
[18:57:35.868751] Test:  [320/800]  eta: 0:17:33  loss: 0.2170 (0.2151)  time: 2.0829  data: 2.0693  max mem: 337
[18:57:38.582088] Test:  [320/800]  eta: 0:17:37  loss: 0.2235 (0.2147)  time: 2.1089  data: 2.0971  max mem: 337
[18:57:40.699782] Test:  [330/800]  eta: 0:16:47  loss: 0.2223 (0.2149)  time: 2.1088  data: 2.0955  max mem: 337
[18:57:40.702402] Test:  [330/800]  eta: 0:16:47  loss: 0.2215 (0.2145)  time: 2.1091  data: 2.0954  max mem: 337
[18:57:40.703627] Test:  [330/800]  eta: 0:16:47  loss: 0.2182 (0.2148)  time: 2.1090  data: 2.0956  max mem: 337
[18:57:40.704074] Test:  [330/800]  eta: 0:16:47  loss: 0.2225 (0.2150)  time: 2.1092  data: 2.0956  max mem: 337
[18:57:51.086217] Test:  [330/800]  eta: 0:17:01  loss: 0.2213 (0.2147)  time: 2.0561  data: 2.0444  max mem: 337
[18:57:51.093527] Test:  [330/800]  eta: 0:17:01  loss: 0.2228 (0.2148)  time: 2.0566  data: 2.0447  max mem: 337
[18:57:51.633288] Test:  [330/800]  eta: 0:17:02  loss: 0.2193 (0.2149)  time: 2.0525  data: 2.0407  max mem: 337
[18:57:51.653361] Test:  [330/800]  eta: 0:17:02  loss: 0.2235 (0.2148)  time: 2.0535  data: 2.0416  max mem: 337
[18:57:51.673511] Test:  [330/800]  eta: 0:17:02  loss: 0.2221 (0.2147)  time: 2.0550  data: 2.0432  max mem: 337
[18:57:51.989296] Test:  [330/800]  eta: 0:17:03  loss: 0.2208 (0.2149)  time: 2.0683  data: 2.0546  max mem: 337
[18:57:52.085773] Test:  [330/800]  eta: 0:17:03  loss: 0.2202 (0.2144)  time: 2.1095  data: 2.0978  max mem: 337
[18:57:52.127890] Test:  [330/800]  eta: 0:17:03  loss: 0.2193 (0.2147)  time: 2.0751  data: 2.0616  max mem: 337
[18:57:52.156405] Test:  [330/800]  eta: 0:17:03  loss: 0.2241 (0.2147)  time: 2.0642  data: 2.0507  max mem: 337
[18:57:52.376570] Test:  [330/800]  eta: 0:17:03  loss: 0.2180 (0.2148)  time: 2.0614  data: 2.0478  max mem: 337
[18:57:52.900919] Test:  [330/800]  eta: 0:17:04  loss: 0.2267 (0.2139)  time: 2.1032  data: 2.0914  max mem: 337
[18:57:55.374510] Test:  [330/800]  eta: 0:17:07  loss: 0.2235 (0.2145)  time: 2.0929  data: 2.0811  max mem: 337
[18:58:00.667619] Test:  [340/800]  eta: 0:16:23  loss: 0.2265 (0.2161)  time: 2.0557  data: 2.0422  max mem: 337
[18:58:00.671984] Test:  [340/800]  eta: 0:16:23  loss: 0.2270 (0.2159)  time: 2.0564  data: 2.0428  max mem: 337
[18:58:00.766480] Test:  [340/800]  eta: 0:16:23  loss: 0.2300 (0.2156)  time: 2.0607  data: 2.0471  max mem: 337
[18:58:00.811232] Test:  [340/800]  eta: 0:16:23  loss: 0.2288 (0.2160)  time: 2.0504  data: 2.0370  max mem: 337
[18:58:16.620076] Test:  [340/800]  eta: 0:16:45  loss: 0.2228 (0.2159)  time: 2.1048  data: 2.0932  max mem: 337
[18:58:16.660505] Test:  [340/800]  eta: 0:16:45  loss: 0.2279 (0.2156)  time: 2.1074  data: 2.0956  max mem: 337
[18:58:16.769453] Test:  [340/800]  eta: 0:16:45  loss: 0.2304 (0.2160)  time: 2.1121  data: 2.1002  max mem: 337
[18:58:17.123751] Test:  [340/800]  eta: 0:16:45  loss: 0.2311 (0.2159)  time: 2.0997  data: 2.0879  max mem: 337
[18:58:17.171696] Test:  [340/800]  eta: 0:16:45  loss: 0.2269 (0.2159)  time: 2.0983  data: 2.0864  max mem: 337
[18:58:17.175002] Test:  [340/800]  eta: 0:16:45  loss: 0.2241 (0.2160)  time: 2.0978  data: 2.0860  max mem: 337
[18:58:17.569187] Test:  [340/800]  eta: 0:16:46  loss: 0.2307 (0.2151)  time: 2.1003  data: 2.0885  max mem: 337
[18:58:17.730115] Test:  [340/800]  eta: 0:16:46  loss: 0.2305 (0.2158)  time: 2.1103  data: 2.0967  max mem: 337
[18:58:17.736187] Test:  [340/800]  eta: 0:16:46  loss: 0.2253 (0.2160)  time: 2.1202  data: 2.1065  max mem: 337
[18:58:17.809905] Test:  [340/800]  eta: 0:16:46  loss: 0.2268 (0.2158)  time: 2.1106  data: 2.0970  max mem: 337
[18:58:17.991958] Test:  [340/800]  eta: 0:16:47  loss: 0.2309 (0.2160)  time: 2.1061  data: 2.0925  max mem: 337
[18:58:21.196322] Test:  [340/800]  eta: 0:16:51  loss: 0.2261 (0.2156)  time: 2.1307  data: 2.1189  max mem: 337
[18:58:24.164266] Test:  [350/800]  eta: 0:16:04  loss: 0.1951 (0.2142)  time: 2.1730  data: 2.1595  max mem: 337
[18:58:24.208788] Test:  [350/800]  eta: 0:16:05  loss: 0.1935 (0.2146)  time: 2.1752  data: 2.1616  max mem: 337
[18:58:24.212448] Test:  [350/800]  eta: 0:16:05  loss: 0.1941 (0.2144)  time: 2.1754  data: 2.1618  max mem: 337
[18:58:24.213784] Test:  [350/800]  eta: 0:16:05  loss: 0.1837 (0.2146)  time: 2.1756  data: 2.1625  max mem: 337
[18:58:34.865053] Test:  [350/800]  eta: 0:16:18  loss: 0.1886 (0.2145)  time: 2.1244  data: 2.1108  max mem: 337
[18:58:34.876627] Test:  [350/800]  eta: 0:16:18  loss: 0.1960 (0.2144)  time: 2.1360  data: 2.1224  max mem: 337
[18:58:34.913686] Test:  [350/800]  eta: 0:16:18  loss: 0.1950 (0.2146)  time: 2.1462  data: 2.1325  max mem: 337
[18:58:35.154025] Test:  [350/800]  eta: 0:16:19  loss: 0.1941 (0.2144)  time: 2.2033  data: 2.1916  max mem: 337
[18:58:35.267344] Test:  [350/800]  eta: 0:16:19  loss: 0.1897 (0.2143)  time: 2.1569  data: 2.1433  max mem: 337
[18:58:35.369218] Test:  [350/800]  eta: 0:16:19  loss: 0.1968 (0.2146)  time: 2.2137  data: 2.2018  max mem: 337
[18:58:35.529841] Test:  [350/800]  eta: 0:16:19  loss: 0.1949 (0.2146)  time: 2.1948  data: 2.1830  max mem: 337
[18:58:35.546957] Test:  [350/800]  eta: 0:16:19  loss: 0.1919 (0.2145)  time: 2.1946  data: 2.1828  max mem: 337
[18:58:35.692340] Test:  [350/800]  eta: 0:16:19  loss: 0.1890 (0.2144)  time: 2.2009  data: 2.1891  max mem: 337
[18:58:36.129779] Test:  [350/800]  eta: 0:16:20  loss: 0.1871 (0.2142)  time: 2.2022  data: 2.1904  max mem: 337
[18:58:37.051377] Test:  [350/800]  eta: 0:16:21  loss: 0.1940 (0.2136)  time: 2.2075  data: 2.1957  max mem: 337
[18:58:38.392835] Test:  [350/800]  eta: 0:16:23  loss: 0.1884 (0.2142)  time: 2.1509  data: 2.1392  max mem: 337
[18:58:42.187037] Test:  [360/800]  eta: 0:15:39  loss: 0.1837 (0.2152)  time: 2.0687  data: 2.0554  max mem: 337
[18:58:42.193509] Test:  [360/800]  eta: 0:15:39  loss: 0.1962 (0.2152)  time: 2.0762  data: 2.0626  max mem: 337
[18:58:42.197220] Test:  [360/800]  eta: 0:15:39  loss: 0.1951 (0.2148)  time: 2.0715  data: 2.0578  max mem: 337
[18:58:42.242256] Test:  [360/800]  eta: 0:15:39  loss: 0.1941 (0.2151)  time: 2.0785  data: 2.0648  max mem: 337
[18:58:58.539955] Test:  [360/800]  eta: 0:15:59  loss: 0.1871 (0.2148)  time: 2.0939  data: 2.0822  max mem: 337
[18:58:58.597844] Test:  [360/800]  eta: 0:15:59  loss: 0.1941 (0.2150)  time: 2.0988  data: 2.0870  max mem: 337
[18:58:58.613482] Test:  [360/800]  eta: 0:15:59  loss: 0.1968 (0.2152)  time: 2.0922  data: 2.0803  max mem: 337
[18:58:58.909526] Test:  [360/800]  eta: 0:15:59  loss: 0.1919 (0.2151)  time: 2.0868  data: 2.0750  max mem: 337
[18:58:58.913119] Test:  [360/800]  eta: 0:15:59  loss: 0.1949 (0.2152)  time: 2.0869  data: 2.0750  max mem: 337
[18:58:59.034919] Test:  [360/800]  eta: 0:15:59  loss: 0.1890 (0.2150)  time: 2.0955  data: 2.0837  max mem: 337
[18:58:59.381229] Test:  [360/800]  eta: 0:16:00  loss: 0.1940 (0.2143)  time: 2.0906  data: 2.0787  max mem: 337
[18:58:59.606800] Test:  [360/800]  eta: 0:16:00  loss: 0.1897 (0.2150)  time: 2.0938  data: 2.0802  max mem: 337
[18:58:59.712101] Test:  [360/800]  eta: 0:16:00  loss: 0.1950 (0.2152)  time: 2.0987  data: 2.0856  max mem: 337
[18:58:59.806357] Test:  [360/800]  eta: 0:16:00  loss: 0.1960 (0.2150)  time: 2.0998  data: 2.0862  max mem: 337
[18:58:59.809313] Test:  [360/800]  eta: 0:16:00  loss: 0.1886 (0.2151)  time: 2.0908  data: 2.0772  max mem: 337
[18:59:03.585167] Test:  [360/800]  eta: 0:16:05  loss: 0.1884 (0.2148)  time: 2.1194  data: 2.1077  max mem: 337
[18:59:06.651554] Test:  [370/800]  eta: 0:15:21  loss: 0.2372 (0.2158)  time: 2.1218  data: 2.1083  max mem: 337
[18:59:06.693264] Test:  [370/800]  eta: 0:15:21  loss: 0.2393 (0.2159)  time: 2.1242  data: 2.1105  max mem: 337
[18:59:06.695478] Test:  [370/800]  eta: 0:15:21  loss: 0.2287 (0.2157)  time: 2.1241  data: 2.1105  max mem: 337
[18:59:06.698840] Test:  [370/800]  eta: 0:15:21  loss: 0.2335 (0.2155)  time: 2.1267  data: 2.1130  max mem: 337
[18:59:17.531034] Test:  [370/800]  eta: 0:15:34  loss: 0.2251 (0.2157)  time: 2.1333  data: 2.1196  max mem: 337
[18:59:17.568455] Test:  [370/800]  eta: 0:15:34  loss: 0.2321 (0.2157)  time: 2.1345  data: 2.1209  max mem: 337
[18:59:17.735386] Test:  [370/800]  eta: 0:15:34  loss: 0.2363 (0.2159)  time: 2.1410  data: 2.1278  max mem: 337
[18:59:17.837788] Test:  [370/800]  eta: 0:15:34  loss: 0.2354 (0.2157)  time: 2.1341  data: 2.1223  max mem: 337
[18:59:18.075574] Test:  [370/800]  eta: 0:15:34  loss: 0.2263 (0.2157)  time: 2.1404  data: 2.1270  max mem: 337
[18:59:18.145068] Test:  [370/800]  eta: 0:15:34  loss: 0.2344 (0.2159)  time: 2.1387  data: 2.1269  max mem: 337
[18:59:18.338188] Test:  [370/800]  eta: 0:15:35  loss: 0.2358 (0.2159)  time: 2.1404  data: 2.1286  max mem: 337
[18:59:18.346153] Test:  [370/800]  eta: 0:15:35  loss: 0.2314 (0.2158)  time: 2.1399  data: 2.1281  max mem: 337
[18:59:18.522233] Test:  [370/800]  eta: 0:15:35  loss: 0.2362 (0.2157)  time: 2.1414  data: 2.1296  max mem: 337
[18:59:18.990466] Test:  [370/800]  eta: 0:15:35  loss: 0.2257 (0.2155)  time: 2.1430  data: 2.1312  max mem: 337
[18:59:19.898338] Test:  [370/800]  eta: 0:15:37  loss: 0.2328 (0.2150)  time: 2.1423  data: 2.1305  max mem: 337
[18:59:20.900098] Test:  [370/800]  eta: 0:15:38  loss: 0.2293 (0.2156)  time: 2.1253  data: 2.1136  max mem: 337
[18:59:23.939122] Test:  [380/800]  eta: 0:14:55  loss: 0.2335 (0.2158)  time: 2.0870  data: 2.0734  max mem: 337
[18:59:23.945680] Test:  [380/800]  eta: 0:14:55  loss: 0.2393 (0.2162)  time: 2.0876  data: 2.0741  max mem: 337
[18:59:24.012164] Test:  [380/800]  eta: 0:14:55  loss: 0.2354 (0.2161)  time: 2.0884  data: 2.0749  max mem: 337
[18:59:24.012902] Test:  [380/800]  eta: 0:14:55  loss: 0.2372 (0.2161)  time: 2.0912  data: 2.0780  max mem: 337
[18:59:40.693453] Test:  [380/800]  eta: 0:15:14  loss: 0.2357 (0.2162)  time: 2.1040  data: 2.0921  max mem: 337
[18:59:40.693694] Test:  [380/800]  eta: 0:15:14  loss: 0.2354 (0.2160)  time: 2.1047  data: 2.0929  max mem: 337
[18:59:40.694290] Test:  [380/800]  eta: 0:15:14  loss: 0.2284 (0.2159)  time: 2.1077  data: 2.0960  max mem: 337
[18:59:40.994411] Test:  [380/800]  eta: 0:15:14  loss: 0.2328 (0.2161)  time: 2.1042  data: 2.0924  max mem: 337
[18:59:41.040147] Test:  [380/800]  eta: 0:15:14  loss: 0.2372 (0.2162)  time: 2.1063  data: 2.0945  max mem: 337
[18:59:41.043475] Test:  [380/800]  eta: 0:15:14  loss: 0.2362 (0.2161)  time: 2.1004  data: 2.0886  max mem: 337
[18:59:41.633915] Test:  [380/800]  eta: 0:15:15  loss: 0.2328 (0.2153)  time: 2.1126  data: 2.1007  max mem: 337
[18:59:41.865052] Test:  [380/800]  eta: 0:15:15  loss: 0.2280 (0.2160)  time: 2.1129  data: 2.0995  max mem: 337
[18:59:41.915032] Test:  [380/800]  eta: 0:15:15  loss: 0.2363 (0.2162)  time: 2.1101  data: 2.0965  max mem: 337
[18:59:41.965821] Test:  [380/800]  eta: 0:15:15  loss: 0.2321 (0.2160)  time: 2.1079  data: 2.0944  max mem: 337
[18:59:42.202072] Test:  [380/800]  eta: 0:15:15  loss: 0.2346 (0.2160)  time: 2.1196  data: 2.1061  max mem: 337
[18:59:46.482533] Test:  [380/800]  eta: 0:15:20  loss: 0.2318 (0.2159)  time: 2.1448  data: 2.1332  max mem: 337
[18:59:47.698802] Test:  [390/800]  eta: 0:14:36  loss: 0.2292 (0.2157)  time: 2.0501  data: 2.0366  max mem: 337
[18:59:47.811020] Test:  [390/800]  eta: 0:14:36  loss: 0.2304 (0.2159)  time: 2.0558  data: 2.0425  max mem: 337
[18:59:47.811220] Test:  [390/800]  eta: 0:14:36  loss: 0.2240 (0.2157)  time: 2.0579  data: 2.0447  max mem: 337
[18:59:47.811371] Test:  [390/800]  eta: 0:14:36  loss: 0.2294 (0.2154)  time: 2.0556  data: 2.0420  max mem: 337
[18:59:59.099793] Test:  [390/800]  eta: 0:14:48  loss: 0.2239 (0.2158)  time: 2.0682  data: 2.0550  max mem: 337
[18:59:59.166086] Test:  [390/800]  eta: 0:14:48  loss: 0.2273 (0.2157)  time: 2.0798  data: 2.0665  max mem: 337
[18:59:59.246263] Test:  [390/800]  eta: 0:14:48  loss: 0.2250 (0.2156)  time: 2.0704  data: 2.0587  max mem: 337
[18:59:59.338124] Test:  [390/800]  eta: 0:14:49  loss: 0.2284 (0.2157)  time: 2.0631  data: 2.0495  max mem: 337
[18:59:59.356272] Test:  [390/800]  eta: 0:14:49  loss: 0.2296 (0.2157)  time: 2.0912  data: 2.0781  max mem: 337
[18:59:59.443850] Test:  [390/800]  eta: 0:14:49  loss: 0.2256 (0.2159)  time: 2.0649  data: 2.0530  max mem: 337
[18:59:59.504851] Test:  [390/800]  eta: 0:14:49  loss: 0.2280 (0.2158)  time: 2.0583  data: 2.0465  max mem: 337
[18:59:59.514413] Test:  [390/800]  eta: 0:14:49  loss: 0.2328 (0.2157)  time: 2.0584  data: 2.0466  max mem: 337
[19:00:00.171157] Test:  [390/800]  eta: 0:14:49  loss: 0.2270 (0.2155)  time: 2.0590  data: 2.0473  max mem: 337
[19:00:00.191839] Test:  [390/800]  eta: 0:14:49  loss: 0.2292 (0.2158)  time: 2.0834  data: 2.0716  max mem: 337
[19:00:01.116860] Test:  [390/800]  eta: 0:14:50  loss: 0.2266 (0.2151)  time: 2.0609  data: 2.0491  max mem: 337
[19:00:03.918509] Test:  [390/800]  eta: 0:14:53  loss: 0.2279 (0.2155)  time: 2.1509  data: 2.1391  max mem: 337
[19:00:06.535363] Test:  [400/800]  eta: 0:14:12  loss: 0.2284 (0.2158)  time: 2.1261  data: 2.1125  max mem: 337
[19:00:06.538386] Test:  [400/800]  eta: 0:14:12  loss: 0.2240 (0.2158)  time: 2.1262  data: 2.1126  max mem: 337
[19:00:06.549854] Test:  [400/800]  eta: 0:14:12  loss: 0.2274 (0.2159)  time: 2.1302  data: 2.1166  max mem: 337
[19:00:06.651634] Test:  [400/800]  eta: 0:14:13  loss: 0.2294 (0.2155)  time: 2.1356  data: 2.1220  max mem: 337
[19:00:23.223777] Test:  [400/800]  eta: 0:14:29  loss: 0.2249 (0.2159)  time: 2.1265  data: 2.1146  max mem: 337
[19:00:23.254213] Test:  [400/800]  eta: 0:14:29  loss: 0.2210 (0.2157)  time: 2.1280  data: 2.1163  max mem: 337
[19:00:23.259956] Test:  [400/800]  eta: 0:14:29  loss: 0.2266 (0.2156)  time: 2.1282  data: 2.1166  max mem: 337
[19:00:23.566779] Test:  [400/800]  eta: 0:14:29  loss: 0.2230 (0.2158)  time: 2.1261  data: 2.1143  max mem: 337
[19:00:23.569878] Test:  [400/800]  eta: 0:14:29  loss: 0.2227 (0.2158)  time: 2.1264  data: 2.1146  max mem: 337
[19:00:23.580338] Test:  [400/800]  eta: 0:14:29  loss: 0.2249 (0.2158)  time: 2.1293  data: 2.1174  max mem: 337
[19:00:24.339267] Test:  [400/800]  eta: 0:14:30  loss: 0.2266 (0.2152)  time: 2.1352  data: 2.1235  max mem: 337
[19:00:24.560691] Test:  [400/800]  eta: 0:14:30  loss: 0.2239 (0.2158)  time: 2.1322  data: 2.1189  max mem: 337
[19:00:24.597601] Test:  [400/800]  eta: 0:14:30  loss: 0.2244 (0.2158)  time: 2.1315  data: 2.1181  max mem: 337
[19:00:24.613159] Test:  [400/800]  eta: 0:14:30  loss: 0.2271 (0.2158)  time: 2.1374  data: 2.1237  max mem: 337
[19:00:24.835978] Test:  [400/800]  eta: 0:14:31  loss: 0.2225 (0.2157)  time: 2.1317  data: 2.1184  max mem: 337
[19:00:29.702797] Test:  [400/800]  eta: 0:14:36  loss: 0.2274 (0.2156)  time: 2.1610  data: 2.1493  max mem: 337
[19:00:30.184366] Test:  [410/800]  eta: 0:13:53  loss: 0.2160 (0.2160)  time: 2.1186  data: 2.1051  max mem: 337
[19:00:30.192464] Test:  [410/800]  eta: 0:13:53  loss: 0.2173 (0.2160)  time: 2.1246  data: 2.1112  max mem: 337
[19:00:30.197822] Test:  [410/800]  eta: 0:13:53  loss: 0.2146 (0.2162)  time: 2.1193  data: 2.1059  max mem: 337
[19:00:30.219439] Test:  [410/800]  eta: 0:13:53  loss: 0.2172 (0.2157)  time: 2.1204  data: 2.1070  max mem: 337
[19:00:42.326408] Test:  [410/800]  eta: 0:14:05  loss: 0.2180 (0.2160)  time: 2.1580  data: 2.1445  max mem: 337
[19:00:42.354666] Test:  [410/800]  eta: 0:14:05  loss: 0.2187 (0.2160)  time: 2.1499  data: 2.1365  max mem: 337
[19:00:42.420759] Test:  [410/800]  eta: 0:14:05  loss: 0.2205 (0.2160)  time: 2.1541  data: 2.1405  max mem: 337
[19:00:42.612154] Test:  [410/800]  eta: 0:14:05  loss: 0.2238 (0.2160)  time: 2.1756  data: 2.1623  max mem: 337
[19:00:42.628377] Test:  [410/800]  eta: 0:14:05  loss: 0.2156 (0.2161)  time: 2.1561  data: 2.1444  max mem: 337
[19:00:42.633620] Test:  [410/800]  eta: 0:14:05  loss: 0.2178 (0.2161)  time: 2.1559  data: 2.1441  max mem: 337
[19:00:42.635734] Test:  [410/800]  eta: 0:14:05  loss: 0.2225 (0.2162)  time: 2.1595  data: 2.1476  max mem: 337
[19:00:42.847010] Test:  [410/800]  eta: 0:14:05  loss: 0.2193 (0.2159)  time: 2.1800  data: 2.1682  max mem: 337
[19:00:43.318703] Test:  [410/800]  eta: 0:14:06  loss: 0.2205 (0.2159)  time: 2.1573  data: 2.1456  max mem: 337
[19:00:43.767578] Test:  [410/800]  eta: 0:14:06  loss: 0.2204 (0.2161)  time: 2.1787  data: 2.1669  max mem: 337
[19:00:44.198279] Test:  [410/800]  eta: 0:14:07  loss: 0.2191 (0.2154)  time: 2.1540  data: 2.1422  max mem: 337
[19:00:47.232741] Test:  [410/800]  eta: 0:14:10  loss: 0.2157 (0.2158)  time: 2.1657  data: 2.1540  max mem: 337
[19:00:48.945576] Test:  [420/800]  eta: 0:13:30  loss: 0.2146 (0.2168)  time: 2.1197  data: 2.1063  max mem: 337
[19:00:48.965678] Test:  [420/800]  eta: 0:13:30  loss: 0.2172 (0.2162)  time: 2.1157  data: 2.1022  max mem: 337
[19:00:48.967828] Test:  [420/800]  eta: 0:13:30  loss: 0.2160 (0.2165)  time: 2.1214  data: 2.1079  max mem: 337
[19:00:48.970457] Test:  [420/800]  eta: 0:13:30  loss: 0.2173 (0.2165)  time: 2.1217  data: 2.1083  max mem: 337
[19:01:05.107506] Test:  [420/800]  eta: 0:13:44  loss: 0.2205 (0.2164)  time: 2.0923  data: 2.0805  max mem: 337
[19:01:05.190141] Test:  [420/800]  eta: 0:13:44  loss: 0.2225 (0.2167)  time: 2.0983  data: 2.0864  max mem: 337
[19:01:05.191457] Test:  [420/800]  eta: 0:13:44  loss: 0.2193 (0.2164)  time: 2.0968  data: 2.0851  max mem: 337
[19:01:05.373423] Test:  [420/800]  eta: 0:13:44  loss: 0.2156 (0.2166)  time: 2.0901  data: 2.0784  max mem: 337
[19:01:05.384241] Test:  [420/800]  eta: 0:13:44  loss: 0.2204 (0.2166)  time: 2.0908  data: 2.0790  max mem: 337
[19:01:05.427281] Test:  [420/800]  eta: 0:13:44  loss: 0.2178 (0.2166)  time: 2.0923  data: 2.0805  max mem: 337
[19:01:06.370119] Test:  [420/800]  eta: 0:13:45  loss: 0.2191 (0.2159)  time: 2.1015  data: 2.0897  max mem: 337
[19:01:06.632859] Test:  [420/800]  eta: 0:13:46  loss: 0.2180 (0.2165)  time: 2.1017  data: 2.0882  max mem: 337
[19:01:06.633472] Test:  [420/800]  eta: 0:13:46  loss: 0.2238 (0.2166)  time: 2.1036  data: 2.0904  max mem: 337
[19:01:06.658376] Test:  [420/800]  eta: 0:13:46  loss: 0.2205 (0.2165)  time: 2.1022  data: 2.0886  max mem: 337
[19:01:06.877627] Test:  [420/800]  eta: 0:13:46  loss: 0.2187 (0.2164)  time: 2.1020  data: 2.0887  max mem: 337
[19:01:12.198501] Test:  [420/800]  eta: 0:13:51  loss: 0.2157 (0.2163)  time: 2.1247  data: 2.1129  max mem: 337
[19:01:12.686290] Test:  [430/800]  eta: 0:13:10  loss: 0.2057 (0.2168)  time: 2.1246  data: 2.1110  max mem: 337
[19:01:12.694704] Test:  [430/800]  eta: 0:13:10  loss: 0.2028 (0.2168)  time: 2.1254  data: 2.1118  max mem: 337
[19:01:12.757555] Test:  [430/800]  eta: 0:13:10  loss: 0.2058 (0.2165)  time: 2.1268  data: 2.1132  max mem: 337
[19:01:12.758820] Test:  [430/800]  eta: 0:13:10  loss: 0.2073 (0.2171)  time: 2.1280  data: 2.1143  max mem: 337
[19:01:24.592202] Test:  [430/800]  eta: 0:13:21  loss: 0.2096 (0.2167)  time: 2.0872  data: 2.0755  max mem: 337
[19:01:24.643753] Test:  [430/800]  eta: 0:13:21  loss: 0.2091 (0.2170)  time: 2.1003  data: 2.0884  max mem: 337
[19:01:24.796405] Test:  [430/800]  eta: 0:13:21  loss: 0.2047 (0.2169)  time: 2.1083  data: 2.0965  max mem: 337
[19:01:24.796784] Test:  [430/800]  eta: 0:13:21  loss: 0.2041 (0.2169)  time: 2.1081  data: 2.0962  max mem: 337
[19:01:24.936704] Test:  [430/800]  eta: 0:13:21  loss: 0.2063 (0.2168)  time: 2.1257  data: 2.1121  max mem: 337
[19:01:24.961150] Test:  [430/800]  eta: 0:13:21  loss: 0.2077 (0.2168)  time: 2.1174  data: 2.1038  max mem: 337
[19:01:25.083230] Test:  [430/800]  eta: 0:13:21  loss: 0.2084 (0.2168)  time: 2.1378  data: 2.1242  max mem: 337
[19:01:25.218034] Test:  [430/800]  eta: 0:13:21  loss: 0.2078 (0.2168)  time: 2.1431  data: 2.1296  max mem: 337
[19:01:25.648780] Test:  [430/800]  eta: 0:13:21  loss: 0.2088 (0.2169)  time: 2.0940  data: 2.0822  max mem: 337
[19:01:25.764789] Test:  [430/800]  eta: 0:13:22  loss: 0.2088 (0.2167)  time: 2.1222  data: 2.1104  max mem: 337
[19:01:26.622114] Test:  [430/800]  eta: 0:13:22  loss: 0.2104 (0.2162)  time: 2.1211  data: 2.1094  max mem: 337
[19:01:30.745182] Test:  [430/800]  eta: 0:13:26  loss: 0.2116 (0.2166)  time: 2.1756  data: 2.1637  max mem: 337
[19:01:33.046246] Test:  [440/800]  eta: 0:12:48  loss: 0.2419 (0.2172)  time: 2.2040  data: 2.1903  max mem: 337
[19:01:33.049474] Test:  [440/800]  eta: 0:12:48  loss: 0.2384 (0.2178)  time: 2.2051  data: 2.1915  max mem: 337
[19:01:33.061923] Test:  [440/800]  eta: 0:12:48  loss: 0.2425 (0.2175)  time: 2.2046  data: 2.1913  max mem: 337
[19:01:33.077907] Test:  [440/800]  eta: 0:12:48  loss: 0.2417 (0.2176)  time: 2.2053  data: 2.1917  max mem: 337
[19:01:49.257237] Test:  [440/800]  eta: 0:13:01  loss: 0.2362 (0.2174)  time: 2.2032  data: 2.1914  max mem: 337
[19:01:49.265685] Test:  [440/800]  eta: 0:13:01  loss: 0.2422 (0.2176)  time: 2.2037  data: 2.1919  max mem: 337
[19:01:49.272445] Test:  [440/800]  eta: 0:13:01  loss: 0.2429 (0.2174)  time: 2.2082  data: 2.1964  max mem: 337
[19:01:49.482507] Test:  [440/800]  eta: 0:13:02  loss: 0.2406 (0.2176)  time: 2.2027  data: 2.1910  max mem: 337
[19:01:49.491580] Test:  [440/800]  eta: 0:13:02  loss: 0.2439 (0.2176)  time: 2.2053  data: 2.1935  max mem: 337
[19:01:49.512157] Test:  [440/800]  eta: 0:13:02  loss: 0.2420 (0.2176)  time: 2.2069  data: 2.1952  max mem: 337
[19:01:50.755591] Test:  [440/800]  eta: 0:13:03  loss: 0.2417 (0.2169)  time: 2.2192  data: 2.2073  max mem: 337
[19:01:50.947239] Test:  [440/800]  eta: 0:13:03  loss: 0.2446 (0.2175)  time: 2.2144  data: 2.2008  max mem: 337
[19:01:50.996173] Test:  [440/800]  eta: 0:13:03  loss: 0.2416 (0.2175)  time: 2.2181  data: 2.2049  max mem: 337
[19:01:51.002765] Test:  [440/800]  eta: 0:13:03  loss: 0.2363 (0.2174)  time: 2.2184  data: 2.2050  max mem: 337
[19:01:51.186374] Test:  [440/800]  eta: 0:13:03  loss: 0.2409 (0.2175)  time: 2.2154  data: 2.2023  max mem: 337
[19:01:55.164594] Test:  [450/800]  eta: 0:12:27  loss: 0.2437 (0.2174)  time: 2.1203  data: 2.1066  max mem: 337
[19:01:55.169561] Test:  [450/800]  eta: 0:12:27  loss: 0.2473 (0.2177)  time: 2.1237  data: 2.1103  max mem: 337
[19:01:55.174053] Test:  [450/800]  eta: 0:12:27  loss: 0.2492 (0.2177)  time: 2.1243  data: 2.1107  max mem: 337
[19:01:55.202290] Test:  [450/800]  eta: 0:12:27  loss: 0.2384 (0.2179)  time: 2.1221  data: 2.1085  max mem: 337
[19:01:57.014773] Test:  [440/800]  eta: 0:13:08  loss: 0.2430 (0.2173)  time: 2.2407  data: 2.2289  max mem: 337
[19:02:06.901927] Test:  [450/800]  eta: 0:12:36  loss: 0.2461 (0.2177)  time: 2.1052  data: 2.0935  max mem: 337
[19:02:06.957280] Test:  [450/800]  eta: 0:12:37  loss: 0.2419 (0.2177)  time: 2.1080  data: 2.0963  max mem: 337
[19:02:06.968412] Test:  [450/800]  eta: 0:12:37  loss: 0.2473 (0.2178)  time: 2.1162  data: 2.1045  max mem: 337
[19:02:07.201149] Test:  [450/800]  eta: 0:12:37  loss: 0.2424 (0.2175)  time: 2.1304  data: 2.1187  max mem: 337
[19:02:07.628874] Test:  [450/800]  eta: 0:12:37  loss: 0.2429 (0.2175)  time: 2.0932  data: 2.0814  max mem: 337
[19:02:08.186458] Test:  [450/800]  eta: 0:12:37  loss: 0.2446 (0.2177)  time: 2.1624  data: 2.1488  max mem: 337
[19:02:08.224235] Test:  [450/800]  eta: 0:12:38  loss: 0.2427 (0.2176)  time: 2.1570  data: 2.1436  max mem: 337
[19:02:08.247688] Test:  [450/800]  eta: 0:12:38  loss: 0.2486 (0.2177)  time: 2.1643  data: 2.1511  max mem: 337
[19:02:08.252522] Test:  [450/800]  eta: 0:12:38  loss: 0.2439 (0.2177)  time: 2.1301  data: 2.1183  max mem: 337
[19:02:08.463904] Test:  [450/800]  eta: 0:12:38  loss: 0.2429 (0.2176)  time: 2.1622  data: 2.1491  max mem: 337
[19:02:08.580202] Test:  [450/800]  eta: 0:12:38  loss: 0.2446 (0.2170)  time: 2.0979  data: 2.0860  max mem: 337
[19:02:14.557282] Test:  [450/800]  eta: 0:12:42  loss: 0.2462 (0.2175)  time: 2.1906  data: 2.1787  max mem: 337
[19:02:16.436356] Test:  [460/800]  eta: 0:12:06  loss: 0.2050 (0.2173)  time: 2.1695  data: 2.1558  max mem: 337
[19:02:16.444569] Test:  [460/800]  eta: 0:12:06  loss: 0.2101 (0.2176)  time: 2.1691  data: 2.1555  max mem: 337
[19:02:16.445694] Test:  [460/800]  eta: 0:12:06  loss: 0.2109 (0.2177)  time: 2.1683  data: 2.1547  max mem: 337
[19:02:16.446226] Test:  [460/800]  eta: 0:12:06  loss: 0.2053 (0.2178)  time: 2.1698  data: 2.1561  max mem: 337
[19:02:32.512854] Test:  [460/800]  eta: 0:12:18  loss: 0.2053 (0.2177)  time: 2.1623  data: 2.1506  max mem: 337
[19:02:32.514807] Test:  [460/800]  eta: 0:12:18  loss: 0.2014 (0.2174)  time: 2.1628  data: 2.1511  max mem: 337
[19:02:32.560575] Test:  [460/800]  eta: 0:12:18  loss: 0.2060 (0.2175)  time: 2.1644  data: 2.1525  max mem: 337
[19:02:32.851815] Test:  [460/800]  eta: 0:12:18  loss: 0.2066 (0.2177)  time: 2.1680  data: 2.1562  max mem: 337
[19:02:32.857038] Test:  [460/800]  eta: 0:12:18  loss: 0.2037 (0.2177)  time: 2.1672  data: 2.1554  max mem: 337
[19:02:32.921296] Test:  [460/800]  eta: 0:12:18  loss: 0.2036 (0.2177)  time: 2.1719  data: 2.1601  max mem: 337
[19:02:34.115768] Test:  [460/800]  eta: 0:12:19  loss: 0.2023 (0.2169)  time: 2.1680  data: 2.1561  max mem: 337
[19:02:34.243074] Test:  [460/800]  eta: 0:12:19  loss: 0.2086 (0.2176)  time: 2.1647  data: 2.1511  max mem: 337
[19:02:34.258759] Test:  [460/800]  eta: 0:12:19  loss: 0.2003 (0.2176)  time: 2.1631  data: 2.1494  max mem: 337
[19:02:34.259436] Test:  [460/800]  eta: 0:12:19  loss: 0.2040 (0.2174)  time: 2.1628  data: 2.1492  max mem: 337
[19:02:34.465469] Test:  [460/800]  eta: 0:12:19  loss: 0.2040 (0.2175)  time: 2.1639  data: 2.1503  max mem: 337
[19:02:37.858035] Test:  [470/800]  eta: 0:11:45  loss: 0.2101 (0.2181)  time: 2.1344  data: 2.1208  max mem: 337
[19:02:37.865970] Test:  [470/800]  eta: 0:11:45  loss: 0.2109 (0.2183)  time: 2.1345  data: 2.1209  max mem: 337
[19:02:37.871416] Test:  [470/800]  eta: 0:11:45  loss: 0.2053 (0.2183)  time: 2.1334  data: 2.1198  max mem: 337
[19:02:37.879741] Test:  [470/800]  eta: 0:11:45  loss: 0.2050 (0.2179)  time: 2.1357  data: 2.1220  max mem: 337
[19:02:40.857022] Test:  [460/800]  eta: 0:12:24  loss: 0.2065 (0.2174)  time: 2.1921  data: 2.1802  max mem: 337
[19:02:49.681045] Test:  [470/800]  eta: 0:11:53  loss: 0.2053 (0.2183)  time: 2.1356  data: 2.1238  max mem: 337
[19:02:49.916491] Test:  [470/800]  eta: 0:11:53  loss: 0.2014 (0.2180)  time: 2.1357  data: 2.1240  max mem: 337
[19:02:49.979497] Test:  [470/800]  eta: 0:11:53  loss: 0.2036 (0.2183)  time: 2.1511  data: 2.1394  max mem: 337
[19:02:49.979716] Test:  [470/800]  eta: 0:11:53  loss: 0.2037 (0.2183)  time: 2.1538  data: 2.1422  max mem: 337
[19:02:50.460415] Test:  [470/800]  eta: 0:11:53  loss: 0.2003 (0.2182)  time: 2.1106  data: 2.0974  max mem: 337
[19:02:50.483665] Test:  [470/800]  eta: 0:11:53  loss: 0.2086 (0.2182)  time: 2.1148  data: 2.1012  max mem: 337
[19:02:50.495698] Test:  [470/800]  eta: 0:11:53  loss: 0.2040 (0.2180)  time: 2.1135  data: 2.0999  max mem: 337
[19:02:50.715576] Test:  [470/800]  eta: 0:11:54  loss: 0.2060 (0.2181)  time: 2.1543  data: 2.1426  max mem: 337
[19:02:50.890188] Test:  [470/800]  eta: 0:11:54  loss: 0.2040 (0.2181)  time: 2.1213  data: 2.1082  max mem: 337
[19:02:51.184093] Test:  [470/800]  eta: 0:11:54  loss: 0.2066 (0.2183)  time: 2.1465  data: 2.1349  max mem: 337
[19:02:51.885057] Test:  [470/800]  eta: 0:11:54  loss: 0.2023 (0.2175)  time: 2.1652  data: 2.1533  max mem: 337
[19:02:57.389716] Test:  [470/800]  eta: 0:11:58  loss: 0.2065 (0.2180)  time: 2.1416  data: 2.1298  max mem: 337
[19:02:59.436081] Test:  [480/800]  eta: 0:11:23  loss: 0.2063 (0.2182)  time: 2.1494  data: 2.1358  max mem: 337
[19:02:59.447508] Test:  [480/800]  eta: 0:11:23  loss: 0.2042 (0.2178)  time: 2.1505  data: 2.1368  max mem: 337
[19:02:59.463422] Test:  [480/800]  eta: 0:11:23  loss: 0.2016 (0.2181)  time: 2.1508  data: 2.1372  max mem: 337
[19:02:59.496480] Test:  [480/800]  eta: 0:11:23  loss: 0.2017 (0.2180)  time: 2.1525  data: 2.1389  max mem: 337
[19:03:14.867256] Test:  [480/800]  eta: 0:11:34  loss: 0.2046 (0.2179)  time: 2.1176  data: 2.1059  max mem: 337
[19:03:14.875369] Test:  [480/800]  eta: 0:11:34  loss: 0.2017 (0.2181)  time: 2.1181  data: 2.1063  max mem: 337
[19:03:15.081081] Test:  [480/800]  eta: 0:11:34  loss: 0.1991 (0.2179)  time: 2.1260  data: 2.1142  max mem: 337
[19:03:15.139278] Test:  [480/800]  eta: 0:11:34  loss: 0.2082 (0.2181)  time: 2.1108  data: 2.0991  max mem: 337
[19:03:15.159449] Test:  [480/800]  eta: 0:11:34  loss: 0.2036 (0.2181)  time: 2.1151  data: 2.1034  max mem: 337
[19:03:15.165576] Test:  [480/800]  eta: 0:11:34  loss: 0.2107 (0.2181)  time: 2.1156  data: 2.1040  max mem: 337
[19:03:16.507838] Test:  [480/800]  eta: 0:11:35  loss: 0.2078 (0.2181)  time: 2.1124  data: 2.0992  max mem: 337
[19:03:16.553874] Test:  [480/800]  eta: 0:11:35  loss: 0.2044 (0.2179)  time: 2.1147  data: 2.1011  max mem: 337
[19:03:16.573176] Test:  [480/800]  eta: 0:11:35  loss: 0.2105 (0.2180)  time: 2.1165  data: 2.1028  max mem: 337
[19:03:16.725223] Test:  [480/800]  eta: 0:11:35  loss: 0.2045 (0.2174)  time: 2.1304  data: 2.1186  max mem: 337
[19:03:17.042497] Test:  [480/800]  eta: 0:11:35  loss: 0.2064 (0.2180)  time: 2.1288  data: 2.1157  max mem: 337
[19:03:21.234127] Test:  [490/800]  eta: 0:11:02  loss: 0.2064 (0.2178)  time: 2.1677  data: 2.1539  max mem: 337
[19:03:21.238048] Test:  [490/800]  eta: 0:11:02  loss: 0.2090 (0.2182)  time: 2.1683  data: 2.1546  max mem: 337
[19:03:21.284525] Test:  [490/800]  eta: 0:11:02  loss: 0.2039 (0.2180)  time: 2.1713  data: 2.1577  max mem: 337
[19:03:21.299950] Test:  [490/800]  eta: 0:11:02  loss: 0.2064 (0.2182)  time: 2.1716  data: 2.1580  max mem: 337
[19:03:23.843587] Test:  [480/800]  eta: 0:11:40  loss: 0.2111 (0.2179)  time: 2.1493  data: 2.1375  max mem: 337
[19:03:33.459579] Test:  [490/800]  eta: 0:11:10  loss: 0.2119 (0.2182)  time: 2.1889  data: 2.1770  max mem: 337
[19:03:33.467528] Test:  [490/800]  eta: 0:11:10  loss: 0.2075 (0.2179)  time: 2.1775  data: 2.1657  max mem: 337
[19:03:33.507229] Test:  [490/800]  eta: 0:11:10  loss: 0.2072 (0.2180)  time: 2.1505  data: 2.1369  max mem: 337
[19:03:33.556029] Test:  [490/800]  eta: 0:11:10  loss: 0.2107 (0.2182)  time: 2.1547  data: 2.1410  max mem: 337
[19:03:34.061155] Test:  [490/800]  eta: 0:11:10  loss: 0.2103 (0.2181)  time: 2.1585  data: 2.1449  max mem: 337
[19:03:34.076415] Test:  [490/800]  eta: 0:11:10  loss: 0.2047 (0.2182)  time: 2.2048  data: 2.1931  max mem: 337
[19:03:34.079101] Test:  [490/800]  eta: 0:11:10  loss: 0.2082 (0.2182)  time: 2.2049  data: 2.1932  max mem: 337
[19:03:34.131790] Test:  [490/800]  eta: 0:11:10  loss: 0.2086 (0.2181)  time: 2.1824  data: 2.1687  max mem: 337
[19:03:34.850735] Test:  [490/800]  eta: 0:11:11  loss: 0.2107 (0.2182)  time: 2.1833  data: 2.1715  max mem: 337
[19:03:34.890843] Test:  [490/800]  eta: 0:11:11  loss: 0.2114 (0.2180)  time: 2.2087  data: 2.1969  max mem: 337
[19:03:36.100537] Test:  [490/800]  eta: 0:11:12  loss: 0.2045 (0.2174)  time: 2.2107  data: 2.1989  max mem: 337
[19:03:41.047568] Test:  [490/800]  eta: 0:11:15  loss: 0.2111 (0.2180)  time: 2.1828  data: 2.1710  max mem: 337
[19:03:41.748985] Test:  [500/800]  eta: 0:10:40  loss: 0.2045 (0.2178)  time: 2.1156  data: 2.1019  max mem: 337
[19:03:41.786272] Test:  [500/800]  eta: 0:10:40  loss: 0.2064 (0.2175)  time: 2.1169  data: 2.1032  max mem: 337
[19:03:41.789910] Test:  [500/800]  eta: 0:10:40  loss: 0.1981 (0.2178)  time: 2.1163  data: 2.1027  max mem: 337
[19:03:41.790074] Test:  [500/800]  eta: 0:10:40  loss: 0.1989 (0.2177)  time: 2.1146  data: 2.1010  max mem: 337
[19:03:57.514842] Test:  [500/800]  eta: 0:10:50  loss: 0.1993 (0.2179)  time: 2.1319  data: 2.1202  max mem: 337
[19:03:57.534146] Test:  [500/800]  eta: 0:10:50  loss: 0.2004 (0.2176)  time: 2.1333  data: 2.1215  max mem: 337
[19:03:57.676521] Test:  [500/800]  eta: 0:10:50  loss: 0.1990 (0.2179)  time: 2.1255  data: 2.1137  max mem: 337
[19:03:57.690466] Test:  [500/800]  eta: 0:10:50  loss: 0.1984 (0.2178)  time: 2.1275  data: 2.1158  max mem: 337
[19:03:57.725360] Test:  [500/800]  eta: 0:10:50  loss: 0.1962 (0.2179)  time: 2.1282  data: 2.1166  max mem: 337
[19:03:57.773864] Test:  [500/800]  eta: 0:10:50  loss: 0.1993 (0.2176)  time: 2.1346  data: 2.1229  max mem: 337
[19:03:59.143802] Test:  [500/800]  eta: 0:10:51  loss: 0.1995 (0.2176)  time: 2.1294  data: 2.1162  max mem: 337
[19:03:59.153798] Test:  [500/800]  eta: 0:10:51  loss: 0.1993 (0.2178)  time: 2.1322  data: 2.1190  max mem: 337
[19:03:59.331606] Test:  [500/800]  eta: 0:10:51  loss: 0.2009 (0.2177)  time: 2.1379  data: 2.1243  max mem: 337
[19:03:59.393072] Test:  [500/800]  eta: 0:10:51  loss: 0.2008 (0.2171)  time: 2.1333  data: 2.1215  max mem: 337
[19:03:59.782389] Test:  [500/800]  eta: 0:10:51  loss: 0.2003 (0.2177)  time: 2.1370  data: 2.1238  max mem: 337
[19:04:02.932936] Test:  [510/800]  eta: 0:10:19  loss: 0.2181 (0.2186)  time: 2.0824  data: 2.0688  max mem: 337
[19:04:02.935255] Test:  [510/800]  eta: 0:10:19  loss: 0.2161 (0.2184)  time: 2.0850  data: 2.0714  max mem: 337
[19:04:02.959073] Test:  [510/800]  eta: 0:10:19  loss: 0.2197 (0.2188)  time: 2.0860  data: 2.0723  max mem: 337
[19:04:02.959168] Test:  [510/800]  eta: 0:10:19  loss: 0.2160 (0.2187)  time: 2.0829  data: 2.0693  max mem: 337
[19:04:06.990080] Test:  [500/800]  eta: 0:10:56  loss: 0.1977 (0.2176)  time: 2.1573  data: 2.1456  max mem: 337
[19:04:15.233997] Test:  [510/800]  eta: 0:10:26  loss: 0.2193 (0.2187)  time: 2.0887  data: 2.0770  max mem: 337
[19:04:15.460636] Test:  [510/800]  eta: 0:10:26  loss: 0.2180 (0.2185)  time: 2.0996  data: 2.0878  max mem: 337
[19:04:15.983630] Test:  [510/800]  eta: 0:10:26  loss: 0.2196 (0.2188)  time: 2.0953  data: 2.0835  max mem: 337
[19:04:15.989171] Test:  [510/800]  eta: 0:10:26  loss: 0.2154 (0.2187)  time: 2.0955  data: 2.0836  max mem: 337
[19:04:16.074434] Test:  [510/800]  eta: 0:10:26  loss: 0.2151 (0.2187)  time: 2.1259  data: 2.1127  max mem: 337
[19:04:16.077055] Test:  [510/800]  eta: 0:10:26  loss: 0.2141 (0.2185)  time: 2.1284  data: 2.1152  max mem: 337
[19:04:16.229504] Test:  [510/800]  eta: 0:10:26  loss: 0.2176 (0.2186)  time: 2.1048  data: 2.0912  max mem: 337
[19:04:16.596337] Test:  [510/800]  eta: 0:10:27  loss: 0.2146 (0.2187)  time: 2.1267  data: 2.1136  max mem: 337
[19:04:16.803052] Test:  [510/800]  eta: 0:10:27  loss: 0.2209 (0.2185)  time: 2.0956  data: 2.0838  max mem: 337
[19:04:17.106802] Test:  [510/800]  eta: 0:10:27  loss: 0.2173 (0.2188)  time: 2.1128  data: 2.1010  max mem: 337
[19:04:18.070090] Test:  [510/800]  eta: 0:10:28  loss: 0.2184 (0.2180)  time: 2.0984  data: 2.0866  max mem: 337
[19:04:23.229464] Test:  [520/800]  eta: 0:09:57  loss: 0.2414 (0.2186)  time: 2.0719  data: 2.0585  max mem: 337
[19:04:23.234547] Test:  [520/800]  eta: 0:09:57  loss: 0.2345 (0.2187)  time: 2.0742  data: 2.0608  max mem: 337
[19:04:23.327829] Test:  [520/800]  eta: 0:09:57  loss: 0.2449 (0.2183)  time: 2.0770  data: 2.0636  max mem: 337
[19:04:23.331504] Test:  [520/800]  eta: 0:09:57  loss: 0.2367 (0.2187)  time: 2.0770  data: 2.0636  max mem: 337
[19:04:24.108299] Test:  [510/800]  eta: 0:10:31  loss: 0.2175 (0.2185)  time: 2.1530  data: 2.1413  max mem: 337
[19:04:38.986884] Test:  [520/800]  eta: 0:10:05  loss: 0.2392 (0.2185)  time: 2.0726  data: 2.0608  max mem: 337
[19:04:38.994121] Test:  [520/800]  eta: 0:10:05  loss: 0.2372 (0.2187)  time: 2.0739  data: 2.0620  max mem: 337
[19:04:39.268276] Test:  [520/800]  eta: 0:10:06  loss: 0.2466 (0.2188)  time: 2.0795  data: 2.0678  max mem: 337
[19:04:39.293115] Test:  [520/800]  eta: 0:10:06  loss: 0.2389 (0.2187)  time: 2.0783  data: 2.0665  max mem: 337
[19:04:39.294363] Test:  [520/800]  eta: 0:10:06  loss: 0.2413 (0.2188)  time: 2.0801  data: 2.0684  max mem: 337
[19:04:39.432004] Test:  [520/800]  eta: 0:10:06  loss: 0.2419 (0.2185)  time: 2.0829  data: 2.0710  max mem: 337
[19:04:40.812849] Test:  [520/800]  eta: 0:10:06  loss: 0.2441 (0.2187)  time: 2.0829  data: 2.0692  max mem: 337
[19:04:40.884010] Test:  [520/800]  eta: 0:10:06  loss: 0.2424 (0.2185)  time: 2.0870  data: 2.0735  max mem: 337
[19:04:41.056612] Test:  [520/800]  eta: 0:10:07  loss: 0.2411 (0.2186)  time: 2.0862  data: 2.0727  max mem: 337
[19:04:41.092944] Test:  [520/800]  eta: 0:10:07  loss: 0.2391 (0.2180)  time: 2.0849  data: 2.0732  max mem: 337
[19:04:41.420266] Test:  [520/800]  eta: 0:10:07  loss: 0.2368 (0.2186)  time: 2.0818  data: 2.0684  max mem: 337
[19:04:44.773839] Test:  [530/800]  eta: 0:09:36  loss: 0.1961 (0.2180)  time: 2.0919  data: 2.0784  max mem: 337
[19:04:44.785798] Test:  [530/800]  eta: 0:09:36  loss: 0.2024 (0.2184)  time: 2.0913  data: 2.0779  max mem: 337
[19:04:44.788875] Test:  [530/800]  eta: 0:09:36  loss: 0.2027 (0.2184)  time: 2.0914  data: 2.0781  max mem: 337
[19:04:44.850052] Test:  [530/800]  eta: 0:09:36  loss: 0.2056 (0.2183)  time: 2.0958  data: 2.0825  max mem: 337
[19:04:49.215996] Test:  [520/800]  eta: 0:10:11  loss: 0.2425 (0.2185)  time: 2.1112  data: 2.0994  max mem: 337
[19:04:57.052709] Test:  [530/800]  eta: 0:09:42  loss: 0.1975 (0.2184)  time: 2.0909  data: 2.0790  max mem: 337
[19:04:57.071511] Test:  [530/800]  eta: 0:09:42  loss: 0.2016 (0.2182)  time: 2.0805  data: 2.0687  max mem: 337
[19:04:57.200106] Test:  [530/800]  eta: 0:09:42  loss: 0.1998 (0.2182)  time: 2.0561  data: 2.0430  max mem: 337
[19:04:57.272560] Test:  [530/800]  eta: 0:09:42  loss: 0.2025 (0.2184)  time: 2.0599  data: 2.0462  max mem: 337
[19:04:57.828122] Test:  [530/800]  eta: 0:09:42  loss: 0.2030 (0.2183)  time: 2.0615  data: 2.0484  max mem: 337
[19:04:57.872901] Test:  [530/800]  eta: 0:09:42  loss: 0.1978 (0.2184)  time: 2.0944  data: 2.0826  max mem: 337
[19:04:57.873200] Test:  [530/800]  eta: 0:09:42  loss: 0.1993 (0.2184)  time: 2.0942  data: 2.0824  max mem: 337
[19:04:57.938870] Test:  [530/800]  eta: 0:09:42  loss: 0.2005 (0.2183)  time: 2.0854  data: 2.0723  max mem: 337
[19:04:58.737950] Test:  [530/800]  eta: 0:09:43  loss: 0.2058 (0.2186)  time: 2.0815  data: 2.0698  max mem: 337
[19:04:58.741662] Test:  [530/800]  eta: 0:09:43  loss: 0.1995 (0.2182)  time: 2.0969  data: 2.0851  max mem: 337
[19:05:00.073530] Test:  [530/800]  eta: 0:09:44  loss: 0.2029 (0.2177)  time: 2.1001  data: 2.0883  max mem: 337
[19:05:05.784497] Test:  [540/800]  eta: 0:09:14  loss: 0.2058 (0.2188)  time: 2.1277  data: 2.1143  max mem: 337
[19:05:05.792032] Test:  [540/800]  eta: 0:09:14  loss: 0.2032 (0.2185)  time: 2.1232  data: 2.1095  max mem: 337
[19:05:05.793275] Test:  [540/800]  eta: 0:09:14  loss: 0.2035 (0.2189)  time: 2.1230  data: 2.1095  max mem: 337
[19:05:05.800018] Test:  [530/800]  eta: 0:09:46  loss: 0.2007 (0.2182)  time: 2.0845  data: 2.0727  max mem: 337
[19:05:05.811419] Test:  [540/800]  eta: 0:09:14  loss: 0.2076 (0.2190)  time: 2.1288  data: 2.1153  max mem: 337
[19:05:21.233456] Test:  [540/800]  eta: 0:09:22  loss: 0.2049 (0.2190)  time: 2.1119  data: 2.1000  max mem: 337
[19:05:21.271323] Test:  [540/800]  eta: 0:09:22  loss: 0.2085 (0.2187)  time: 2.1142  data: 2.1024  max mem: 337
[19:05:21.325966] Test:  [540/800]  eta: 0:09:22  loss: 0.2048 (0.2189)  time: 2.1016  data: 2.0898  max mem: 337
[19:05:21.333320] Test:  [540/800]  eta: 0:09:22  loss: 0.2065 (0.2191)  time: 2.1032  data: 2.0914  max mem: 337
[19:05:21.341170] Test:  [540/800]  eta: 0:09:22  loss: 0.2009 (0.2189)  time: 2.1023  data: 2.0904  max mem: 337
[19:05:21.794464] Test:  [540/800]  eta: 0:09:22  loss: 0.2042 (0.2188)  time: 2.1181  data: 2.1063  max mem: 337
[19:05:23.138847] Test:  [540/800]  eta: 0:09:23  loss: 0.2080 (0.2190)  time: 2.1163  data: 2.1026  max mem: 337
[19:05:23.140268] Test:  [540/800]  eta: 0:09:23  loss: 0.2101 (0.2187)  time: 2.1128  data: 2.0995  max mem: 337
[19:05:23.366150] Test:  [540/800]  eta: 0:09:23  loss: 0.2043 (0.2183)  time: 2.1136  data: 2.1018  max mem: 337
[19:05:23.383560] Test:  [540/800]  eta: 0:09:23  loss: 0.2038 (0.2188)  time: 2.1163  data: 2.1031  max mem: 337
[19:05:23.680434] Test:  [540/800]  eta: 0:09:23  loss: 0.2065 (0.2189)  time: 2.1130  data: 2.0997  max mem: 337
[19:05:27.333390] Test:  [550/800]  eta: 0:08:53  loss: 0.2371 (0.2191)  time: 2.1273  data: 2.1140  max mem: 337
[19:05:27.337696] Test:  [550/800]  eta: 0:08:53  loss: 0.2433 (0.2192)  time: 2.1274  data: 2.1139  max mem: 337
[19:05:27.345795] Test:  [550/800]  eta: 0:08:53  loss: 0.2397 (0.2188)  time: 2.1285  data: 2.1149  max mem: 337
[19:05:27.357644] Test:  [550/800]  eta: 0:08:53  loss: 0.2407 (0.2190)  time: 2.1253  data: 2.1121  max mem: 337
[19:05:31.939875] Test:  [540/800]  eta: 0:09:27  loss: 0.2055 (0.2188)  time: 2.1361  data: 2.1243  max mem: 337
[19:05:39.573843] Test:  [550/800]  eta: 0:08:59  loss: 0.2436 (0.2192)  time: 2.1260  data: 2.1141  max mem: 337
[19:05:39.642092] Test:  [550/800]  eta: 0:08:59  loss: 0.2355 (0.2189)  time: 2.1285  data: 2.1167  max mem: 337
[19:05:40.074281] Test:  [550/800]  eta: 0:08:59  loss: 0.2391 (0.2191)  time: 2.1100  data: 2.0982  max mem: 337
[19:05:40.085509] Test:  [550/800]  eta: 0:08:59  loss: 0.2390 (0.2192)  time: 2.1106  data: 2.0987  max mem: 337
[19:05:40.911354] Test:  [550/800]  eta: 0:08:59  loss: 0.2371 (0.2190)  time: 2.1084  data: 2.0966  max mem: 337
[19:05:41.161367] Test:  [550/800]  eta: 0:08:59  loss: 0.2384 (0.2193)  time: 2.1211  data: 2.1095  max mem: 337
[19:05:41.520133] Test:  [550/800]  eta: 0:08:59  loss: 0.2386 (0.2192)  time: 2.2123  data: 2.1986  max mem: 337
[19:05:41.558161] Test:  [550/800]  eta: 0:08:59  loss: 0.2358 (0.2190)  time: 2.2179  data: 2.2046  max mem: 337
[19:05:41.801846] Test:  [550/800]  eta: 0:09:00  loss: 0.2393 (0.2190)  time: 2.1931  data: 2.1795  max mem: 337
[19:05:42.041207] Test:  [550/800]  eta: 0:09:00  loss: 0.2428 (0.2191)  time: 2.2106  data: 2.1974  max mem: 337
[19:05:42.369894] Test:  [550/800]  eta: 0:09:00  loss: 0.2405 (0.2185)  time: 2.1148  data: 2.1029  max mem: 337
[19:05:49.285638] Test:  [560/800]  eta: 0:08:32  loss: 0.2252 (0.2189)  time: 2.1737  data: 2.1602  max mem: 337
[19:05:49.289794] Test:  [560/800]  eta: 0:08:32  loss: 0.2244 (0.2186)  time: 2.1752  data: 2.1620  max mem: 337
[19:05:49.292468] Test:  [560/800]  eta: 0:08:32  loss: 0.2288 (0.2184)  time: 2.1750  data: 2.1616  max mem: 337
[19:05:49.294502] Test:  [560/800]  eta: 0:08:32  loss: 0.2309 (0.2188)  time: 2.1750  data: 2.1619  max mem: 337
[19:05:50.384900] Test:  [550/800]  eta: 0:09:03  loss: 0.2420 (0.2190)  time: 2.2292  data: 2.2175  max mem: 337
[19:06:04.368792] Test:  [560/800]  eta: 0:08:38  loss: 0.2298 (0.2188)  time: 2.1567  data: 2.1448  max mem: 337
[19:06:04.389756] Test:  [560/800]  eta: 0:08:38  loss: 0.2266 (0.2185)  time: 2.1559  data: 2.1440  max mem: 337
[19:06:04.614408] Test:  [560/800]  eta: 0:08:38  loss: 0.2268 (0.2188)  time: 2.1636  data: 2.1518  max mem: 337
[19:06:04.663967] Test:  [560/800]  eta: 0:08:39  loss: 0.2236 (0.2187)  time: 2.1669  data: 2.1550  max mem: 337
[19:06:04.664039] Test:  [560/800]  eta: 0:08:39  loss: 0.2274 (0.2189)  time: 2.1665  data: 2.1548  max mem: 337
[19:06:05.084237] Test:  [560/800]  eta: 0:08:39  loss: 0.2307 (0.2186)  time: 2.1644  data: 2.1526  max mem: 337
[19:06:06.468375] Test:  [560/800]  eta: 0:08:39  loss: 0.2253 (0.2186)  time: 2.1664  data: 2.1532  max mem: 337
[19:06:06.501474] Test:  [560/800]  eta: 0:08:39  loss: 0.2284 (0.2188)  time: 2.1681  data: 2.1544  max mem: 337
[19:06:06.763265] Test:  [560/800]  eta: 0:08:39  loss: 0.2293 (0.2186)  time: 2.1689  data: 2.1553  max mem: 337
[19:06:06.807869] Test:  [560/800]  eta: 0:08:39  loss: 0.2266 (0.2182)  time: 2.1720  data: 2.1602  max mem: 337
[19:06:07.034255] Test:  [560/800]  eta: 0:08:40  loss: 0.2268 (0.2187)  time: 2.1676  data: 2.1545  max mem: 337
[19:06:09.299911] Test:  [570/800]  eta: 0:08:10  loss: 0.2122 (0.2189)  time: 2.0983  data: 2.0849  max mem: 337
[19:06:09.300606] Test:  [570/800]  eta: 0:08:10  loss: 0.2185 (0.2190)  time: 2.0981  data: 2.0846  max mem: 337
[19:06:09.301821] Test:  [570/800]  eta: 0:08:10  loss: 0.2159 (0.2185)  time: 2.0978  data: 2.0843  max mem: 337
[19:06:09.303658] Test:  [570/800]  eta: 0:08:10  loss: 0.2120 (0.2187)  time: 2.0973  data: 2.0838  max mem: 337
[19:06:15.719604] Test:  [560/800]  eta: 0:08:43  loss: 0.2264 (0.2186)  time: 2.1889  data: 2.1773  max mem: 337
[19:06:21.582565] Test:  [570/800]  eta: 0:08:15  loss: 0.2134 (0.2186)  time: 2.0970  data: 2.0852  max mem: 337
[19:06:21.667752] Test:  [570/800]  eta: 0:08:15  loss: 0.2154 (0.2189)  time: 2.1047  data: 2.0927  max mem: 337
[19:06:22.180121] Test:  [570/800]  eta: 0:08:15  loss: 0.2160 (0.2188)  time: 2.1052  data: 2.0934  max mem: 337
[19:06:22.257310] Test:  [570/800]  eta: 0:08:15  loss: 0.2144 (0.2189)  time: 2.1085  data: 2.0967  max mem: 337
[19:06:22.830145] Test:  [570/800]  eta: 0:08:15  loss: 0.2136 (0.2187)  time: 2.0959  data: 2.0841  max mem: 337
[19:06:22.890621] Test:  [570/800]  eta: 0:08:16  loss: 0.2098 (0.2190)  time: 2.0864  data: 2.0746  max mem: 337
[19:06:23.662498] Test:  [570/800]  eta: 0:08:16  loss: 0.2130 (0.2187)  time: 2.1052  data: 2.0917  max mem: 337
[19:06:23.711159] Test:  [570/800]  eta: 0:08:16  loss: 0.2138 (0.2189)  time: 2.1095  data: 2.0958  max mem: 337
[19:06:23.953284] Test:  [570/800]  eta: 0:08:16  loss: 0.2186 (0.2187)  time: 2.1075  data: 2.0939  max mem: 337
[19:06:24.206086] Test:  [570/800]  eta: 0:08:16  loss: 0.2165 (0.2189)  time: 2.1082  data: 2.0947  max mem: 337
[19:06:24.451081] Test:  [570/800]  eta: 0:08:16  loss: 0.2197 (0.2183)  time: 2.1040  data: 2.0922  max mem: 337
[19:06:32.215235] Test:  [580/800]  eta: 0:07:49  loss: 0.2329 (0.2195)  time: 2.1460  data: 2.1325  max mem: 337
[19:06:32.220194] Test:  [580/800]  eta: 0:07:49  loss: 0.2282 (0.2192)  time: 2.1463  data: 2.1327  max mem: 337
[19:06:32.225247] Test:  [580/800]  eta: 0:07:49  loss: 0.2268 (0.2193)  time: 2.1467  data: 2.1334  max mem: 337
[19:06:32.226643] Test:  [580/800]  eta: 0:07:49  loss: 0.2296 (0.2196)  time: 2.1470  data: 2.1334  max mem: 337
[19:06:32.942406] Test:  [570/800]  eta: 0:08:20  loss: 0.2189 (0.2188)  time: 2.1278  data: 2.1160  max mem: 337
[19:06:47.092948] Test:  [580/800]  eta: 0:07:55  loss: 0.2306 (0.2195)  time: 2.1362  data: 2.1243  max mem: 337
[19:06:47.107549] Test:  [580/800]  eta: 0:07:55  loss: 0.2299 (0.2192)  time: 2.1358  data: 2.1241  max mem: 337
[19:06:47.382991] Test:  [580/800]  eta: 0:07:55  loss: 0.2240 (0.2195)  time: 2.1384  data: 2.1265  max mem: 337
[19:06:47.383029] Test:  [580/800]  eta: 0:07:55  loss: 0.2327 (0.2195)  time: 2.1359  data: 2.1241  max mem: 337
[19:06:47.517613] Test:  [580/800]  eta: 0:07:55  loss: 0.2305 (0.2196)  time: 2.1426  data: 2.1308  max mem: 337
[19:06:47.795982] Test:  [580/800]  eta: 0:07:55  loss: 0.2246 (0.2193)  time: 2.1355  data: 2.1237  max mem: 337
[19:06:49.296176] Test:  [580/800]  eta: 0:07:56  loss: 0.2278 (0.2195)  time: 2.1397  data: 2.1260  max mem: 337
[19:06:49.332785] Test:  [580/800]  eta: 0:07:56  loss: 0.2293 (0.2193)  time: 2.1432  data: 2.1301  max mem: 337
[19:06:49.538218] Test:  [580/800]  eta: 0:07:56  loss: 0.2274 (0.2193)  time: 2.1387  data: 2.1256  max mem: 337
[19:06:49.608160] Test:  [580/800]  eta: 0:07:56  loss: 0.2263 (0.2189)  time: 2.1400  data: 2.1281  max mem: 337
[19:06:49.855625] Test:  [580/800]  eta: 0:07:56  loss: 0.2211 (0.2194)  time: 2.1410  data: 2.1274  max mem: 337
[19:06:51.332100] Test:  [590/800]  eta: 0:07:27  loss: 0.2382 (0.2199)  time: 2.1016  data: 2.0881  max mem: 337
[19:06:51.336380] Test:  [590/800]  eta: 0:07:27  loss: 0.2391 (0.2197)  time: 2.1016  data: 2.0882  max mem: 337
[19:06:51.354230] Test:  [590/800]  eta: 0:07:27  loss: 0.2408 (0.2196)  time: 2.1026  data: 2.0891  max mem: 337
[19:06:51.388234] Test:  [590/800]  eta: 0:07:27  loss: 0.2427 (0.2201)  time: 2.1043  data: 2.0908  max mem: 337
[19:06:58.791898] Test:  [580/800]  eta: 0:07:59  loss: 0.2318 (0.2194)  time: 2.1536  data: 2.1419  max mem: 337
[19:07:03.227724] Test:  [590/800]  eta: 0:07:31  loss: 0.2394 (0.2196)  time: 2.0822  data: 2.0704  max mem: 337
[19:07:03.231095] Test:  [590/800]  eta: 0:07:31  loss: 0.2385 (0.2200)  time: 2.0781  data: 2.0662  max mem: 337
[19:07:03.957465] Test:  [590/800]  eta: 0:07:32  loss: 0.2385 (0.2200)  time: 2.0850  data: 2.0732  max mem: 337
[19:07:03.966267] Test:  [590/800]  eta: 0:07:32  loss: 0.2398 (0.2199)  time: 2.0893  data: 2.0776  max mem: 337
[19:07:04.365450] Test:  [590/800]  eta: 0:07:32  loss: 0.2368 (0.2200)  time: 2.0737  data: 2.0619  max mem: 337
[19:07:04.570484] Test:  [590/800]  eta: 0:07:32  loss: 0.2406 (0.2197)  time: 2.0870  data: 2.0752  max mem: 337
[19:07:05.622835] Test:  [590/800]  eta: 0:07:32  loss: 0.2370 (0.2199)  time: 2.0955  data: 2.0818  max mem: 337
[19:07:05.639482] Test:  [590/800]  eta: 0:07:32  loss: 0.2338 (0.2197)  time: 2.0988  data: 2.0857  max mem: 337
[19:07:05.878150] Test:  [590/800]  eta: 0:07:32  loss: 0.2339 (0.2197)  time: 2.0962  data: 2.0831  max mem: 337
[19:07:06.123492] Test:  [590/800]  eta: 0:07:32  loss: 0.2382 (0.2199)  time: 2.0958  data: 2.0822  max mem: 337
[19:07:06.289864] Test:  [590/800]  eta: 0:07:32  loss: 0.2419 (0.2193)  time: 2.0919  data: 2.0800  max mem: 337
[19:07:13.659750] Test:  [600/800]  eta: 0:07:06  loss: 0.2189 (0.2200)  time: 2.0716  data: 2.0582  max mem: 337
[19:07:13.687440] Test:  [600/800]  eta: 0:07:06  loss: 0.2156 (0.2196)  time: 2.0733  data: 2.0600  max mem: 337
[19:07:13.690692] Test:  [600/800]  eta: 0:07:06  loss: 0.2194 (0.2199)  time: 2.0737  data: 2.0602  max mem: 337
[19:07:13.694458] Test:  [600/800]  eta: 0:07:06  loss: 0.2144 (0.2197)  time: 2.0734  data: 2.0598  max mem: 337
[19:07:15.222405] Test:  [590/800]  eta: 0:07:36  loss: 0.2373 (0.2198)  time: 2.1140  data: 2.1023  max mem: 337
[19:07:28.168988] Test:  [600/800]  eta: 0:07:11  loss: 0.2186 (0.2200)  time: 2.0538  data: 2.0420  max mem: 337
[19:07:28.174328] Test:  [600/800]  eta: 0:07:11  loss: 0.2203 (0.2196)  time: 2.0533  data: 2.0415  max mem: 337
[19:07:28.606328] Test:  [600/800]  eta: 0:07:11  loss: 0.2149 (0.2200)  time: 2.0611  data: 2.0494  max mem: 337
[19:07:28.638027] Test:  [600/800]  eta: 0:07:11  loss: 0.2114 (0.2198)  time: 2.0627  data: 2.0511  max mem: 337
[19:07:28.776720] Test:  [600/800]  eta: 0:07:11  loss: 0.2169 (0.2200)  time: 2.0629  data: 2.0512  max mem: 337
[19:07:29.082092] Test:  [600/800]  eta: 0:07:11  loss: 0.2114 (0.2197)  time: 2.0643  data: 2.0525  max mem: 337
[19:07:30.594645] Test:  [600/800]  eta: 0:07:12  loss: 0.2181 (0.2199)  time: 2.0649  data: 2.0512  max mem: 337
[19:07:30.631204] Test:  [600/800]  eta: 0:07:12  loss: 0.2161 (0.2197)  time: 2.0649  data: 2.0513  max mem: 337
[19:07:30.906051] Test:  [600/800]  eta: 0:07:12  loss: 0.2167 (0.2193)  time: 2.0648  data: 2.0531  max mem: 337
[19:07:30.924839] Test:  [600/800]  eta: 0:07:12  loss: 0.2136 (0.2197)  time: 2.0693  data: 2.0557  max mem: 337
[19:07:31.171296] Test:  [600/800]  eta: 0:07:12  loss: 0.2147 (0.2198)  time: 2.0657  data: 2.0521  max mem: 337
[19:07:33.739905] Test:  [610/800]  eta: 0:06:44  loss: 0.2383 (0.2207)  time: 2.1203  data: 2.1070  max mem: 337
[19:07:33.740243] Test:  [610/800]  eta: 0:06:44  loss: 0.2286 (0.2208)  time: 2.1175  data: 2.1042  max mem: 337
[19:07:33.740774] Test:  [610/800]  eta: 0:06:44  loss: 0.2411 (0.2205)  time: 2.1202  data: 2.1066  max mem: 337
[19:07:33.744959] Test:  [610/800]  eta: 0:06:44  loss: 0.2377 (0.2204)  time: 2.1195  data: 2.1061  max mem: 337
[19:07:40.556710] Test:  [600/800]  eta: 0:07:15  loss: 0.2188 (0.2198)  time: 2.0882  data: 2.0764  max mem: 337
[19:07:45.254357] Test:  [610/800]  eta: 0:06:48  loss: 0.2411 (0.2205)  time: 2.1013  data: 2.0895  max mem: 337
[19:07:45.274000] Test:  [610/800]  eta: 0:06:48  loss: 0.2405 (0.2208)  time: 2.1021  data: 2.0904  max mem: 337
[19:07:46.211659] Test:  [610/800]  eta: 0:06:48  loss: 0.2384 (0.2206)  time: 2.1122  data: 2.1004  max mem: 337
[19:07:46.213719] Test:  [610/800]  eta: 0:06:48  loss: 0.2368 (0.2208)  time: 2.1128  data: 2.1009  max mem: 337
[19:07:46.567712] Test:  [610/800]  eta: 0:06:48  loss: 0.2353 (0.2208)  time: 2.1101  data: 2.0983  max mem: 337
[19:07:46.849504] Test:  [610/800]  eta: 0:06:49  loss: 0.2409 (0.2206)  time: 2.1139  data: 2.1021  max mem: 337
[19:07:47.776116] Test:  [610/800]  eta: 0:06:49  loss: 0.2348 (0.2205)  time: 2.1068  data: 2.0932  max mem: 337
[19:07:47.778327] Test:  [610/800]  eta: 0:06:49  loss: 0.2424 (0.2207)  time: 2.1077  data: 2.0940  max mem: 337
[19:07:48.006531] Test:  [610/800]  eta: 0:06:49  loss: 0.2347 (0.2205)  time: 2.1064  data: 2.0930  max mem: 337
[19:07:48.222642] Test:  [610/800]  eta: 0:06:49  loss: 0.2338 (0.2207)  time: 2.1049  data: 2.0913  max mem: 337
[19:07:48.538844] Test:  [610/800]  eta: 0:06:49  loss: 0.2356 (0.2202)  time: 2.1124  data: 2.1007  max mem: 337
[19:07:56.441617] Test:  [620/800]  eta: 0:06:24  loss: 0.2379 (0.2207)  time: 2.1373  data: 2.1237  max mem: 337
[19:07:56.445173] Test:  [620/800]  eta: 0:06:24  loss: 0.2270 (0.2209)  time: 2.1392  data: 2.1257  max mem: 337
[19:07:56.454973] Test:  [620/800]  eta: 0:06:24  loss: 0.2383 (0.2209)  time: 2.1382  data: 2.1247  max mem: 337
[19:07:56.513031] Test:  [620/800]  eta: 0:06:24  loss: 0.2380 (0.2206)  time: 2.1412  data: 2.1278  max mem: 337
[19:07:57.932163] Test:  [610/800]  eta: 0:06:52  loss: 0.2378 (0.2206)  time: 2.1354  data: 2.1236  max mem: 337
[19:08:10.813072] Test:  [620/800]  eta: 0:06:28  loss: 0.2405 (0.2210)  time: 2.1322  data: 2.1203  max mem: 337
[19:08:10.866593] Test:  [620/800]  eta: 0:06:28  loss: 0.2384 (0.2206)  time: 2.1346  data: 2.1228  max mem: 337
[19:08:11.320507] Test:  [620/800]  eta: 0:06:28  loss: 0.2393 (0.2208)  time: 2.1341  data: 2.1224  max mem: 337
[19:08:11.335733] Test:  [620/800]  eta: 0:06:28  loss: 0.2383 (0.2209)  time: 2.1364  data: 2.1248  max mem: 337
[19:08:11.462277] Test:  [620/800]  eta: 0:06:28  loss: 0.2338 (0.2209)  time: 2.1342  data: 2.1226  max mem: 337
[19:08:11.879046] Test:  [620/800]  eta: 0:06:28  loss: 0.2335 (0.2207)  time: 2.1398  data: 2.1280  max mem: 337
[19:08:13.328652] Test:  [620/800]  eta: 0:06:28  loss: 0.2282 (0.2207)  time: 2.1348  data: 2.1212  max mem: 337
[19:08:13.466825] Test:  [620/800]  eta: 0:06:28  loss: 0.2386 (0.2209)  time: 2.1436  data: 2.1300  max mem: 337
[19:08:13.592668] Test:  [620/800]  eta: 0:06:29  loss: 0.2347 (0.2207)  time: 2.1333  data: 2.1199  max mem: 337
[19:08:13.611823] Test:  [620/800]  eta: 0:06:29  loss: 0.2356 (0.2204)  time: 2.1352  data: 2.1235  max mem: 337
[19:08:13.874296] Test:  [620/800]  eta: 0:06:29  loss: 0.2366 (0.2208)  time: 2.1351  data: 2.1215  max mem: 337
[19:08:15.963053] Test:  [630/800]  eta: 0:06:02  loss: 0.2245 (0.2211)  time: 2.1111  data: 2.0977  max mem: 337
[19:08:15.963233] Test:  [630/800]  eta: 0:06:02  loss: 0.2233 (0.2207)  time: 2.1109  data: 2.0976  max mem: 337
[19:08:15.964556] Test:  [630/800]  eta: 0:06:02  loss: 0.2237 (0.2209)  time: 2.1111  data: 2.0975  max mem: 337
[19:08:16.180800] Test:  [630/800]  eta: 0:06:02  loss: 0.2246 (0.2210)  time: 2.1220  data: 2.1086  max mem: 337
[19:08:23.774185] Test:  [620/800]  eta: 0:06:31  loss: 0.2378 (0.2207)  time: 2.1608  data: 2.1490  max mem: 337
[19:08:28.390471] Test:  [630/800]  eta: 0:06:05  loss: 0.2298 (0.2212)  time: 2.1558  data: 2.1439  max mem: 337
[19:08:28.424365] Test:  [630/800]  eta: 0:06:05  loss: 0.2270 (0.2208)  time: 2.1585  data: 2.1466  max mem: 337
[19:08:29.016691] Test:  [630/800]  eta: 0:06:05  loss: 0.2261 (0.2210)  time: 2.1402  data: 2.1286  max mem: 337
[19:08:29.054578] Test:  [630/800]  eta: 0:06:05  loss: 0.2249 (0.2211)  time: 2.1420  data: 2.1303  max mem: 337
[19:08:29.782894] Test:  [630/800]  eta: 0:06:05  loss: 0.2221 (0.2208)  time: 2.1466  data: 2.1348  max mem: 337
[19:08:30.255963] Test:  [630/800]  eta: 0:06:06  loss: 0.2191 (0.2211)  time: 2.1844  data: 2.1727  max mem: 337
[19:08:30.923053] Test:  [630/800]  eta: 0:06:06  loss: 0.2239 (0.2209)  time: 2.1573  data: 2.1437  max mem: 337
[19:08:31.042287] Test:  [630/800]  eta: 0:06:06  loss: 0.2224 (0.2211)  time: 2.1632  data: 2.1499  max mem: 337
[19:08:31.157145] Test:  [630/800]  eta: 0:06:06  loss: 0.2258 (0.2209)  time: 2.1575  data: 2.1439  max mem: 337
[19:08:31.385828] Test:  [630/800]  eta: 0:06:06  loss: 0.2283 (0.2206)  time: 2.1423  data: 2.1306  max mem: 337
[19:08:31.425027] Test:  [630/800]  eta: 0:06:06  loss: 0.2230 (0.2210)  time: 2.1601  data: 2.1465  max mem: 337
[19:08:39.416225] Test:  [640/800]  eta: 0:05:41  loss: 0.2481 (0.2214)  time: 2.1451  data: 2.1317  max mem: 337
[19:08:39.423141] Test:  [640/800]  eta: 0:05:41  loss: 0.2491 (0.2216)  time: 2.1490  data: 2.1354  max mem: 337
[19:08:39.426281] Test:  [640/800]  eta: 0:05:41  loss: 0.2437 (0.2217)  time: 2.1485  data: 2.1351  max mem: 337
[19:08:39.432428] Test:  [640/800]  eta: 0:05:41  loss: 0.2514 (0.2217)  time: 2.1493  data: 2.1359  max mem: 337
[19:08:41.517522] Test:  [630/800]  eta: 0:06:09  loss: 0.2209 (0.2209)  time: 2.1792  data: 2.1674  max mem: 337
[19:08:53.898941] Test:  [640/800]  eta: 0:05:45  loss: 0.2517 (0.2219)  time: 2.1542  data: 2.1424  max mem: 337
[19:08:54.081627] Test:  [640/800]  eta: 0:05:45  loss: 0.2462 (0.2215)  time: 2.1607  data: 2.1489  max mem: 337
[19:08:54.366832] Test:  [640/800]  eta: 0:05:45  loss: 0.2525 (0.2217)  time: 2.1523  data: 2.1404  max mem: 337
[19:08:54.411209] Test:  [640/800]  eta: 0:05:45  loss: 0.2466 (0.2218)  time: 2.1537  data: 2.1419  max mem: 337
[19:08:54.514772] Test:  [640/800]  eta: 0:05:45  loss: 0.2476 (0.2218)  time: 2.1526  data: 2.1408  max mem: 337
[19:08:55.124087] Test:  [640/800]  eta: 0:05:45  loss: 0.2430 (0.2215)  time: 2.1622  data: 2.1504  max mem: 337
[19:08:56.539829] Test:  [640/800]  eta: 0:05:45  loss: 0.2544 (0.2216)  time: 2.1605  data: 2.1469  max mem: 337
[19:08:56.559358] Test:  [640/800]  eta: 0:05:45  loss: 0.2467 (0.2218)  time: 2.1546  data: 2.1412  max mem: 337
[19:08:56.721551] Test:  [640/800]  eta: 0:05:45  loss: 0.2543 (0.2216)  time: 2.1564  data: 2.1428  max mem: 337
[19:08:56.777498] Test:  [640/800]  eta: 0:05:45  loss: 0.2442 (0.2213)  time: 2.1582  data: 2.1466  max mem: 337
[19:08:56.978617] Test:  [640/800]  eta: 0:05:45  loss: 0.2524 (0.2218)  time: 2.1552  data: 2.1416  max mem: 337
[19:08:58.314194] Test:  [650/800]  eta: 0:05:19  loss: 0.2514 (0.2217)  time: 2.1175  data: 2.1040  max mem: 337
[19:08:58.314361] Test:  [650/800]  eta: 0:05:19  loss: 0.2491 (0.2215)  time: 2.1174  data: 2.1039  max mem: 337
[19:08:58.453027] Test:  [650/800]  eta: 0:05:19  loss: 0.2481 (0.2214)  time: 2.1244  data: 2.1109  max mem: 337
[19:08:58.553319] Test:  [650/800]  eta: 0:05:19  loss: 0.2493 (0.2217)  time: 2.1186  data: 2.1049  max mem: 337
[19:09:07.479278] Test:  [640/800]  eta: 0:05:48  loss: 0.2495 (0.2216)  time: 2.1852  data: 2.1736  max mem: 337
[19:09:11.795493] Test:  [650/800]  eta: 0:05:22  loss: 0.2517 (0.2218)  time: 2.1702  data: 2.1583  max mem: 337
[19:09:11.954184] Test:  [650/800]  eta: 0:05:22  loss: 0.2483 (0.2214)  time: 2.1764  data: 2.1646  max mem: 337
[19:09:12.817039] Test:  [650/800]  eta: 0:05:22  loss: 0.2481 (0.2217)  time: 2.1881  data: 2.1763  max mem: 337
[19:09:12.821385] Test:  [650/800]  eta: 0:05:22  loss: 0.2516 (0.2216)  time: 2.1902  data: 2.1785  max mem: 337
[19:09:13.130540] Test:  [650/800]  eta: 0:05:22  loss: 0.2476 (0.2217)  time: 2.1437  data: 2.1319  max mem: 337
[19:09:13.498991] Test:  [650/800]  eta: 0:05:23  loss: 0.2511 (0.2217)  time: 2.1228  data: 2.1094  max mem: 337
[19:09:13.505788] Test:  [650/800]  eta: 0:05:23  loss: 0.2544 (0.2215)  time: 2.1291  data: 2.1155  max mem: 337
[19:09:13.665422] Test:  [650/800]  eta: 0:05:23  loss: 0.2464 (0.2214)  time: 2.1941  data: 2.1823  max mem: 337
[19:09:13.705659] Test:  [650/800]  eta: 0:05:23  loss: 0.2494 (0.2215)  time: 2.1274  data: 2.1138  max mem: 337
[19:09:14.052870] Test:  [650/800]  eta: 0:05:23  loss: 0.2531 (0.2217)  time: 2.1313  data: 2.1177  max mem: 337
[19:09:15.408451] Test:  [650/800]  eta: 0:05:23  loss: 0.2442 (0.2212)  time: 2.2011  data: 2.1893  max mem: 337
[19:09:22.076557] Test:  [660/800]  eta: 0:04:58  loss: 0.2390 (0.2219)  time: 2.1330  data: 2.1197  max mem: 338
[19:09:22.077047] Test:  [660/800]  eta: 0:04:58  loss: 0.2462 (0.2220)  time: 2.1326  data: 2.1192  max mem: 338
[19:09:22.077460] Test:  [660/800]  eta: 0:04:58  loss: 0.2463 (0.2222)  time: 2.1325  data: 2.1190  max mem: 338
[19:09:22.078476] Test:  [660/800]  eta: 0:04:58  loss: 0.2452 (0.2222)  time: 2.1323  data: 2.1190  max mem: 338
[19:09:24.729627] Test:  [650/800]  eta: 0:05:25  loss: 0.2495 (0.2216)  time: 2.1606  data: 2.1489  max mem: 337
[19:09:37.358569] Test:  [660/800]  eta: 0:05:02  loss: 0.2416 (0.2223)  time: 2.1729  data: 2.1612  max mem: 338
[19:09:37.374521] Test:  [660/800]  eta: 0:05:02  loss: 0.2483 (0.2220)  time: 2.1646  data: 2.1530  max mem: 338
[19:09:37.645231] Test:  [660/800]  eta: 0:05:02  loss: 0.2437 (0.2222)  time: 2.1617  data: 2.1499  max mem: 338
[19:09:37.650197] Test:  [660/800]  eta: 0:05:02  loss: 0.2464 (0.2221)  time: 2.1641  data: 2.1525  max mem: 338
[19:09:37.873605] Test:  [660/800]  eta: 0:05:02  loss: 0.2421 (0.2222)  time: 2.1679  data: 2.1561  max mem: 338
[19:09:38.402988] Test:  [660/800]  eta: 0:05:02  loss: 0.2460 (0.2219)  time: 2.1639  data: 2.1521  max mem: 338
[19:09:39.286210] Test:  [670/800]  eta: 0:04:36  loss: 0.2417 (0.2224)  time: 2.0366  data: 2.0231  max mem: 338
[19:09:39.301496] Test:  [670/800]  eta: 0:04:36  loss: 0.2462 (0.2223)  time: 2.0493  data: 2.0358  max mem: 338
[19:09:39.309864] Test:  [670/800]  eta: 0:04:36  loss: 0.2411 (0.2224)  time: 2.0497  data: 2.0364  max mem: 338
[19:09:39.327916] Test:  [670/800]  eta: 0:04:36  loss: 0.2442 (0.2221)  time: 2.0437  data: 2.0303  max mem: 338
[19:09:39.773943] Test:  [660/800]  eta: 0:05:02  loss: 0.2465 (0.2222)  time: 2.1607  data: 2.1473  max mem: 338
[19:09:39.790779] Test:  [660/800]  eta: 0:05:02  loss: 0.2428 (0.2220)  time: 2.1625  data: 2.1489  max mem: 338
[19:09:40.051977] Test:  [660/800]  eta: 0:05:02  loss: 0.2467 (0.2221)  time: 2.1665  data: 2.1530  max mem: 338
[19:09:40.261221] Test:  [660/800]  eta: 0:05:02  loss: 0.2440 (0.2218)  time: 2.1741  data: 2.1625  max mem: 338
[19:09:40.384753] Test:  [660/800]  eta: 0:05:02  loss: 0.2486 (0.2223)  time: 2.1703  data: 2.1567  max mem: 338
[19:09:51.174280] Test:  [660/800]  eta: 0:05:04  loss: 0.2422 (0.2221)  time: 2.1847  data: 2.1729  max mem: 338
[19:09:54.428334] Test:  [670/800]  eta: 0:04:39  loss: 0.2378 (0.2221)  time: 2.1237  data: 2.1120  max mem: 338
[19:09:54.485653] Test:  [670/800]  eta: 0:04:39  loss: 0.2385 (0.2225)  time: 2.1345  data: 2.1228  max mem: 338
[19:09:54.887313] Test:  [670/800]  eta: 0:04:39  loss: 0.2433 (0.2223)  time: 2.1033  data: 2.0914  max mem: 338
[19:09:54.902063] Test:  [670/800]  eta: 0:04:39  loss: 0.2437 (0.2225)  time: 2.1042  data: 2.0923  max mem: 338
[19:09:55.661476] Test:  [670/800]  eta: 0:04:39  loss: 0.2409 (0.2221)  time: 2.0998  data: 2.0879  max mem: 338
[19:09:55.799469] Test:  [670/800]  eta: 0:04:39  loss: 0.2401 (0.2224)  time: 2.1334  data: 2.1216  max mem: 338
[19:09:56.953746] Test:  [670/800]  eta: 0:04:40  loss: 0.2438 (0.2225)  time: 2.1727  data: 2.1590  max mem: 338
[19:09:56.973589] Test:  [670/800]  eta: 0:04:40  loss: 0.2377 (0.2222)  time: 2.1733  data: 2.1597  max mem: 338
[19:09:57.248793] Test:  [670/800]  eta: 0:04:40  loss: 0.2385 (0.2223)  time: 2.1771  data: 2.1639  max mem: 338
[19:09:57.428690] Test:  [670/800]  eta: 0:04:40  loss: 0.2415 (0.2220)  time: 2.1010  data: 2.0893  max mem: 338
[19:09:57.660137] Test:  [670/800]  eta: 0:04:40  loss: 0.2398 (0.2225)  time: 2.1803  data: 2.1667  max mem: 338
[19:10:04.874053] Test:  [680/800]  eta: 0:04:16  loss: 0.2225 (0.2220)  time: 2.1398  data: 2.1262  max mem: 338
[19:10:04.876585] Test:  [680/800]  eta: 0:04:16  loss: 0.2186 (0.2222)  time: 2.1399  data: 2.1263  max mem: 338
[19:10:04.898025] Test:  [680/800]  eta: 0:04:16  loss: 0.2224 (0.2219)  time: 2.1410  data: 2.1275  max mem: 338
[19:10:04.900135] Test:  [680/800]  eta: 0:04:16  loss: 0.2166 (0.2222)  time: 2.1410  data: 2.1274  max mem: 338
[19:10:08.476218] Test:  [670/800]  eta: 0:04:42  loss: 0.2422 (0.2223)  time: 2.1873  data: 2.1755  max mem: 338
[19:10:20.115040] Test:  [680/800]  eta: 0:04:18  loss: 0.2156 (0.2220)  time: 2.1370  data: 2.1252  max mem: 338
[19:10:20.135009] Test:  [680/800]  eta: 0:04:18  loss: 0.2158 (0.2223)  time: 2.1388  data: 2.1269  max mem: 338
[19:10:20.722050] Test:  [680/800]  eta: 0:04:18  loss: 0.2172 (0.2223)  time: 2.1538  data: 2.1419  max mem: 338
[19:10:20.723876] Test:  [680/800]  eta: 0:04:18  loss: 0.2183 (0.2222)  time: 2.1536  data: 2.1420  max mem: 338
[19:10:20.837310] Test:  [680/800]  eta: 0:04:18  loss: 0.2200 (0.2222)  time: 2.1481  data: 2.1365  max mem: 338
[19:10:21.317806] Test:  [680/800]  eta: 0:04:19  loss: 0.2175 (0.2219)  time: 2.1457  data: 2.1339  max mem: 338
[19:10:21.439502] Test:  [690/800]  eta: 0:03:53  loss: 0.1963 (0.2214)  time: 2.1064  data: 2.0930  max mem: 338
[19:10:21.453287] Test:  [690/800]  eta: 0:03:53  loss: 0.2008 (0.2215)  time: 2.1083  data: 2.0947  max mem: 338
[19:10:21.525973] Test:  [690/800]  eta: 0:03:54  loss: 0.2014 (0.2213)  time: 2.1112  data: 2.0976  max mem: 338
[19:10:21.526553] Test:  [690/800]  eta: 0:03:54  loss: 0.2005 (0.2212)  time: 2.1099  data: 2.0967  max mem: 338
[19:10:22.886961] Test:  [680/800]  eta: 0:04:19  loss: 0.2197 (0.2222)  time: 2.1556  data: 2.1419  max mem: 338
[19:10:22.887164] Test:  [680/800]  eta: 0:04:19  loss: 0.2182 (0.2220)  time: 2.1548  data: 2.1412  max mem: 338
[19:10:23.085792] Test:  [680/800]  eta: 0:04:19  loss: 0.2203 (0.2221)  time: 2.1516  data: 2.1384  max mem: 338
[19:10:23.306339] Test:  [680/800]  eta: 0:04:19  loss: 0.2203 (0.2218)  time: 2.1522  data: 2.1404  max mem: 338
[19:10:23.433059] Test:  [680/800]  eta: 0:04:19  loss: 0.2179 (0.2223)  time: 2.1524  data: 2.1388  max mem: 338
[19:10:34.489479] Test:  [680/800]  eta: 0:04:21  loss: 0.2207 (0.2221)  time: 2.1657  data: 2.1539  max mem: 338
[19:10:36.609432] Test:  [690/800]  eta: 0:03:56  loss: 0.2054 (0.2212)  time: 2.1090  data: 2.0974  max mem: 338
[19:10:36.610640] Test:  [690/800]  eta: 0:03:56  loss: 0.2042 (0.2216)  time: 2.1062  data: 2.0945  max mem: 338
[19:10:37.349134] Test:  [690/800]  eta: 0:03:56  loss: 0.2074 (0.2215)  time: 2.1223  data: 2.1104  max mem: 338
[19:10:37.350655] Test:  [690/800]  eta: 0:03:56  loss: 0.2008 (0.2214)  time: 2.1231  data: 2.1115  max mem: 338
[19:10:37.806982] Test:  [690/800]  eta: 0:03:56  loss: 0.2007 (0.2212)  time: 2.1072  data: 2.0954  max mem: 338
[19:10:37.993551] Test:  [690/800]  eta: 0:03:56  loss: 0.2000 (0.2214)  time: 2.1097  data: 2.0980  max mem: 338
[19:10:39.470871] Test:  [690/800]  eta: 0:03:56  loss: 0.2050 (0.2215)  time: 2.1258  data: 2.1121  max mem: 338
[19:10:39.476527] Test:  [690/800]  eta: 0:03:56  loss: 0.1969 (0.2213)  time: 2.1251  data: 2.1115  max mem: 338
[19:10:39.721873] Test:  [690/800]  eta: 0:03:56  loss: 0.2004 (0.2213)  time: 2.1236  data: 2.1102  max mem: 338
[19:10:39.960360] Test:  [690/800]  eta: 0:03:56  loss: 0.2021 (0.2211)  time: 2.1265  data: 2.1149  max mem: 338
[19:10:40.101602] Test:  [690/800]  eta: 0:03:56  loss: 0.2037 (0.2215)  time: 2.1220  data: 2.1084  max mem: 338
[19:10:46.688391] Test:  [700/800]  eta: 0:03:33  loss: 0.1254 (0.2199)  time: 2.0907  data: 2.0770  max mem: 338
[19:10:46.692338] Test:  [700/800]  eta: 0:03:33  loss: 0.1218 (0.2201)  time: 2.0907  data: 2.0771  max mem: 338
[19:10:46.719242] Test:  [700/800]  eta: 0:03:33  loss: 0.1238 (0.2198)  time: 2.0910  data: 2.0778  max mem: 338
[19:10:46.752643] Test:  [700/800]  eta: 0:03:33  loss: 0.1271 (0.2201)  time: 2.0926  data: 2.0792  max mem: 338
[19:10:51.332442] Test:  [690/800]  eta: 0:03:58  loss: 0.2023 (0.2214)  time: 2.1428  data: 2.1310  max mem: 338
[19:11:01.749892] Test:  [700/800]  eta: 0:03:35  loss: 0.1218 (0.2202)  time: 2.0807  data: 2.0690  max mem: 338
[19:11:01.769446] Test:  [700/800]  eta: 0:03:35  loss: 0.1213 (0.2199)  time: 2.0827  data: 2.0710  max mem: 338
[19:11:02.484958] Test:  [700/800]  eta: 0:03:35  loss: 0.1203 (0.2201)  time: 2.0880  data: 2.0762  max mem: 338
[19:11:02.508340] Test:  [700/800]  eta: 0:03:35  loss: 0.1206 (0.2201)  time: 2.0893  data: 2.0775  max mem: 338
[19:11:02.725902] Test:  [700/800]  eta: 0:03:35  loss: 0.1247 (0.2200)  time: 2.0944  data: 2.0826  max mem: 338
[19:11:03.074218] Test:  [700/800]  eta: 0:03:35  loss: 0.1222 (0.2198)  time: 2.0878  data: 2.0760  max mem: 338
[19:11:03.819194] Test:  [710/800]  eta: 0:03:11  loss: 0.1271 (0.2190)  time: 2.1189  data: 2.1053  max mem: 338
[19:11:03.824196] Test:  [710/800]  eta: 0:03:11  loss: 0.1233 (0.2191)  time: 2.1185  data: 2.1048  max mem: 338
[19:11:03.854970] Test:  [710/800]  eta: 0:03:11  loss: 0.1254 (0.2190)  time: 2.1164  data: 2.1028  max mem: 338
[19:11:03.861021] Test:  [710/800]  eta: 0:03:11  loss: 0.1256 (0.2189)  time: 2.1167  data: 2.1031  max mem: 338
[19:11:04.749065] Test:  [700/800]  eta: 0:03:35  loss: 0.1243 (0.2201)  time: 2.0931  data: 2.0794  max mem: 338
[19:11:04.873928] Test:  [700/800]  eta: 0:03:35  loss: 0.1217 (0.2199)  time: 2.0993  data: 2.0857  max mem: 338
[19:11:05.013280] Test:  [700/800]  eta: 0:03:35  loss: 0.1222 (0.2199)  time: 2.0963  data: 2.0830  max mem: 338
[19:11:05.285867] Test:  [700/800]  eta: 0:03:35  loss: 0.1229 (0.2197)  time: 2.0989  data: 2.0873  max mem: 338
[19:11:05.412828] Test:  [700/800]  eta: 0:03:35  loss: 0.1216 (0.2201)  time: 2.0989  data: 2.0853  max mem: 338
[19:11:16.804439] Test:  [700/800]  eta: 0:03:37  loss: 0.1213 (0.2200)  time: 2.1157  data: 2.1039  max mem: 338
[19:11:18.749494] Test:  [710/800]  eta: 0:03:13  loss: 0.1213 (0.2188)  time: 2.1070  data: 2.0951  max mem: 338
[19:11:18.749696] Test:  [710/800]  eta: 0:03:13  loss: 0.1256 (0.2192)  time: 2.1069  data: 2.0950  max mem: 338
[19:11:19.473204] Test:  [710/800]  eta: 0:03:13  loss: 0.1248 (0.2191)  time: 2.1061  data: 2.0943  max mem: 338
[19:11:19.500326] Test:  [710/800]  eta: 0:03:13  loss: 0.1206 (0.2191)  time: 2.1075  data: 2.0959  max mem: 338
[19:11:19.680951] Test:  [710/800]  eta: 0:03:13  loss: 0.1243 (0.2191)  time: 2.0843  data: 2.0727  max mem: 338
[19:11:20.152886] Test:  [710/800]  eta: 0:03:13  loss: 0.1232 (0.2188)  time: 2.1172  data: 2.1054  max mem: 338
[19:11:21.842729] Test:  [710/800]  eta: 0:03:13  loss: 0.1243 (0.2191)  time: 2.1185  data: 2.1052  max mem: 338
[19:11:22.015194] Test:  [710/800]  eta: 0:03:13  loss: 0.1233 (0.2189)  time: 2.1269  data: 2.1133  max mem: 338
[19:11:22.035900] Test:  [710/800]  eta: 0:03:13  loss: 0.1254 (0.2189)  time: 2.1157  data: 2.1022  max mem: 338
[19:11:22.287582] Test:  [710/800]  eta: 0:03:13  loss: 0.1229 (0.2187)  time: 2.1163  data: 2.1045  max mem: 338
[19:11:22.361480] Test:  [710/800]  eta: 0:03:13  loss: 0.1232 (0.2191)  time: 2.1130  data: 2.0994  max mem: 338
[19:11:28.655760] Test:  [720/800]  eta: 0:02:50  loss: 0.1331 (0.2181)  time: 2.0951  data: 2.0815  max mem: 338
[19:11:28.663159] Test:  [720/800]  eta: 0:02:50  loss: 0.1359 (0.2179)  time: 2.0971  data: 2.0835  max mem: 338
[19:11:28.666406] Test:  [720/800]  eta: 0:02:50  loss: 0.1324 (0.2181)  time: 2.0987  data: 2.0850  max mem: 338
[19:11:28.680568] Test:  [720/800]  eta: 0:02:50  loss: 0.1368 (0.2180)  time: 2.0996  data: 2.0860  max mem: 338
[19:11:33.930566] Test:  [710/800]  eta: 0:03:15  loss: 0.1213 (0.2190)  time: 2.1299  data: 2.1180  max mem: 338
[19:11:44.071541] Test:  [720/800]  eta: 0:02:52  loss: 0.1344 (0.2182)  time: 2.1160  data: 2.1041  max mem: 338
[19:11:44.081642] Test:  [720/800]  eta: 0:02:52  loss: 0.1328 (0.2178)  time: 2.1156  data: 2.1037  max mem: 338
[19:11:44.955536] Test:  [720/800]  eta: 0:02:52  loss: 0.1344 (0.2181)  time: 2.1235  data: 2.1117  max mem: 338
[19:11:44.999929] Test:  [720/800]  eta: 0:02:52  loss: 0.1336 (0.2181)  time: 2.1245  data: 2.1128  max mem: 338
[19:11:45.209084] Test:  [720/800]  eta: 0:02:52  loss: 0.1337 (0.2180)  time: 2.1241  data: 2.1124  max mem: 338
[19:11:45.441989] Test:  [730/800]  eta: 0:02:28  loss: 0.1324 (0.2167)  time: 2.0808  data: 2.0676  max mem: 338
[19:11:45.443025] Test:  [730/800]  eta: 0:02:28  loss: 0.1347 (0.2167)  time: 2.0794  data: 2.0661  max mem: 338
[19:11:45.476050] Test:  [720/800]  eta: 0:02:52  loss: 0.1360 (0.2178)  time: 2.1200  data: 2.1082  max mem: 338
[19:11:45.491694] Test:  [730/800]  eta: 0:02:28  loss: 0.1331 (0.2167)  time: 2.0836  data: 2.0699  max mem: 338
[19:11:45.491772] Test:  [730/800]  eta: 0:02:28  loss: 0.1336 (0.2165)  time: 2.0815  data: 2.0678  max mem: 338
[19:11:47.432941] Test:  [720/800]  eta: 0:02:52  loss: 0.1335 (0.2179)  time: 2.1279  data: 2.1143  max mem: 338
[19:11:47.480875] Test:  [720/800]  eta: 0:02:52  loss: 0.1332 (0.2181)  time: 2.1365  data: 2.1232  max mem: 338
[19:11:47.498886] Test:  [720/800]  eta: 0:02:52  loss: 0.1329 (0.2179)  time: 2.1242  data: 2.1106  max mem: 338
[19:11:47.775402] Test:  [720/800]  eta: 0:02:52  loss: 0.1361 (0.2177)  time: 2.1244  data: 2.1128  max mem: 338
[19:11:47.875835] Test:  [720/800]  eta: 0:02:52  loss: 0.1358 (0.2181)  time: 2.1231  data: 2.1095  max mem: 338
[19:11:59.634480] Test:  [720/800]  eta: 0:02:53  loss: 0.1333 (0.2180)  time: 2.1415  data: 2.1296  max mem: 338
[19:12:01.044994] Test:  [730/800]  eta: 0:02:30  loss: 0.1344 (0.2169)  time: 2.1147  data: 2.1028  max mem: 338
[19:12:01.045489] Test:  [730/800]  eta: 0:02:30  loss: 0.1328 (0.2165)  time: 2.1147  data: 2.1029  max mem: 338
[19:12:02.115882] Test:  [730/800]  eta: 0:02:30  loss: 0.1344 (0.2168)  time: 2.1321  data: 2.1203  max mem: 338
[19:12:02.131868] Test:  [730/800]  eta: 0:02:30  loss: 0.1337 (0.2167)  time: 2.1225  data: 2.1107  max mem: 338
[19:12:02.142803] Test:  [730/800]  eta: 0:02:30  loss: 0.1336 (0.2168)  time: 2.1321  data: 2.1203  max mem: 338
[19:12:02.507008] Test:  [730/800]  eta: 0:02:30  loss: 0.1360 (0.2165)  time: 2.1177  data: 2.1058  max mem: 338
[19:12:04.488060] Test:  [730/800]  eta: 0:02:30  loss: 0.1335 (0.2166)  time: 2.1236  data: 2.1100  max mem: 338
[19:12:04.488821] Test:  [730/800]  eta: 0:02:30  loss: 0.1322 (0.2168)  time: 2.1323  data: 2.1186  max mem: 338
[19:12:04.574516] Test:  [730/800]  eta: 0:02:30  loss: 0.1308 (0.2166)  time: 2.1269  data: 2.1133  max mem: 338
[19:12:04.759609] Test:  [730/800]  eta: 0:02:30  loss: 0.1349 (0.2164)  time: 2.1236  data: 2.1119  max mem: 338
[19:12:04.874378] Test:  [730/800]  eta: 0:02:30  loss: 0.1331 (0.2168)  time: 2.1256  data: 2.1120  max mem: 338
[19:12:09.814664] Test:  [740/800]  eta: 0:02:07  loss: 0.0828 (0.2150)  time: 2.0579  data: 2.0442  max mem: 338
[19:12:09.824059] Test:  [740/800]  eta: 0:02:07  loss: 0.0848 (0.2147)  time: 2.0580  data: 2.0443  max mem: 338
[19:12:09.844227] Test:  [740/800]  eta: 0:02:07  loss: 0.0795 (0.2149)  time: 2.0588  data: 2.0456  max mem: 338
[19:12:09.894686] Test:  [740/800]  eta: 0:02:07  loss: 0.0819 (0.2149)  time: 2.0607  data: 2.0474  max mem: 338
[19:12:16.800094] Test:  [730/800]  eta: 0:02:31  loss: 0.1333 (0.2166)  time: 2.1434  data: 2.1316  max mem: 338
[19:12:25.200533] Test:  [740/800]  eta: 0:02:09  loss: 0.0804 (0.2151)  time: 2.0564  data: 2.0445  max mem: 338
[19:12:25.216750] Test:  [740/800]  eta: 0:02:09  loss: 0.0799 (0.2147)  time: 2.0567  data: 2.0449  max mem: 338
[19:12:26.254783] Test:  [740/800]  eta: 0:02:09  loss: 0.0869 (0.2150)  time: 2.0649  data: 2.0531  max mem: 338
[19:12:26.273702] Test:  [750/800]  eta: 0:01:46  loss: 0.1046 (0.2141)  time: 2.0390  data: 2.0253  max mem: 338
[19:12:26.281278] Test:  [740/800]  eta: 0:02:09  loss: 0.0787 (0.2150)  time: 2.0640  data: 2.0522  max mem: 338
[19:12:26.290941] Test:  [740/800]  eta: 0:02:09  loss: 0.0841 (0.2149)  time: 2.0540  data: 2.0423  max mem: 338
[19:12:26.314187] Test:  [750/800]  eta: 0:01:46  loss: 0.1099 (0.2143)  time: 2.0411  data: 2.0274  max mem: 338
[19:12:26.317555] Test:  [750/800]  eta: 0:01:46  loss: 0.1045 (0.2143)  time: 2.0437  data: 2.0302  max mem: 338
[19:12:26.339635] Test:  [750/800]  eta: 0:01:46  loss: 0.1112 (0.2142)  time: 2.0448  data: 2.0312  max mem: 338
[19:12:26.862841] Test:  [740/800]  eta: 0:02:09  loss: 0.0817 (0.2147)  time: 2.0693  data: 2.0577  max mem: 338
[19:12:28.769666] Test:  [740/800]  eta: 0:02:09  loss: 0.0798 (0.2150)  time: 2.0644  data: 2.0510  max mem: 338
[19:12:28.773764] Test:  [740/800]  eta: 0:02:09  loss: 0.0820 (0.2148)  time: 2.0670  data: 2.0534  max mem: 338
[19:12:28.896645] Test:  [740/800]  eta: 0:02:09  loss: 0.0814 (0.2148)  time: 2.0698  data: 2.0562  max mem: 338
[19:12:28.989059] Test:  [740/800]  eta: 0:02:09  loss: 0.0827 (0.2146)  time: 2.0606  data: 2.0488  max mem: 338
[19:12:29.108952] Test:  [740/800]  eta: 0:02:09  loss: 0.0835 (0.2150)  time: 2.0616  data: 2.0485  max mem: 338
[19:12:41.318203] Test:  [740/800]  eta: 0:02:10  loss: 0.0804 (0.2148)  time: 2.0841  data: 2.0723  max mem: 338
[19:12:41.709779] Test:  [750/800]  eta: 0:01:47  loss: 0.1059 (0.2140)  time: 2.0332  data: 2.0213  max mem: 338
[19:12:41.786326] Test:  [750/800]  eta: 0:01:47  loss: 0.1066 (0.2144)  time: 2.0370  data: 2.0251  max mem: 338
[19:12:42.740496] Test:  [750/800]  eta: 0:01:47  loss: 0.1064 (0.2143)  time: 2.0298  data: 2.0180  max mem: 338
[19:12:42.741936] Test:  [750/800]  eta: 0:01:47  loss: 0.1044 (0.2143)  time: 2.0313  data: 2.0196  max mem: 338
[19:12:43.044588] Test:  [750/800]  eta: 0:01:47  loss: 0.1043 (0.2143)  time: 2.0456  data: 2.0338  max mem: 338
[19:12:43.368625] Test:  [750/800]  eta: 0:01:47  loss: 0.1048 (0.2140)  time: 2.0430  data: 2.0314  max mem: 338
[19:12:45.260989] Test:  [750/800]  eta: 0:01:47  loss: 0.1057 (0.2143)  time: 2.0386  data: 2.0252  max mem: 338
[19:12:45.281021] Test:  [750/800]  eta: 0:01:47  loss: 0.1130 (0.2141)  time: 2.0396  data: 2.0260  max mem: 338
[19:12:45.442904] Test:  [750/800]  eta: 0:01:47  loss: 0.1073 (0.2141)  time: 2.0434  data: 2.0298  max mem: 338
[19:12:45.496192] Test:  [750/800]  eta: 0:01:47  loss: 0.1047 (0.2139)  time: 2.0368  data: 2.0249  max mem: 338
[19:12:45.709517] Test:  [750/800]  eta: 0:01:47  loss: 0.1102 (0.2143)  time: 2.0417  data: 2.0286  max mem: 338
[19:12:50.497725] Test:  [760/800]  eta: 0:01:25  loss: 0.1495 (0.2130)  time: 2.0336  data: 2.0200  max mem: 338
[19:12:50.501899] Test:  [760/800]  eta: 0:01:25  loss: 0.1538 (0.2132)  time: 2.0328  data: 2.0194  max mem: 338
[19:12:50.604000] Test:  [760/800]  eta: 0:01:25  loss: 0.1493 (0.2132)  time: 2.0394  data: 2.0257  max mem: 338
[19:12:50.766040] Test:  [760/800]  eta: 0:01:25  loss: 0.1506 (0.2131)  time: 2.0435  data: 2.0300  max mem: 338
[19:12:58.000791] Test:  [750/800]  eta: 0:01:48  loss: 0.1080 (0.2142)  time: 2.0600  data: 2.0482  max mem: 338
[19:13:06.491586] Test:  [760/800]  eta: 0:01:25  loss: 0.1475 (0.2130)  time: 2.0637  data: 2.0520  max mem: 338
[19:13:06.536051] Test:  [760/800]  eta: 0:01:25  loss: 0.1482 (0.2134)  time: 2.0667  data: 2.0550  max mem: 338
[19:13:07.285838] Test:  [760/800]  eta: 0:01:25  loss: 0.1523 (0.2132)  time: 2.0515  data: 2.0398  max mem: 338
[19:13:07.336013] Test:  [760/800]  eta: 0:01:25  loss: 0.1487 (0.2132)  time: 2.0527  data: 2.0408  max mem: 338
[19:13:07.358604] Test:  [760/800]  eta: 0:01:25  loss: 0.1519 (0.2132)  time: 2.0533  data: 2.0415  max mem: 338
[19:13:07.575400] Test:  [770/800]  eta: 0:01:03  loss: 0.1168 (0.2121)  time: 2.0650  data: 2.0514  max mem: 338
[19:13:07.654748] Test:  [770/800]  eta: 0:01:03  loss: 0.1175 (0.2122)  time: 2.0657  data: 2.0521  max mem: 338
[19:13:07.697570] Test:  [770/800]  eta: 0:01:03  loss: 0.1131 (0.2123)  time: 2.0690  data: 2.0554  max mem: 338
[19:13:07.698412] Test:  [770/800]  eta: 0:01:03  loss: 0.1172 (0.2123)  time: 2.0692  data: 2.0555  max mem: 338
[19:13:08.215973] Test:  [760/800]  eta: 0:01:26  loss: 0.1502 (0.2129)  time: 2.0676  data: 2.0558  max mem: 338
[19:13:10.013615] Test:  [760/800]  eta: 0:01:26  loss: 0.1455 (0.2132)  time: 2.0621  data: 2.0485  max mem: 338
[19:13:10.015880] Test:  [760/800]  eta: 0:01:26  loss: 0.1454 (0.2130)  time: 2.0621  data: 2.0485  max mem: 338
[19:13:10.201443] Test:  [760/800]  eta: 0:01:26  loss: 0.1517 (0.2131)  time: 2.0652  data: 2.0517  max mem: 338
[19:13:10.231513] Test:  [760/800]  eta: 0:01:26  loss: 0.1480 (0.2128)  time: 2.0621  data: 2.0504  max mem: 338
[19:13:10.484695] Test:  [760/800]  eta: 0:01:26  loss: 0.1507 (0.2132)  time: 2.0687  data: 2.0551  max mem: 338
[19:13:23.057012] Test:  [760/800]  eta: 0:01:26  loss: 0.1472 (0.2131)  time: 2.0869  data: 2.0751  max mem: 338
[19:13:23.165494] Test:  [770/800]  eta: 0:01:04  loss: 0.1200 (0.2121)  time: 2.0727  data: 2.0611  max mem: 338
[19:13:23.167436] Test:  [770/800]  eta: 0:01:04  loss: 0.1141 (0.2125)  time: 2.0690  data: 2.0573  max mem: 338
[19:13:24.022753] Test:  [770/800]  eta: 0:01:04  loss: 0.1181 (0.2123)  time: 2.0641  data: 2.0522  max mem: 338
[19:13:24.181853] Test:  [770/800]  eta: 0:01:04  loss: 0.1110 (0.2123)  time: 2.0719  data: 2.0601  max mem: 338
[19:13:24.837113] Test:  [770/800]  eta: 0:01:04  loss: 0.1116 (0.2120)  time: 2.0734  data: 2.0616  max mem: 338
[19:13:24.898236] Test:  [770/800]  eta: 0:01:04  loss: 0.1163 (0.2123)  time: 2.0926  data: 2.0808  max mem: 338
[19:13:26.519136] Test:  [770/800]  eta: 0:01:04  loss: 0.1187 (0.2121)  time: 2.0619  data: 2.0482  max mem: 338
[19:13:26.556757] Test:  [770/800]  eta: 0:01:04  loss: 0.1193 (0.2123)  time: 2.0647  data: 2.0511  max mem: 338
[19:13:26.848701] Test:  [770/800]  eta: 0:01:04  loss: 0.1165 (0.2121)  time: 2.0702  data: 2.0571  max mem: 338
[19:13:26.907274] Test:  [770/800]  eta: 0:01:04  loss: 0.1124 (0.2119)  time: 2.0705  data: 2.0588  max mem: 338
[19:13:27.011633] Test:  [770/800]  eta: 0:01:04  loss: 0.1165 (0.2123)  time: 2.0651  data: 2.0515  max mem: 338
[19:13:31.484670] Test:  [780/800]  eta: 0:00:42  loss: 0.1228 (0.2112)  time: 2.0491  data: 2.0357  max mem: 338
[19:13:31.488332] Test:  [780/800]  eta: 0:00:42  loss: 0.1240 (0.2112)  time: 2.0442  data: 2.0309  max mem: 338
[19:13:31.488704] Test:  [780/800]  eta: 0:00:42  loss: 0.1237 (0.2110)  time: 2.0495  data: 2.0359  max mem: 338
[19:13:31.490181] Test:  [780/800]  eta: 0:00:42  loss: 0.1291 (0.2112)  time: 2.0362  data: 2.0228  max mem: 338
[19:13:39.890782] Test:  [770/800]  eta: 0:01:04  loss: 0.1151 (0.2122)  time: 2.0945  data: 2.0826  max mem: 338
[19:13:47.962312] Test:  [780/800]  eta: 0:00:42  loss: 0.1216 (0.2110)  time: 2.0735  data: 2.0617  max mem: 338
[19:13:47.997219] Test:  [780/800]  eta: 0:00:42  loss: 0.1220 (0.2114)  time: 2.0730  data: 2.0611  max mem: 338
[19:13:48.682423] Test:  [780/800]  eta: 0:00:42  loss: 0.1190 (0.2113)  time: 2.0698  data: 2.0580  max mem: 338
[19:13:48.682925] Test:  [780/800]  eta: 0:00:42  loss: 0.1183 (0.2112)  time: 2.0662  data: 2.0544  max mem: 338
[19:13:48.904791] Test:  [780/800]  eta: 0:00:42  loss: 0.1244 (0.2113)  time: 2.0784  data: 2.0667  max mem: 338
[19:13:48.994236] Test:  [790/800]  eta: 0:00:21  loss: 0.1029 (0.2098)  time: 2.0669  data: 2.0536  max mem: 338
[19:13:49.005116] Test:  [790/800]  eta: 0:00:21  loss: 0.1045 (0.2097)  time: 2.0714  data: 2.0578  max mem: 338
[19:13:49.053177] Test:  [790/800]  eta: 0:00:21  loss: 0.1007 (0.2098)  time: 2.0677  data: 2.0543  max mem: 338
[19:13:49.053444] Test:  [790/800]  eta: 0:00:21  loss: 0.1058 (0.2098)  time: 2.0677  data: 2.0544  max mem: 338
[19:13:49.699493] Test:  [780/800]  eta: 0:00:42  loss: 0.1245 (0.2110)  time: 2.0741  data: 2.0624  max mem: 338
[19:13:51.371029] Test:  [780/800]  eta: 0:00:43  loss: 0.1244 (0.2110)  time: 2.0677  data: 2.0541  max mem: 338
[19:13:51.430189] Test:  [780/800]  eta: 0:00:43  loss: 0.1193 (0.2112)  time: 2.0708  data: 2.0576  max mem: 338
[19:13:51.643462] Test:  [780/800]  eta: 0:00:43  loss: 0.1177 (0.2111)  time: 2.0721  data: 2.0588  max mem: 338
[19:13:51.680492] Test:  [780/800]  eta: 0:00:43  loss: 0.1245 (0.2108)  time: 2.0724  data: 2.0606  max mem: 338
[19:13:51.883457] Test:  [780/800]  eta: 0:00:43  loss: 0.1292 (0.2113)  time: 2.0699  data: 2.0563  max mem: 338
[19:14:04.408039] Test:  [790/800]  eta: 0:00:21  loss: 0.1051 (0.2096)  time: 2.0621  data: 2.0502  max mem: 338
[19:14:04.503688] Test:  [790/800]  eta: 0:00:21  loss: 0.1035 (0.2100)  time: 2.0668  data: 2.0549  max mem: 338
[19:14:05.148987] Test:  [780/800]  eta: 0:00:43  loss: 0.1294 (0.2111)  time: 2.1046  data: 2.0927  max mem: 338
[19:14:05.361489] Test:  [790/800]  eta: 0:00:21  loss: 0.1036 (0.2099)  time: 2.0669  data: 2.0552  max mem: 338
[19:14:05.647414] Test:  [790/800]  eta: 0:00:21  loss: 0.1076 (0.2099)  time: 2.0732  data: 2.0614  max mem: 338
[19:14:05.671279] Test:  [799/800]  eta: 0:00:02  loss: 0.0606 (0.2086)  time: 1.7097  data: 1.6955  max mem: 338
[19:14:05.673239] Test:  [799/800]  eta: 0:00:02  loss: 0.0648 (0.2085)  time: 1.7099  data: 1.6957  max mem: 338
[19:14:05.673910] Test:  [799/800]  eta: 0:00:02  loss: 0.0636 (0.2086)  time: 1.7101  data: 1.6959  max mem: 338
[19:14:05.674013] Test:  [799/800]  eta: 0:00:02  loss: 0.0664 (0.2087)  time: 1.7099  data: 1.6958  max mem: 338
[19:14:05.926680] Test: Total time: 0:28:14 (2.1181 s / it)
[19:14:05.933982] (25596, 14) (25596, 14)
[19:14:05.943629] Test: Total time: 0:28:14 (2.1181 s / it)
[19:14:05.944239] Test: Total time: 0:28:14 (2.1181 s / it)
[19:14:05.947616] Test: Total time: 0:28:14 (2.1181 s / it)
[19:14:05.951194] (25596, 14) (25596, 14)
[19:14:05.951844] (25596, 14) (25596, 14)
[19:14:05.954919] (25596, 14) (25596, 14)
[19:14:06.025258] [0.7675503023520511, 0.8979174093588773, 0.8284245455340742, 0.6943683772347509, 0.8090003973487312, 0.7178507763110682, 0.7201692647481555, 0.8575017817953412, 0.7494847786325703, 0.8510874459234883, 0.912135075119543, 0.8174679660686329, 0.7884173431210315, 0.8928274821547411]
[19:14:06.041689] [0.7678857062551365, 0.9011125098653235, 0.8281807262571321, 0.6938832035024576, 0.8080165370381601, 0.7127750054814953, 0.7170753837580243, 0.8578477172278726, 0.7511532286753912, 0.8505199561362667, 0.9119525256331926, 0.8177786092049957, 0.7890192799019985, 0.8976630231646505]
[19:14:06.045733] [0.7684478876912931, 0.9000794565451495, 0.8305109975059367, 0.6933056303211871, 0.8091378045892483, 0.7144841576882845, 0.7180194211223324, 0.8572596245380238, 0.7495871243832756, 0.8503670465487984, 0.913411334624186, 0.8184568162641783, 0.7888572034862958, 0.8958438551229339]
[19:14:06.046151] [0.7683911762262587, 0.8995051290910462, 0.8286466482388188, 0.6938022411461788, 0.8108890512337098, 0.7177038534751266, 0.7171390631076746, 0.8575797217903813, 0.7526138131663551, 0.8491166015028038, 0.914306420794526, 0.8153549531819679, 0.7877954956280692, 0.9044676962066858]
[19:14:06.499135] Test:  [790/800]  eta: 0:00:21  loss: 0.1000 (0.2096)  time: 2.0831  data: 2.0713  max mem: 338
[19:14:06.674922] Test:  [790/800]  eta: 0:00:21  loss: 0.1056 (0.2098)  time: 2.0888  data: 2.0770  max mem: 338
[19:14:07.678800] Test:  [790/800]  eta: 0:00:21  loss: 0.1044 (0.2096)  time: 2.0579  data: 2.0443  max mem: 338
[19:14:07.681502] Test:  [790/800]  eta: 0:00:21  loss: 0.1041 (0.2099)  time: 2.0562  data: 2.0430  max mem: 338
[19:14:07.862655] Test:  [790/800]  eta: 0:00:21  loss: 0.1047 (0.2097)  time: 2.0507  data: 2.0370  max mem: 338
[19:14:08.143727] Test:  [790/800]  eta: 0:00:21  loss: 0.1025 (0.2099)  time: 2.0566  data: 2.0429  max mem: 338
[19:14:08.437236] Test:  [790/800]  eta: 0:00:21  loss: 0.1027 (0.2094)  time: 2.0765  data: 2.0646  max mem: 338
[19:14:20.838695] Test:  [790/800]  eta: 0:00:21  loss: 0.1080 (0.2098)  time: 2.0474  data: 2.0357  max mem: 338
[19:14:20.914567] Test:  [799/800]  eta: 0:00:02  loss: 0.0626 (0.2084)  time: 1.9786  data: 1.9665  max mem: 338
[19:14:21.032365] Test: Total time: 0:28:29 (2.1370 s / it)
[19:14:21.040340] (25596, 14) (25596, 14)
[19:14:21.057329] Test:  [799/800]  eta: 0:00:02  loss: 0.0649 (0.2088)  time: 1.9853  data: 1.9731  max mem: 338
[19:14:21.121656] Test: Total time: 0:28:29 (2.1371 s / it)
[19:14:21.129446] (25596, 14) (25596, 14)
[19:14:21.133998] [0.7683345330882906, 0.8998313383891133, 0.8291900752107031, 0.6948555201287457, 0.8094992422451269, 0.7167353681599954, 0.717625472603309, 0.8571599863625464, 0.75091481578402, 0.8486054860340458, 0.9134823903968441, 0.8139278677500803, 0.7883418863526613, 0.8938382576828056]
[19:14:21.221470] [0.7666487813737903, 0.8997034367310907, 0.8287096139941836, 0.6940627205125215, 0.8066200831812771, 0.7145993262753315, 0.7213402812180817, 0.8572307590853152, 0.750467969718632, 0.8501558783865946, 0.9117230039124735, 0.8142259024297318, 0.7875949215913299, 0.898422643195099]
[19:14:21.359065] Test:  [799/800]  eta: 0:00:02  loss: 0.0664 (0.2084)  time: 1.9916  data: 1.9793  max mem: 338
[19:14:21.455297] Test: Total time: 0:28:30 (2.1375 s / it)
[19:14:21.463294] (25596, 14) (25596, 14)
[19:14:21.556374] [0.7676335247188126, 0.9003295579682847, 0.828175250974057, 0.6961713064203034, 0.809996000125125, 0.7145345327006952, 0.7196067998032777, 0.8589346717041573, 0.7501619055330766, 0.8501805051778706, 0.912737462287326, 0.8139827784927138, 0.7894226462398863, 0.8953654745517033]
[19:14:21.597898] Test:  [799/800]  eta: 0:00:02  loss: 0.0658 (0.2087)  time: 1.9265  data: 1.9122  max mem: 338
[19:14:21.597950] Test:  [799/800]  eta: 0:00:02  loss: 0.0617 (0.2085)  time: 1.9246  data: 1.9104  max mem: 338
[19:14:21.731691] Test:  [799/800]  eta: 0:00:02  loss: 0.0628 (0.2087)  time: 1.9057  data: 1.8916  max mem: 338
[19:14:21.849465] Test: Total time: 0:28:30 (2.1380 s / it)
[19:14:21.851673] Test: Total time: 0:28:30 (2.1380 s / it)
[19:14:21.851842] Test: Total time: 0:28:30 (2.1380 s / it)
[19:14:21.854062] Test:  [799/800]  eta: 0:00:02  loss: 0.0646 (0.2085)  time: 1.8946  data: 1.8804  max mem: 338
[19:14:21.857725] (25596, 14) (25596, 14)
[19:14:21.859634] (25596, 14) (25596, 14)
[19:14:21.860284] (25596, 14) (25596, 14)
[19:14:21.934138] Test: Total time: 0:28:30 (2.1381 s / it)
[19:14:21.942016] (25596, 14) (25596, 14)
[19:14:21.950555] [0.7670484605481501, 0.8977843016247191, 0.8290717978176054, 0.6932738634183397, 0.8103489817662894, 0.7140649142176507, 0.7213274374170504, 0.8580992753936825, 0.7493618710587184, 0.8491885538004463, 0.913694922954894, 0.8142415259521782, 0.7888346093899348, 0.9007985924352511]
[19:14:21.952735] [0.7672191825560235, 0.8986851816346567, 0.8289252468419612, 0.693586468888152, 0.808914637766879, 0.7163086712436478, 0.7156487144866204, 0.8567309072989422, 0.7506878943453597, 0.8484469236777615, 0.9134573920574881, 0.8157629920781432, 0.79086371309054, 0.898513350897505]
[19:14:21.954177] [0.7681552428663024, 0.8983626146828968, 0.8295493522124922, 0.693306142555148, 0.8102912289849442, 0.7140317078319188, 0.7197754961143005, 0.859025228062031, 0.7507365708416204, 0.8516339897921512, 0.9136305321614372, 0.8135051646705561, 0.7876851011952545, 0.8960396287821466]
[19:14:22.032937] [0.7659916184827612, 0.9005297344561727, 0.8293462643250938, 0.6943831522454724, 0.8097564490246005, 0.7163001254826138, 0.7166818669634052, 0.8585179390034038, 0.7482226649675071, 0.8486738889187107, 0.9147009427566407, 0.8164627157428003, 0.7869987630313642, 0.8959885772109433]
[19:14:22.436139] Test:  [799/800]  eta: 0:00:02  loss: 0.0630 (0.2087)  time: 1.9332  data: 1.9208  max mem: 338
[19:14:22.436178] Test:  [799/800]  eta: 0:00:02  loss: 0.0635 (0.2087)  time: 1.9317  data: 1.9193  max mem: 338
[19:14:22.604034] Test: Total time: 0:28:31 (2.1390 s / it)
[19:14:22.611939] (25596, 14) (25596, 14)
[19:14:22.618046] Test: Total time: 0:28:31 (2.1390 s / it)
[19:14:22.625866] (25596, 14) (25596, 14)
[19:14:22.682781] Test:  [799/800]  eta: 0:00:02  loss: 0.0630 (0.2086)  time: 1.9182  data: 1.9059  max mem: 338
[19:14:22.704280] [0.7680436032726642, 0.8981143058281011, 0.8290329222824376, 0.6940908807844481, 0.8097875263779681, 0.7148826214662989, 0.7185629981245172, 0.8570642263686402, 0.7504074774141405, 0.8499494865949407, 0.9130211813039008, 0.8152520298016407, 0.7875326134063528, 0.8920229640906895]
[19:14:22.717610] [0.7671419319748572, 0.8986783736889413, 0.8278497792312547, 0.6942314260271013, 0.8106434309935465, 0.7159771471186046, 0.7205770644251536, 0.8576025654252913, 0.7500432377492368, 0.8495894402772923, 0.9136555491702025, 0.8158981675252751, 0.7899103960714681, 0.8967946906365949]
[19:14:22.750459] Test: Total time: 0:28:31 (2.1391 s / it)
[19:14:22.758227] (25596, 14) (25596, 14)
[19:14:22.853599] [0.7676181716270136, 0.8984390086059681, 0.8288693456372309, 0.6952565825254928, 0.8095101451066519, 0.7167710804455792, 0.7176924618400597, 0.8571949718148655, 0.7499627512437586, 0.8494812050914358, 0.9126291050344341, 0.8141008228845317, 0.7885821744780165, 0.8969512639822048]
[19:14:22.856520] Test:  [799/800]  eta: 0:00:02  loss: 0.0692 (0.2083)  time: 1.8490  data: 1.8367  max mem: 338
[19:14:22.934226] Test: Total time: 0:28:31 (2.1394 s / it)
[19:14:22.941918] (25596, 14) (25596, 14)
[19:14:23.035057] [0.7680817229429566, 0.9002284876257292, 0.8293626491609631, 0.6943306146753787, 0.8106750600868813, 0.7186514305899538, 0.7203641163626787, 0.857902494497114, 0.7522005147290421, 0.8499483472771949, 0.9137592017318938, 0.8163449911306816, 0.7903790044279062, 0.9006668611488426]
[19:14:23.895184] Test:  [799/800]  eta: 0:00:02  loss: 0.0700 (0.2086)  time: 1.3586  data: 1.3465  max mem: 338
[19:14:23.977745] Test: Total time: 0:28:32 (2.1407 s / it)
[19:14:23.985490] (25596, 14) (25596, 14)
[19:14:24.077188] [0.7681818972548686, 0.899283136665067, 0.8291906904110486, 0.6948407745084972, 0.810156688393387, 0.7159183034497708, 0.7205057939213924, 0.8565612245824675, 0.7516124234187929, 0.8510918717347318, 0.9121856505499505, 0.8159129687570665, 0.7891808196408279, 0.8961738670653552]
[19:14:24.078558] Loss 0.209
[19:14:24.078578] Loss 0.209
[19:14:24.078398] Loss 0.209
[19:14:24.078406] Loss 0.209
[19:14:24.078412] Loss 0.209
[19:14:24.078642] Average AUC of the network on the test set images: 0.8079[19:14:24.078622] 
Loss 0.209
[19:14:24.078634] Loss 0.209
[19:14:24.078364] Loss 0.209
[19:14:24.078395] [19:14:24.078402] Loss 0.209
Loss 0.209
[19:14:24.078427] Loss 0.209
[19:14:24.078351] Loss 0.209
[19:14:24.078356] [19:14:24.078359] Loss 0.209
Loss 0.209[19:14:24.078362] 
Loss 0.209
[19:14:24.078508] Average AUC of the network on the test set images: 0.8075
[19:14:24.078435] Average AUC of the network on the test set images: 0.8073
[19:14:24.078762] Average AUC of the network on the test set images: 0.8073
[19:14:24.078489] Average AUC of the network on the test set images: 0.8078
[19:14:24.078444] Average AUC of the network on the test set images: 0.8077
[19:14:24.078494] Average AUC of the network on the test set images: 0.8070
[19:14:24.078448] Average AUC of the network on the test set images: 0.8074
[19:14:24.078564] Average AUC of the network on the test set images: 0.8088
[19:14:24.078454] Average AUC of the network on the test set images: 0.8076
[19:14:24.078604] Average AUC of the network on the test set images: 0.8074
[19:14:24.078842] Average AUC of the network on the test set images: 0.8077
[19:14:24.078627] Average AUC of the network on the test set images: 0.8084
[19:14:24.078675] Average AUC of the network on the test set images: 0.8077
[19:14:24.078882] Average AUC of the network on the test set images: 0.8073
[19:14:24.079296] Loss 0.209
[19:14:24.080046] Average AUC of the network on the test set images: 0.8074
