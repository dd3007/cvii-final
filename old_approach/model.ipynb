{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computer Vision II: Final Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Improving on 3D Reconstruction Using a Single Image*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Approach:\n",
    "\n",
    "1) Integration of ResNet and VAE for Enhanced Feature Extraction and Model Flexibility:\n",
    "- Approach from [19†source]: Use deep residual networks (ResNets) for their ability to learn deep features effectively due to residual connections that help mitigate the vanishing gradient problem in deep architectures.\n",
    "- Residual Networks (ResNets): ResNets are beneficial for learning deep features from images due to their architecture that includes skip connections, allowing them to mitigate the vanishing gradient problem often encountered in deep neural networks. This characteristic makes them highly effective at processing complex image data and extracting rich feature sets that are crucial for accurate 3D modeling.\n",
    "(More about ResNets can be found in He et al., 2016: https://arxiv.org/abs/1512.03385)\n",
    "-------------------------------------------------------------------------------------------------\n",
    "- Approach from [19†source]: Combine this with the capabilities of Variational Auto-Encoders (VAEs) to manage the encoding and decoding processes within a probabilistic framework. This would allow for the generation of continuous, dense latent spaces that are ideal for complex 3D shapes.\n",
    "- Variational Auto-Encoders (VAEs): VAEs are designed to encode inputs into a compressed latent space and reconstruct the input from this space. In the context of 3D reconstruction, VAEs can be used to generate and refine 3D models from these latent representations. They provide a probabilistic framework that handles the inherent uncertainties in predicting 3D shapes from 2D images, making them particularly suitable for tasks where precise and detailed reconstructions are required.\n",
    "(Introductory details on VAEs can be accessed in Kingma and Welling, 2013: https://arxiv.org/abs/1312.6114).\n",
    "-------------------------------------------------------------------------------------------------\n",
    "- Innovation: By integrating ResNets and VAEs, you can create a robust feature extraction mechanism that captures intricate details and variability of 3D shapes, while the VAE framework handles the uncertainty and variability in 3D object dimensions more naturally.\n",
    "- Combination for 3D Reconstruction:\n",
    "By combining ResNets and VAEs, you can leverage the deep feature extraction capabilities of ResNets along with the generative properties of VAEs. This combination allows for the detailed capture of 3D geometries from 2D data while accommodating the variability and complexity of real-world objects.\n",
    "This approach enables the model to not only identify and replicate the general shape of the object but also to fine-tune the details and textures that are critical for a realistic 3D output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tf\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs CUDA available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tf\u001b[38;5;241m.\u001b[39mtest\u001b[38;5;241m.\u001b[39mis_built_with_cuda())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Is CUDA available:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPUs available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Flatten, Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms, ToTensor, Resize, Compose\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet18\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from vae_net import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeNetVoxelDataset(Dataset):\n",
    "    def __init__(self, image_dir, voxel_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (string): Directory with all the images.\n",
    "            voxel_dir (string): Directory with all the voxel data, aligned with the images by filename.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "        self.voxel_dir = voxel_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_files[index]\n",
    "        base_filename = os.path.basename(img_path)\n",
    "        voxel_filename = base_filename.replace('.jpg', '.npy').replace('.png', '.npy')\n",
    "        voxel_path = os.path.join(self.voxel_dir, voxel_filename)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        voxel = np.load(voxel_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = ToTensor()(image)\n",
    "        voxel = torch.tensor(voxel, dtype=torch.float32)\n",
    "\n",
    "        return image, voxel\n",
    "\n",
    "# Example usage\n",
    "transform = Compose([Resize((256, 256))])\n",
    "train_dataset = ShapeNetVoxelDataset(image_dir='3DRecon/train_imgs',\n",
    "                                     voxel_dir='3DRecon/train_voxels',\n",
    "                                     transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "val_dataset = ShapeNetVoxelDataset(image_dir='3DRecon/val_imgs',\n",
    "                                   voxel_dir='3DRecon/val_voxels',\n",
    "                                   transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper 1: Deep Residual Learning for Image Recognition (ResNets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained ResNet50 model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers to prevent learning during training\n",
    "\n",
    "# Remove the final classification layer to use as a feature extractor\n",
    "modules = list(resnet.children())[:-2]  # Remove the last pooling and linear layer\n",
    "resnet = nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper 2: Auto-Encoding Variational Bayes (VAEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup args\n",
    "class Args:\n",
    "    def __init__(self, Nz, decoder_type):\n",
    "        self.Nz = Nz\n",
    "        self.decoder_type = decoder_type\n",
    "\n",
    "# Initialize VAE with appropriate dimensions and arguments\n",
    "vae_args = Args(Nz=100, decoder_type='gaussian')  # Example arguments\n",
    "feature_dim = 2048 * 7 * 7  # Adjust based on your ResNet output\n",
    "vae = VAE(args=vae_args, d=feature_dim, h_num=500, scaled=True)\n",
    "\n",
    "# Adapt the forward function in the main training loop\n",
    "def forward(data):\n",
    "    data = resnet(data)  # Get features from ResNet\n",
    "    data = data.view(data.size(0), -1)  # Flatten the features\n",
    "    output, _, _, _ = vae(data)  # Assuming Gaussian output\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-loader Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function for VAE with Gaussian outputs\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')  # Use MSE if output is continuous\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Example Training Loop\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "for epoch in range(10):\n",
    "    for data in train_loader:\n",
    "        images, _ = data  # Assuming images are your input\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = forward(images)\n",
    "        mu, logvar = vae.encode(images.view(images.size(0), -1))\n",
    "        loss = loss_function(outputs, images.view(images.size(0), -1), mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sample = torch.randn(64, 20).to(device)\n",
    "    sample = vae.decode(sample).cpu()\n",
    "    # Visualize or further process the sample output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have some output to visualize\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(batch[0].numpy().squeeze(), cmap='gray')  # Original Image\n",
    "axes[1].imshow(reconstructed_images[0].detach().numpy().squeeze(), cmap='gray')  # Reconstructed Image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative Approaches (For Experimental Purposes) - Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNto3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNto3D, self).__init__()\n",
    "        # Feature Extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # More layers as needed\n",
    "        )\n",
    "        # 3D Generator (simplified version, assuming output as voxels)\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(192 * 28 * 28, 4096),  # Adjust size according to feature map size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 64*64*64),  # Assuming outputting a 64x64x64 voxel grid\n",
    "            nn.Sigmoid()  # Assuming voxel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        output_3d = self.generator(features)\n",
    "        output_3d = output_3d.view(-1, 64, 64, 64)  # Reshape to voxel grid\n",
    "        return output_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "model = CNNto3D()  # Initializing the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()  # Using Mean Squared Error loss for voxel reconstruction\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        for inputs, true_voxels in train_loader:\n",
    "            inputs, true_voxels = inputs.to(device), true_voxels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predicted_voxels = model(inputs)\n",
    "            loss = criterion(predicted_voxels, true_voxels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print training loss every epoch\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {running_loss / len(train_loader):.3f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, true_voxels in val_loader:\n",
    "                inputs, true_voxels = inputs.to(device), true_voxels.to(device)\n",
    "                predicted_voxels = model(inputs)\n",
    "                loss = criterion(predicted_voxels, true_voxels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print validation loss every epoch\n",
    "        print(f'Epoch {epoch+1}, Val Loss: {val_loss / len(val_loader):.3f}')\n",
    "\n",
    "train_model(train_loader, val_loader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_slices(voxel_grid, slice_direction='z', num_slices=10):\n",
    "    if slice_direction == 'z':\n",
    "        slices = np.linspace(0, voxel_grid.shape[2] - 1, num_slices, dtype=int)\n",
    "        fig, axs = plt.subplots(1, len(slices), figsize=(15, 3))\n",
    "        for i, slice_idx in enumerate(slices):\n",
    "            axs[i].imshow(voxel_grid[:, :, slice_idx], cmap='gray')\n",
    "            axs[i].title.set_text(f'Slice {slice_idx}')\n",
    "            axs[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example of how to use this function with a sample batch from the validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images, voxels = next(iter(val_loader))\n",
    "    images, voxels = images.to(device), voxels.to(device)\n",
    "    preds = model(images)\n",
    "    visualize_slices(preds[0].cpu().numpy())  # Visualizing the first voxel grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project Environment",
   "language": "python",
   "name": "project-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
